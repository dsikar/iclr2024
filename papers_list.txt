{'paperID': '96f8e5820c5b59e65c2331d577aa738676e8c605', 'abstract': 'While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that the number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a Bayesian inference framework and tackle it using variational inference. We validate our Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on multiple realistic task- and class-imbalanced datasets, on which it significantly outperforms existing meta-learning approaches. Further ablation study confirms the effectiveness of each balancing component and the Bayesian learning framework.', 'bibtex': '@Article{Lee2019LearningTB,\n author = {Haebeom Lee and Hayeon Lee and Donghyun Na and Saehoon Kim and Minseop Park and Eunho Yang and S. Hwang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks},\n volume = {abs/1905.12917},\n year = {2019}\n}\n', 'references': ['1ec049a29d369294a5a5ffdfb67e872dce899dac', 'e13ca1bd60af3325afc64dc09979e3322818e365', 'd8bafd3a23c5ce9a7ebef036d5f2c67e1386ff11', '712f4f21b9d3e6a7f110a2ecd9b3a2f900397b9f', '2686aef553af3ebd43cb0ea98a538a9435f7dd5f', '04f739a0c29b75877243731aeead512bf0ed1dff', '849c91ff8bb3ff67d278adb5295fee78049c6288', '38e2f851b705faa0d0a698ed9885bd6834440073', '7b0aad12a6917b7d444ba2f87c7f8ccc5357797a', 'e6a83abec5cffb0bf1669f2f2c1efdf2b15cb171', '208cd4b25768f0096fb2e80e7690473da0e2a563', '90dc22818bd2d97d8deaff168b0137b75a962767', 'df093d69cd98cf4b26542f53614a79754754eb78', 'd4c4a5f0c71eba13d2827b24f70bbfdc3bd858ec', 'bfe284e4338e62f0a61bb33398353efd687f206f', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', 'd33ad6a25264ba1747d8c93f6621c7f90a7ec601', '3e08a3912ebe494242f6bcd772929cc65307129c', 'c269858a7bb34e8350f2442ccf37797856ae9bca', 'a456265138c088a894301c0433dae938705a9bec', 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518', '29c887794eed2ca9462638ff853e6fe1ab91d5d8', '3904315e2eca50d0086e4b7273f7fd707c652230', 'be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6', '405c31c85a324942811f3c9dc53ce3528f9284df', '815c84ab906e43f3e6322f2ca3fd5e1360c64285', 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', 'ba640d55b77407f3170e9c1bd5f2cfbcbfd67df5', '522d65a3db7431015aeaa201a7fc4450a57e40c3', '02b28f3b71138a06e40dbd614abf8568420ae183', '696e44a895bd153006b90a47be0c6bfba0a6ef4c', '53bb7789be36a58f865f2ec84f6d8f816ddaae6a', 'd3fde30a26bff87d24aba05400334a8dd5aba2c3', '02227c94dd41fe0b439e050d377b0beb5d427cda']}
{'paperID': '1322719978980a831e1aee78aa80a141379c44dd', 'abstract': "Likelihood-based generative models are a promising resource to detect out-of-distribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, we pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. We report a set of experiments supporting this hypothesis, and use an estimate of input complexity to derive an efficient and parameter-free OOD score, which can be seen as a likelihood-ratio, akin to Bayesian model comparison. We find such score to perform comparably to, or even better than, existing OOD detection approaches under a wide range of data sets, models, model sizes, and complexity estimates.", 'bibtex': '@Article{Serrà2019InputCA,\n author = {J. Serrà and David Álvarez and V. Gómez and Olga Slizovskaia and José F. Núñez and J. Luque},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Input complexity and out-of-distribution detection with likelihood-based generative models},\n volume = {abs/1909.11480},\n year = {2019}\n}\n', 'references': ['5647d92d8e7248cd2d4770edfe0688c1c0a2181b', '925182b91f51f8f2b747f7829e9d25ffc2729e5d', '37c790f3d5506195c2d9b0b3d59effd932359a96', '4bb3301a284d646b4c1ffabcca78ee85c11d1cda', '2d8c97db4bae00ff243d122b957091a236a697a7', '6507909a8f77c88144c3a67b9336bd1c85e84cac', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', '21b786b3f870fc7fa247c143aa41de88b1fc6141', '1d078f860ffafc2cb7fcabd29c47584be79c5f2c', 'b36a5bb1707bb9c70025294b3a310138aae8327a', '79f618ec9ea278efb3381375d41f83ddf964edf2', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '2e77b99e8bd10b9e4551a780c0bde9dd10fdbe9b', '802168a81571dde28f5ddb94d84677bc007afa7b', '6ff2a434578ff2746b9283e45abf296887f48a2d', '815c84ab906e43f3e6322f2ca3fd5e1360c64285', '39e0c341351f8f4a39ac890b96217c7f4bde5369', '4543670c4b2d88a9b67525e0084044adef94ae76', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '0d3bb75852098b25d90f31d2f48fd0cb4944702b', '22fe619996b59c09cb73be40103a123d2e328111', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'd40ee5dd758c525dfb9932d726bb4e844b7b8478', 'a69db833affd92308c7ef5c2cff098c217714f9d', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2', 'f7f15848cd0fbb3d08f351595da833b1627de9c3', '4bdf6ec7229d307d172e6cce48052b11524b8789', 'b9de7b4b1cbc6fe6fd83bd8e0f174735b296630b', '7dbdb4209626fd92d2436a058663206216036e68']}
{'paperID': 'b6c4a3f913ee819afa49e115f9739fe31ecf13f5', 'abstract': 'Model-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms "vanilla" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.', 'bibtex': '@Article{Kwon2021RepurposingPM,\n author = {Namyeong Kwon and Hwidong Na and Gabriel Huang and S. Lacoste-Julien},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning},\n volume = {abs/2103.09027},\n year = {2021}\n}\n', 'references': ['e91f12d3306f704717d6ae7b18f6de920c63c045', '17b99c60d6b2fdd656af6a7661b8d6af05255792', '948839277bface5780896e8e8791906818aa41ac', '59aa504c75b12cb6e5100049f4446ced5b031042', 'cb4e58d9de165a4fb64eccbe2f4b49c1cd83b650', '7ccb657207d1de7e6016ba5ca6f6f68eb7204662', '712f4f21b9d3e6a7f110a2ecd9b3a2f900397b9f', '8195787260dfc6bc9abea3b1dac1ce15f747caa2', '3a26e26cfa8e8dbd70145f76e058d55fd83997f7', '32760a5d2a55a586b2a9aab7278db89de065eae3', '46b072cf918ec6f50403568a73d4347ea86b7e66', 'd4c4a5f0c71eba13d2827b24f70bbfdc3bd858ec', 'bfe284e4338e62f0a61bb33398353efd687f206f', '1c60ac28884d2414efcf0eec90561fae2a377311', '1b11679c3a2c0f2f65e70d40ec3bcb8cfc8e6f8a', '7e9c1e0d247b20a0683f4797d9ea248c3b53d424', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'c269858a7bb34e8350f2442ccf37797856ae9bca', 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518', '345afa0e85cb2f5cb438ae44027499ff2c392409', '802168a81571dde28f5ddb94d84677bc007afa7b', '29c887794eed2ca9462638ff853e6fe1ab91d5d8', '3904315e2eca50d0086e4b7273f7fd707c652230', 'be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', '1a2a770d23b4a171fa81de62a78a3deb0588f238', 'ba640d55b77407f3170e9c1bd5f2cfbcbfd67df5', '66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde', '02b28f3b71138a06e40dbd614abf8568420ae183', '76073b1cf23e392f6c16c72fa3a2f8f31322eacf', '0500862417b0349c664586d3566aef416cb5786e', 'd3fde30a26bff87d24aba05400334a8dd5aba2c3', 'df24c3011fc42b72195e876ce052a0a072a1d923']}
{'paperID': '703ffd7ab0bdfcb091400ebb9c7b92446204831f', 'abstract': 'We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for levels of noise. Once trained, we calculate the noisy score estimates for N in-distribution samples and take the L2-norms across the input dimensions (resulting in an NxL matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this L-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.', 'bibtex': '@Article{Mahmood2020MultiscaleSM,\n author = {Ahsan Mahmood and Junier B. Oliva and M. Styner},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Multiscale Score Matching for Out-of-Distribution Detection},\n volume = {abs/2010.13132},\n year = {2020}\n}\n', 'references': ['30b22164a29d34994b497bf9815076781555ff4e', '1156e277fa7ec195b043161d3c5c97715da17658', '7f5b344f292645011a19801b23277d3820acc70e', '97cd86d8d8c0f27cd3e64c6ca5cfdeb957ee39f4', '501c02c7caa7fc2c7077405299b4cbe7d294b170', 'c178e64c65f5401e61d775e95649d46fb43c1965', 'd756e0bb4b3576a644b1c20c50bb60e20ae45a9d', '965359b3008ab50dd04e171551220ec0e7f83aba', '925182b91f51f8f2b747f7829e9d25ffc2729e5d', '6507909a8f77c88144c3a67b9336bd1c85e84cac', '2d8c97db4bae00ff243d122b957091a236a697a7', '21b786b3f870fc7fa247c143aa41de88b1fc6141', 'abeb5bc02ea3f80b67cf58e3e290448af38933e5', '431ba9fae8fccad1665979d455c6307786e47318', '36653f8705b56e39642bcd123494eb680cd1636b', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '585bf7bea8fa5267738bc465611d6f197e0f87dd', '6ff2a434578ff2746b9283e45abf296887f48a2d', 'e86f71ca2948d17b003a5f068db1ecb2b77827f7', '10a498003e9204f5fc1328e706510a37e514d8c7', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', '3433627f803953280b66ae1576d083fc9a68385a', '4543670c4b2d88a9b67525e0084044adef94ae76', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '872bae24c109f7c30e052ac218b17a8b028d08a0', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '9966e890f2eedb4577e11b9d5a66380a4d9341fe', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '0f4374e62ae889dd2a35dc4c97b9a0510146fa87', 'abstract': 'Consider a prediction setting where a few inputs (e.g., satellite images) are expensively annotated with the prediction targets (e.g., crop types), and many inputs are cheaply annotated with auxiliary information (e.g., climate information). How should we best leverage this auxiliary information for the prediction task? Empirically across three image and time-series datasets, and theoretically in a multi-task linear regression setting, we show that (i) using auxiliary information as input features improves in-distribution error but can hurt out-of-distribution (OOD) error; while (ii) using auxiliary information as outputs of auxiliary tasks to pre-train a model improves OOD error. To get the best of both worlds, we introduce In-N-Out, which first trains a model with auxiliary inputs and uses it to pseudolabel all the in-distribution inputs, then pre-trains a model on OOD auxiliary outputs and fine-tunes this model with the pseudolabels (self-training). We show both theoretically and empirically that In-N-Out outperforms auxiliary inputs or outputs alone on both in-distribution and OOD error.', 'bibtex': '@Article{Xie2020InNOutPA,\n author = {Sang Michael Xie and Ananya Kumar and Robbie Jones and Fereshte Khani and Tengyu Ma and Percy Liang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness},\n volume = {abs/2012.04550},\n year = {2020}\n}\n', 'references': ['767c6702045f2290012a259744db9edb4d55bcb8', '6e2d24dbf959aeb855926430bf1cb476346719b3', '368c72c2298e5f8276398b2cb198702281eac4f8', '3d4c4cd680da0ebd2d774c5b5236b9db3da15185', '7bfaca28948164006002a3a71a38165d36af51c5', '7438c34a128c13e811aca2e599028bb3760e1816', '7d0029510d7f47d3ee716aa8e01b69e353f9e143', '8dc871b80e50135c9d99de9f8eb8c64b63ee924f', '3f8e190773a8898ef296da67b646e40dca1374cb', '02841af780570666389643f2815460a10d9ae286', '6eb53e1763f2eab95093eb1e6f67d34d2618fa9b', '20ba55ee3229db5cb190a00e788c59f08d2a767d', 'db787640c9b42416ff8d7015546e667e58267177', '998039a4876edc440e0cabb0bc42239b0eb29644', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', 'a8420e6a3a196a4c34566b5b8717318743eb6e4b', '3f7bc67330b3eff749459568e7995f0017dfe645', '4e0bb8c1c683b43357c5d5216f6b74ff2cb32434', 'aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '12c77ac9898c049474850c84235439ec564221bf', '05447ad7c0bc55f8b3766e23da1578bb13f46bbf', '008c901b3fd9e46ee8d3bddb616121e2887b7e67', 'b3de1062d8a462dfdc2938558258f8884abe9f4e', '907a90967f68da4311802247408e0515e363f930', '18bc1d4271abe8dd6e16179cdb06524a4f396e16', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', '7a9e471e31ac156cf22a5e2a5c1463697df866ab', '4f975da00a5b2a2f7236e34edcb7274e5fdab937', '37acbbbcfe9d8eb89e5b01da28dac6d44c3903ee', 'c8f9a5a2f6da022a08b1b0c5b7c4d311fcb1ce34', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'a04622cfd1154dd6dc04591cbb25188207bc6142', '1d5972b32a9b5a455a6eef389de5b7fca25771ad', '6364fdaa0a0eccd823a779fcdd489173f938e91a', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', 'eb42cf88027de515750f230b23b1a057dc782108', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '611503ed9d36bad843374774c59ab335ebf7eac4', '900b806495ce8a175e66cff13755fd99ec525ab5', 'a25fbcbbae1e8f79c4360d26aa11a3abf1a11972', '424c3ab98f1ae2d9e9ecf892e15a12c09f4e9ffe', '0adf759f449435bc8343c9a8a2a25ad5bb63e5b8', '723d063248ac896fe63325d4c88d750162d10bfc', '7ad785d8cd5d45ee07a82a2c0d7d4c733b1251d5', '5bbff3807fef66bbf758874d523f95cf75d945cd', '161ffb54a3fdf0715b198bb57bd22f910242eb49', '5b6e487e98785c5f44fd0ffd1e1bafdb3aac1456', '35a8b3dee2fe190bc34fa79222269325ee1d08c7', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '32e29041fa352a9df0889f42807ed6141bc0b5ff', '6b96f4daa52f7fe7e6334bf2b2e3d8e245f652c7', 'ecbd467eacde24de43f43bc703c891df447ff389', 'f6a883e5ce485ab9300d56cb440e8634d9aa1105']}
{'paperID': '2be5c2f23b997e1cb22479dc1d5d1735997ad4be', 'abstract': 'Several data augmentation methods deploy unlabeled-in-distribution (UID) data to bridge the gap between the training and inference of neural networks. However, these methods have clear limitations in terms of availability of UID data and dependence of algorithms on pseudo-labels. Herein, we propose a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. We show how to improve generalization theoretically using OOD data in each learning scenario and complement our theoretical analysis with experiments on CIFAR-10, CIFAR-100, and a subset of ImageNet. The results indicate that undesirable features are shared even among image data that seem to have little correlation from a human point of view. We also present the advantages of the proposed method through comparison with other data augmentation methods, which can be used in the absence of UID data. Furthermore, we demonstrate that the proposed method can further improve the existing state-of-the-art adversarial training.', 'bibtex': '@Article{Lee2021RemovingUF,\n author = {Saehyung Lee and Changhwa Park and Hyungyu Lee and Jihun Yi and Jonghyun Lee and Sungroh Yoon},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Removing Undesirable Feature Contributions Using Out-of-Distribution Data},\n volume = {abs/2101.06639},\n year = {2021}\n}\n', 'references': ['36dfaba17bd6c23d869143838c49e0041f10e7fa', '365fb36b15f13c0c69596a9fc98ddcaed3fe739c', '18939eadc9c4460c8385e0591cde214a1ead067b', '65c63d4143b70ba718c423743bb1a4c43513e7fc', '2d75cf1dc599d7274fa57a02af5c2da2747db36c', '8733fe2371b615609b04e2e910b1ecfa8e77cbc2', '20ba55ee3229db5cb190a00e788c59f08d2a767d', 'ec570b827cf0cd132da7ebd37537df4f0bb7f877', '91a05cb84f1c7dbb0354da2ff11ae92549152435', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', 'bceeb52a9d4a4127d6664eea4870e8a60b378eff', 'bc1138738f24c4a23d865d7786fc4c8229e4662a', 'ed17929e66da7f8fbc3666bf5eb613d302ddde0c', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '3f7bc67330b3eff749459568e7995f0017dfe645', 'aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'a25b63a6a0071d7d88ff4671c1fd40f320a08533', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', '2d8c97db4bae00ff243d122b957091a236a697a7', 'f986968735459e789890f24b6b277b0920a9725d', '1b9c6022598085dd892f360122c0fa4c630b3f18', '804fb9542f4f56e264dd2df57c255a9a2011c00f', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', '651adaa058f821a890f2c5d1053d69eb481a8352', 'e8da4ff1519011ed018202bb96dee4b611f5d842', '36653f8705b56e39642bcd123494eb680cd1636b', '4feef0fd284feb1233399b400eb897f59ec92755', '33d682c52eb24875c556ec007bc38068d3e682c0', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '3c57a1aa483d8bffe1339914b80d2913f2dc8376', '54ddb00fa691728944fd8becea90a373d21597cf', '16aa01ca0834a924c25faad5d8bfef3fd1acfcfe', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '1c4e9156ca07705531e45960b7a919dc473abb51', '77f0a39b8e02686fd85b01971f8feb7f60971f80', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'e33cbb25a8c7390aec6a398e36381f4f7770c283', '583b55367f787eb0c4e295707b642e63547b9806', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '54d2b5c64a67f65c5dd812b89e07973f97699552', '144adacded5ed56c35a5f157fe231a0459620ec8', '1729f731482a628177a0fb81050966514c385e5e', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'df24c3011fc42b72195e876ce052a0a072a1d923', '1ea82cc13f6b7352943aba6c987e3895e5161b9b', 'f6a883e5ce485ab9300d56cb440e8634d9aa1105']}
{'paperID': '3485aa52bf7a8310e80e139907d8c2e649e3ab66', 'abstract': 'Though remarkable progress has been achieved in various vision tasks, deep neural networks still suffer obvious performance degradation when tested in out-of-distribution scenarios. We argue that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization ability of deep learning models. Common methods often consider the feature statistics as deterministic values measured from the learned features and do not explicitly consider the uncertain statistics discrepancy caused by potential domain shifts during testing. In this paper, we improve the network generalization ability by modeling the uncertainty of domain shifts with synthesized feature statistics during training. Specifically, we hypothesize that the feature statistic, after considering the potential uncertainties, follows a multivariate Gaussian distribution. Hence, each feature statistic is no longer a deterministic value, but a probabilistic point with diverse distribution possibilities. With the uncertain feature statistics, the models can be trained to alleviate the domain perturbations and achieve better robustness against potential domain shifts. Our method can be readily integrated into networks without additional parameters. Extensive experiments demonstrate that our proposed method consistently improves the network generalization ability on multiple vision tasks, including image classification, semantic segmentation, and instance retrieval. The code can be available at https://github.com/lixiaotong97/DSU.', 'bibtex': '@Article{Li2022UncertaintyMF,\n author = {Xiaotong Li and Yongxing Dai and Yixiao Ge and Jun Liu and Ying Shan and Ling-Yu Duan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Uncertainty Modeling for Out-of-Distribution Generalization},\n volume = {abs/2202.03958},\n year = {2022}\n}\n', 'references': ['e5b2e2a284db5ba7c2c011daba9769d2c56b6586', 'a0bd9ac2544efe6624a8d2861471e1df9c485e7d', '498b323fc8d2eaf9e5a29a8b33d18971c3ed1408', '94001e6bdf94fd61be3fba6ab9e6e77fbd888867', '7ffbc531bbb0f1cf38b47e1d728a0e297f8b4dec', '4f6eafafc9563a5b904535078df7e74afe39ef59', 'cd8394d8b6679bb446cf154a6d123cf9a00e561e', 'b249fe4e5e2bada6655ce5d61e7f50da5d471cb4', '085907c9b2bfbf39bcaf6fe3d16bd1dadcef5af5', '1403fd998f477256482474a0876d4341075ebd4b', '639b07626d140513a2aeac68ff52c3bb6db90bed', 'f6e289df5734c06d235cc9eb71585955c27f2005', 'b27ad18e20d27efe8a9fbc54b1c2dcef8b2da19f', 'c57d53767654948647cbf4a68e459851e56ea553', '7167f44f61fe3306bb49e8c834e7211bf8c495e5', 'dc0092d06ab76465431edfd51b08d823b7d1ff3f', '3f88e25db654ea030c2241f540f49b56b2d727ec', '09472ff0d3c3f975ef1fdc02cfb1605d3d4275fa', 'dddc3f4a6d2d668eb9a840ca8ff7c2d83366a507', 'd0d955edbc44067e7fb469d1884eb59236dc770b', '1f49f005859e966af8d8fcfb2ec3e880e5afde94', '46803220e5f6b636b65ef0ff87c0c9c4b95dec31', '22b52d3f18eaf43993a3a91053f5efe6267144e7', '11babff42b5bf9841ebb87781bfc21f74acb3d28', '9fcd6d95bea171c9a5156cf9d4b6c8654a7b684f', 'ed17929e66da7f8fbc3666bf5eb613d302ddde0c', '53ea629b5933430eafa98692edf39ab2bd737d09', '49b64383fe36268410c430352637ed23b16820c5', 'a60540a8407fd117fd8e6857d4728e661f53dcc8', 'd39a5ea57b1c242a0450385523fd3471b172458c', '1b59eea8ec4684381a885b59acd09c9151a49487', 'ba6ba7f488c1ece0803f4b9e1c83a3196d061610', 'cabc42832388b1995d1f815f9fc4253f3f593993', '4feef0fd284feb1233399b400eb897f59ec92755', 'b39b45a59c27a0cb3214d5a84547f54722d40c69', 'b1e7f07965a53491690bd31fdab626bfac606eae', 'b8ebda42e272d3617375118542d4675a0c0e501d', 'be0ef77fb0345c5851bb5d297f3ed84ae3c581ee', 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518', '85aefde69e916523d9587b6abd01419420039474', '4d9d25e67ebabbfc0acd63798f1a260cb2c8a9bd', 'cab372bc3824780cce20d9dd1c22d4df39ed081a', 'c8c494ee5488fe20e0aa01bddf3fc4632086d654', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'e24c261f5cfcd58a595efb7ca684aedcb2a2f22c', 'f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '5cb309a35313308d0e75e409be84c176dc64c61c', '66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '4609f6bdc3beab00c9beceaa12dd8101fefe6f1c', '9642a175637a400b425f0ac0cb6a2b067cc8fe6b', 'e7c642fbbe31fea90cf3c643c380e354c20d9aa4', '34f25a8704614163c4095b3ee2fc969b60de4698', '1c46943103bd7b7a2c7be86859995a4144d1938b', 'f6a883e5ce485ab9300d56cb440e8634d9aa1105']}
{'paperID': '29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d', 'abstract': 'When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer—the “head”). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head—this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).', 'bibtex': '@Article{Kumar2022FineTuningCD,\n author = {Ananya Kumar and Aditi Raghunathan and Robbie Jones and Tengyu Ma and Percy Liang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},\n volume = {abs/2202.10054},\n year = {2022}\n}\n', 'references': ['fccce60283729934467877f0730317c3e9fcc61e', '9289826beb6206eeaf500105f7329d6d5a495d8a', '96ea07447d2f9adefe03852a878517a2a6d45b96', '106cc848e51ad0938e73c1b3b2ebb90d4bdea143', '4b1db6ebbdfcfe8ef67c5db511b6ad169fcc8f7f', 'd5d375628b5ed09a4e40c54eccdd1ae97a3a31fc', 'd4d37dfff71691cda8b4ff2314884b172fb5671b', 'ffdbd7f0b03b85747b001b4734d5ee31b5229aa4', '739ceacfafb1c4eaa17509351b647c773270b3ae', '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', '602f1f77058b67212d1c03559ddf0e86c23a919d', 'a81912ad6c66e999f514927d5b05a9218d1d55af', '1fc4470c1766aea4c435015f5ef873b066f50121', '40848b41ed8c9c255ecd8a920006877691b52d03', '0f4374e62ae889dd2a35dc4c97b9a0510146fa87', '753c96b2b770272e2f30d6247b228c2282224440', '0ab72e9a9c4c89d92898d5f883563fac3b1a1260', '1e76e2fbf27198986271a672f462dc38d790d00f', 'b88c11922cac84e5ea902f82d27ae21c3dda2e04', '569ef4e3c9f5ae968fc94000c12d212a2b679907', '022622e024890d6e044ac50e2da6b44c59bdf418', '9e0aa0df70aedd3fda5f37fc3210eacaa5395ebf', '6e2d24dbf959aeb855926430bf1cb476346719b3', '7438c34a128c13e811aca2e599028bb3760e1816', 'a1b8a8df281bbaec148a897927a49ea47ea31515', '3f8e190773a8898ef296da67b646e40dca1374cb', '02841af780570666389643f2815460a10d9ae286', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '292475b9280d21ee5ad1a81ef6dac5244efd364e', 'add2f205338d70e10ce5e686df4a690e2851bdfc', 'ab70853cd5912c470f6ff95e95481980f0a2a41b', '024462d02d81b882b782df5a4270c13f4db10d7a', 'd01fa0311e8e15b8b874b376123530c815f52852', '41c0be3adfd33cb6d0cb24c6fb1de109929276ca', '45557cc70cd6989ab6b03e5aeb787e34299099f7', 'c563c0e06684f42bc5d76dfc7304581c11312393', '455a8838cde44f288d456d01c76ede95b56dc675', '369eb43e6a1ec995f741b621f3e2be88dff79183', '4ae0c4a511697e960c477ea3e37b3e11bf3e0e02', 'c232afff29037ca4058f2b45e987cd12081bae0a', 'bd949f2a0c23742427d8886e43c1d8c98c1865ea', 'e58ef5ecbd163465658025a83729e27ade57e61c', 'f8a5278d4142215b33b516db5df1d9eb0d1d066e', '8659bf379ca8756755125a487c43cfe8611ce842', '4e0bb8c1c683b43357c5d5216f6b74ff2cb32434', '29ddc1f43f28af7c846515e32cc167bc66886d0c', 'aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '3217278e346fefbd34f0727321059c7ea5792612', 'e87b5f4c64056431dfc62ebff0f23d9c94252598', '0e662587c790e5d11475f5b0bce3638b4d1effa0', 'f445493badf53febbaeab340a4fca98d9e4ab7f7', '8a8cfa45b4c0d071fbffa091c02670b19c94b693', 'c01e8108b4ca599eb2b1e6f26b2ecf1c42e323a9', '05447ad7c0bc55f8b3766e23da1578bb13f46bbf', '6ea8cbf0cc4cda3d981348a279b464524a8485cc', '3febb2bed8865945e7fddc99efd791887bb7e14f', '5f1141287c577f2e45f8c35a5fd30cfb91311257', '1e077413b25c4d34945cc2707e17e46ed4fe784a', 'e3b7201deb277dbc1ce1f1849cf099c71b2ff3a1', 'a588d38ec81c0337b445931eadf6f443aea13380', '1c012e5b3ddb8a60420e8f92162d32ad135f9ba1', '4e8917e73e02c76d55ded62e43541d44684a4c8a', '263210f256603e3b62476ffb5b9bbbbc6403b646', 'cad99e8f78618510eb37394be948c8e993af57ed', '4f975da00a5b2a2f7236e34edcb7274e5fdab937', 'b6b8a1b80891c96c28cc6340267b58186157e536', '3504565c8c3cbae18a50247604f07a7e01cde801', '4b675d8f63888d7d6d7d77a0834efa5eaded64c5', 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', '99c970348b8f70ce23d6641e201904ea49266b6e', 'be9a17321537d9289875fe475b71f4821457b435', 'b71ac1e9fb49420d13e084ac67254a0bbd40f83f', '5bcb1fcf2004b6994e7a8068aa2aead8a55596fd', '53d8b356551a2361020a948f64454a6d599af69f', '444d70e3331b5083b40ef32e49390ef683a65e67', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '2815a5e7ba661ae278aa7c19e08ac884cde17bf7', 'abstract': 'Reliable out-of-distribution (OOD) detection is fundamental to implementing safer modern machine learning (ML) systems. In this paper, we introduce Igeood, an effective method for detecting OOD samples. Igeood applies to any pre-trained neural network, works under various degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD samples. By building on the geodesic (Fisher-Rao) distance between the underlying data distributions, our discriminator can combine confidence scores from the logits outputs and the learned features of a deep neural network. Empirically, we show that Igeood outperforms competing state-of-the-art methods on a variety of network architectures and datasets.', 'bibtex': '@Article{Gomes2022IgeoodAI,\n author = {E. Gomes and F. Alberge and P. Duhamel and P. Piantanida},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Igeood: An Information Geometry Approach to Out-of-Distribution Detection},\n volume = {abs/2203.07798},\n year = {2022}\n}\n', 'references': ['4c7d664761c359cffd20c9d555031271ec67ab3c', 'e0e19d99bf487ba7a471af23acc96542ee19ba36', '14014c024674991149f3ecf9314c93f7e029ef1a', '06572d93ca5edee2dfa651b6a16c1d5d525a866f', '2be5c2f23b997e1cb22479dc1d5d1735997ad4be', '1b4a54670bb4fe15bcb0d06de0391d5b6d10ace2', '703ffd7ab0bdfcb091400ebb9c7b92446204831f', '35b966347dae2f0d496ea713edf03a68211838a5', '0930f7234a861c34050685070e117c295ebb184a', 'bfc65dba18cdbe559f87dfe2cb8848452a2ee8d3', '94192bcdf3507e3543910c03b16bd06c5338fd47', 'c2eff53cc9db9eaca8d9ffe06f2d618b0e360c9d', '5771af144ce1d4d783a0af70c190a74e5123d0a0', '3368fc9f59480f3b7b755b66dee0488916774f1f', '549e8e5eea04b301cbb805f5502afffef492d344', '47cbd8cfd6fcba83d3b3714faef480260bc9d5de', 'ae9bf201f128cabaa4350b54ff6607525c736cd5', '1aeeaf9c03897d68fc09a8d929a3366aa8ecf9b1', 'af5b1a35271efd17ff3d5ddd152bacc96dff0e81', '5fda9d3b9d658b64139b1cb077d98ec5a580b880', '33fc67c7425c669bac36b4dba7fd427e55d309fe', '925182b91f51f8f2b747f7829e9d25ffc2729e5d', '1eb7f46b1a0a7df823194d86543e5554aa21021a', 'a25b63a6a0071d7d88ff4671c1fd40f320a08533', '50577b1a6aa575f401bce336e015e7a4eaf7edcb', '2d8c97db4bae00ff243d122b957091a236a697a7', '6507909a8f77c88144c3a67b9336bd1c85e84cac', '6e1f7b326dd795377a631cf76fc5e5df05f1dce2', 'e31fa9510047c0df23fb4dd37ee7c70783a3fa60', '5ea5224de74847ba3a5d15c718af8f04fa50efe7', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', 'f986968735459e789890f24b6b277b0920a9725d', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '547c854985629cfa9404a5ba8ca29367b5f8c25f', 'e163a2e89c136cb4442e34c72f7173a0ff46dc79', '802168a81571dde28f5ddb94d84677bc007afa7b', '6ff2a434578ff2746b9283e45abf296887f48a2d', '5694e46284460a648fe29117cbc55f6c9be3fa3c', '81acb0286de78f6d14b68e529a1a37d6ae658d3d', 'e86f71ca2948d17b003a5f068db1ecb2b77827f7', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'ed5ab1cff7dd3a902eea4a811b15aa5ea3a36b30', 'd094fb0af5bc6a26fa9c27d638c4a3a0725d8b5c', '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', '3433627f803953280b66ae1576d083fc9a68385a', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '9acc51b06f54b07836fad4cc24633187dc21317f', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', '908091b4a8757c3b2f7d9cfa2c4f616ee12c5157', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'c62043a7d2537bbf40a84b9913957452a47fdb83', 'dbbd5fdc09349bbfdee7aa7365a9d37716852b32', 'e4a742a4f0585b4e4069726f6628f4d4285a0827', '384ce792cf2b2afbe001f2168bfe7d5e7804c736', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', '3e8c2d811bf231d4744b8df9019f4676223f9bae', 'fa97c2238a16e9226f386ecffe22095e3d3d9dff', 'bd2e048c676ad778351bd7d7660240a978422117']}
{'paperID': '9b799db46342be286610f79a924079f5757976c3', 'abstract': 'We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, they usually underestimate uncertainties of out-of-distribution (OOD) samples leading to over-confident PIs. Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Additionally, PI3NN does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples. Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification.', 'bibtex': '@Article{Liu2021PI3NNOP,\n author = {Si-Yuan Liu and Pei Zhang and Dan Lu and Guannan Zhang},\n booktitle = {International Conference on Learning Representations},\n title = {PI3NN: Out-of-distribution-aware Prediction Intervals from Three Neural Networks},\n year = {2021}\n}\n', 'references': ['172b266f190d89ec6e2164560eba3707e8936e6e', 'f2ef185250e909026bf33024f59e2dcc545e3bca', '9eebd3c7971a239cf69a0358563f397bd8a8f99c', '7e4e71d25edb6ac73376c9ea28d835dfaa5728de', '14703aaffc186e98bdf3468c3ee70ddce0ccd90d', '3995dab2b1c3f0a036352e47a324b0edb8542b4f', 'f8a54ba839f6194198b4886097169a53905fbb37', '0c0a97ebb11b9773900f682618815443b2b5153a', '7a0ba4f62a1d98ad5f1e0bab1bdd78dba9ca8ecc', '802168a81571dde28f5ddb94d84677bc007afa7b', '30423f985355d74295546f1d14ed2ddd33cdef99', '9375729d21a344a5ccccd5f53556ddf90b957cd9', 'b0e6fd0c22865fa8ca7a02d18681087052c9f6c5', 'f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6', 'e301beb0e17805dbabf5add06d99c53e8703ea34', 'bccb2f99a9d1c105699f5d88c479569085e2c7ba', 'fad253f283561cd4cd19a732da5288ab1820a21b', 'c18e584b430cd69451a0c3149639dc88f2d2ac3e', 'c669bf2e309831f416226f464d5f413c03494d51', '1f004dc69520636c8daee383ac3a7c9c5cce133f', 'd0188b1ce2f3da36fe64a36901248d16d17ff218', 'b959164d1efca4b73986ba5d21e664aadbbc0457', '9f873fecd24e0cfb800249a30ecd8e3b3155e709', 'c09db7439505f49a0958f68e782df94b3807341a']}
{'paperID': '164b72b345a4cb03f62547abf62a033dcbd784ae', 'abstract': None, 'bibtex': '@Article{Lu2022InvariantCR,\n author = {Chaochao Lu and Yuhuai Wu and José Miguel Hernández-Lobato and B. Scholkopf},\n booktitle = {International Conference on Learning Representations},\n title = {Invariant Causal Representation Learning for Out-of-Distribution Generalization},\n year = {2022}\n}\n', 'references': []}
{'paperID': '31278cec03fa87b45a02f43d275dd92b678fbc5b', 'abstract': None, 'bibtex': '@Article{Jiang2022RevisitingFG,\n author = {Dihong Jiang and Sun Sun and Yaoliang Yu},\n booktitle = {International Conference on Learning Representations},\n title = {Revisiting flow generative models for Out-of-distribution detection},\n year = {2022}\n}\n', 'references': []}
{'paperID': '440c098ce8c0ff2042543d3e4188ebb95acdb75a', 'abstract': 'In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.', 'bibtex': '@Article{Puli2021OutofdistributionGI,\n author = {A. Puli and Lily H. Zhang and E. Oermann and R. Ranganath},\n booktitle = {International Conference on Learning Representations},\n title = {Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations},\n year = {2021}\n}\n', 'references': ['4a9244d0fc9b2b87eca5fa7579835f4a05eb8866', '6aecc93c2d61da073b70dec19795172ca1ff3405', 'de656b3de564933e1ef2351107e2369c0deae6b4', '14d8d5e612028842d35cb5865f8fecf9ecc63c00', 'bfc7bd9442c6a7ecbf0d4f2bf4e23a8861792c01', '340bd0cfeeab5d117a9fdffffa9e05fe2afaf64f', 'a5b1169e536b806c2344261ebbbe3d97bc6e1cdb', 'a1506424cf3a24f50b7291db1e70927ab8e17bb6', '00325cb5408da77827951abd3fa93ec3bd019608', '115bbc50cf015990dc6f35bc8f99575ad2a68253', '7d990e5cc46cf2066f1fcf0d30223033858c5995', '9207480a5cd071a3e85f408082b09283413cbfa5', '51e267ea751f74173358dd7c87ac80a0a3a71322', '4f2be2daf44475b8d652c6d456668cab94683b9d', '396d97307650347d154bf599a9b168828e67baa8', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '2b5a8e6fd214e1471c5d61bf80e4ff9f774765b6', '08cb7d416bcd1b1e4d6492a0cd0b01424abd9515', '26e858cf3c82b66bbd539bb79356b0e885bdc694', '1b04936c2599e59b120f743fbb30df2eed3fd782', '60d7279168e1e1c9e151b68a3a9fc94ad5137ce5', '3621fff4a1c791901ea4a1359c10575193ec712d', '193092aef465bec868d1089ccfcac0279b914bda', '4efafeba13d7e22238583bab2ed3d5b9a3359465', '31e77344130d14f56999be17d48af6d6d7fb3b6e', '753b7a701adc1b6072378bd048cfa8567885d9c7', '6be216d93421bf19c1659e7721241ae73d483baf', 'bff67430f3c2413a63f0e7e1f7276004333c1f15', '8fe578a3daede41faea142eb414cb8c95d5adbf6', '89a816719613e220a64ab2590c938c23bbfe187e', '74d8eb801c838d1dce814a1e9ce1074bd2c47721', '53bd10108f32ff2c98f333c97cf35570703239a3', 'b661520bf0061b7d96ccf12016e351dd3a6ee780', '00192b5adb3dea34a60ca7ebe58b48fc44e80efd', '2bdf35c191cb4df4dcfb7fb18d6582cd032d1c40', 'fbda91cfacd2b792794fb726e9417aef58480c72', 'ba6ba7f488c1ece0803f4b9e1c83a3196d061610', 'c7330852a07170cd0e6990f5fbde5fca12b6ccd6', 'b39b45a59c27a0cb3214d5a84547f54722d40c69', 'd65ce2b8300541414bfe51d03906fca72e93523c', '9a334566b79bc6c6906e2b5285d5ea50b9b99479', '802168a81571dde28f5ddb94d84677bc007afa7b', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', 'f1bb50162b731dfd41bdd3648ba5239579420ac0', '90848562905d26873b57bbf3f2f98319e38e5bde', '9f62067945d991cd78a62cf647de17f01d1b54d3', '96c6bc559b79d8fd518f431c707e8b44ce3bc4de', '922dc3bf6458ebab934608d064374d95ea323cd3', 'ba56155267c29d1b540e089df044db8f22c55a9a']}
{'paperID': 'cfd5982c81538f128bd7aa755929c8c0f4ee2bbe', 'abstract': 'Out-of-Distribution (OOD) detection is essential for safety-critical applications of deep neural networks. OOD detection is challenging since DNN models may produce very high logits value even for OOD samples. Hence, it is of great difficulty to discriminate OOD data by directly adopting Softmax on output logits as the confidence score. Differently, we detect the OOD sample with Hopfield energy in a store-then-compare paradigm. In more detail, penultimate layer outputs on the training set are considered as the representations of in-distribution (ID) data. Thus they can be transformed into stored patterns that serve as anchors to measure the discrepancy of unseen data for OOD detection. Starting from the energy function defined in Modern Hopfield Network for the discrepancy score calculation, we derive a simplified version SHE with theoretical analysis. In SHE, we utilize only one stored pattern to present each class, and these patterns can be obtained by simply averaging the penultimate layer outputs of training samples within this class. SHE has the advantages of hyperparameterfree and high computational efficiency. The evaluations of nine widely-used OOD datasets show the promising performance of such a simple yet effective approach and its superiority over State-of-the-Art models. Code is available at https://github.com/zjs975584714/SHE ood detection.', 'bibtex': '@Article{Zhang2023OutofDistributionDB,\n author = {Jinsong Zhang and Qiang Fu and Xu Chen and Lun Du and Zelin Li and Gang Wang and X. Liu and Shi Han and Dongmei Zhang},\n booktitle = {International Conference on Learning Representations},\n title = {Out-of-Distribution Detection based on In-Distribution Data Patterns Memorization with Modern Hopfield Energy},\n year = {2023}\n}\n', 'references': ['8b1233107bff00355d4d1656795ec62d4f85e523', '7f9760a76e9cf424da0b72d42f75594cefc4a329', '0fe615dc0a422100e85cfb7e26c9306c481f6c75', '35b966347dae2f0d496ea713edf03a68211838a5', '804a6d7c23335bbca6eec3b7d3c8366dcbe395a5', '549e8e5eea04b301cbb805f5502afffef492d344', 'ae9bf201f128cabaa4350b54ff6607525c736cd5', '1322719978980a831e1aee78aa80a141379c44dd', 'b9bd435b65d8214f1bcab9268ac2fce509cfffe6', '25e82c473fbbc54433ac2a70f10efdb9423fbf5a', '2d8c97db4bae00ff243d122b957091a236a697a7', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', 'f986968735459e789890f24b6b277b0920a9725d', '431ba9fae8fccad1665979d455c6307786e47318', '05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '7d39d69b23424446f0400ef603b2e3e22d0309d6', '6ff2a434578ff2746b9283e45abf296887f48a2d', '5694e46284460a648fe29117cbc55f6c9be3fa3c', 'e86f71ca2948d17b003a5f068db1ecb2b77827f7', 'ed332c92664cd64843a7ba9373d992e9547230f6', '1c4e9156ca07705531e45960b7a919dc473abb51', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', '3433627f803953280b66ae1576d083fc9a68385a', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '908091b4a8757c3b2f7d9cfa2c4f616ee12c5157', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '24b9eebe49cf7e00cf50cf7b7d9243386a23fe7c', '98b4d4e24aab57ab4e1124ff8106909050645cfa', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', '1c46943103bd7b7a2c7be86859995a4144d1938b']}
{'paperID': 'eba7eb5d82c08ad1ba700afa08153168993de963', 'abstract': 'Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.', 'bibtex': '@Article{Qiao2023TopologyawareRO,\n author = {Fengchun Qiao and Xi Peng},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Topology-aware Robust Optimization for Out-of-Distribution Generalization},\n volume = {abs/2307.13943},\n year = {2023}\n}\n', 'references': ['2bf13c1ad79954bb24a65767514e2255e7988dfc', 'd590b67abd90934abc7d75224eb3594064383afc', '868b2b07c24bf9f69ecc8847ce5c212bb5d68dbd', 'c4c0de6755c06f4167ffd2cc24b049c7902a10ca', 'dfe5c82d7289468f51f35b5a4664b268ecc7558e', '525dd120c0b5808ddcbbf703677b46346fb0729b', '2c0ccf919d5347b87677e7a16a3ba5e555f51710', '1f32677b1a61c30be6c1ba443df0663eee2b19a4', '693d3fd92c6cf825cfe988cb32cf92e733d6230a', '40848b41ed8c9c255ecd8a920006877691b52d03', '29877659966f5ca2f198712a313cc653789edef1', '07d251a8d721b5f3da6cc8a92e75840b563927f2', '00325cb5408da77827951abd3fa93ec3bd019608', '1e76e2fbf27198986271a672f462dc38d790d00f', '66488a38c3bae5d928bb22aa615fd0e64ccac62b', '99bc80389f5957c7472906a5970e35a46281b469', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', 'e184f730775262869952b0b6de99d0967147301a', '89b95fb53727ee45be440045efef656718517c4c', '3621fff4a1c791901ea4a1359c10575193ec712d', '4aebc6a6c1390c62e37a6bf0c4ddd390d7bf3983', '1d26ab851edfe193213ab4dd7fa91167f14ff8d4', '193092aef465bec868d1089ccfcac0279b914bda', 'e4bf23cfe18acdb9d9b6a8dbdd54e447358e6f63', '753b7a701adc1b6072378bd048cfa8567885d9c7', '72e0763428b55a788ffa16b7d206f8d3e20a9db9', '159395b0f7a2b9ea04f9a758d18887bcb970ee78', 'a1fb7236d104ae0343c1a09e3590ee2283483240', 'd81277895554106a2d42049453c1cd5a4fe6a1e7', '2d15a7546c16d5821ffa8f769eb7ec18e435e64d', '12a17d5dfd222abfa47e121b734c93ae988d05b4', 'b1e7f07965a53491690bd31fdab626bfac606eae', '06268b47de80e55609d31c0fcc7d21ca88529bf4', '8c18d5afe6b8ab0c47e46a3d11d3b3cc0d404492', '10478ed2892f24c49ca0be1588c1c0e29841abb1', '96167ed3ebc9a2c3270f6ae96043e6f086eed4de', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', 'cf6f8ff055b3dc67d38a571be00365a6b121fc2f', 'ef4481cbc18c91e7bf0e53693bb77f3608743626', 'e4350e816a350662ddb5f9ef92437aa8f3fd44f6', 'f4825634e91a1934e00c2bf29c29b5f37696bfff', '012eb8da8885060d22c2598e287e61b25cec2121', '385197d4c02593e2823c71e4f90a0993b703620e', '8d56d4bc69a8c562434b9a129542bb79e9d6f1d6']}
{'paperID': '195d86d6b6a8420e9553fdbfc67cdfa4c87179aa', 'abstract': 'Deep neural networks have witnessed huge successes in many challenging prediction tasks and yet they often suffer from out-of-distribution (OoD) samples, misclassifying them with high confidence. Recent advances show promising OoD detection performance for centralized training, and however, OoD detection in federated learning (FL) is largely overlooked, even though many security sensitive applications such as autonomous driving and voice recognition authorization are commonly trained using FL for data privacy concerns. The main challenge that prevents previous state-of-the-art OoD detection methods from being incorporated to FL is that they require large amount of real OoD samples. However, in real-world scenarios, such large-scale OoD training data can be costly or even infeasible to obtain, especially for resource-limited local devices. On the other hand, a notorious challenge in FL is data heterogeneity where each client collects non-identically and independently distributed (non-iid) data. We propose to take advantage of such heterogeneity and turn the curse into a blessing that facilitates OoD detection in FL. The key is that for each client, non-iid data from other clients (unseen external classes) can serve as an alternative to real OoD samples. Specifically, we propose a novel Federated Out-of-Distribution Synthesizer (FOSTER), which learns a class-conditional generator to synthesize virtual external-class OoD samples, and maintains data confidentiality and communication efficiency required by FL. Experimental results show that our method outperforms the state-of-the-art for OoD tasks by 2.49%, 2.88%, 1.42% AUROC, and 0.01%, 0.89%, 1.74% ID accuracy, on CIFAR-10, CIFAR-100, and STL10, respectively. Codes are available: https://github.com/illidanlab/FOSTER.', 'bibtex': '@Article{Yu2023TurningTC,\n author = {Shuyang Yu and Junyuan Hong and Haotao Wang and Zhangyang Wang and Jiayu Zhou},\n booktitle = {International Conference on Learning Representations},\n title = {Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection},\n year = {2023}\n}\n', 'references': ['8b1233107bff00355d4d1656795ec62d4f85e523', '3b0284d501e9b1b6c199d8b07c6826a165c4b4f2', '80ac4250b3cba55228684756acb55922042d7aaf', 'a50495072b04b0bbb890917dfb2412fb50d3e8e7', '3fdcf2d8fe58dcbc2c353e5974e2030f8281c799', '3471032918f2e6fd65677ad491a79ffa14b1c289', '0fe615dc0a422100e85cfb7e26c9306c481f6c75', '2c0f4711c9c124a8dc056eaee82a2ca5ef276da8', '27875e4bdc53e20d9b555927097816244f5cf86e', '35b966347dae2f0d496ea713edf03a68211838a5', '076af769eeb1ff3e9d1d398b3eed052ecbeb9aae', 'e1bb329621de73d08c47beae9b5439a1c244eb1a', 'e1f2588ee4a7503aa2cf37f338ed50a92b0d0e65', '94192bcdf3507e3543910c03b16bd06c5338fd47', '982e937583efffc247bab32a8f0156863beb22a6', '5771af144ce1d4d783a0af70c190a74e5123d0a0', '47cbd8cfd6fcba83d3b3714faef480260bc9d5de', '47c528344fedb6cb67a38e43d095b41c34715330', '1fad7869d48782062ea345b47081324b21e52f5d', '07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b', '35aebe08b34e5cb0d012a16563e5c3f6fd17a906', 'a95d102ed27f62cf328ab7c5a8732502f2b69012', '46d8c9e2dc9c12615eb5f6813d18f967d61c7e0d', '49bdeb07b045dd77f0bfe2b44436608770235a23', '8000a4a63ac97ffb84917f910e2ce747e48d409f', 'cec3bffdb0968cd820863005af54cc519704c24a', 'cfb40a6546904f03e74be62fe3183cea61ad5ef9', 'a25b63a6a0071d7d88ff4671c1fd40f320a08533', '3217278e346fefbd34f0727321059c7ea5792612', '2d8c97db4bae00ff243d122b957091a236a697a7', '7a96002fe623c7b138584b1161a9bf2eb0a41876', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', 'f986968735459e789890f24b6b277b0920a9725d', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '81d5740cac256489978c2751e867128e97620eae', '7fcb90f68529cbfab49f471b54719ded7528d0ef', '6ff2a434578ff2746b9283e45abf296887f48a2d', '1c4e9156ca07705531e45960b7a919dc473abb51', 'd1dbf643447405984eeef098b1b320dee0b3b8a7', '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', '3433627f803953280b66ae1576d083fc9a68385a', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', 'be9a17321537d9289875fe475b71f4821457b435', 'd748b5e803f595c3a4c154cd87124556e00f7eb2', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': 'c7e4da026f47339697a522508893f2fa261b52ea', 'abstract': 'In this paper, we define, evaluate, and improve the ``relay-generalization\'\' performance of reinforcement learning (RL) agents on the out-of-distribution ``controllable\'\' states. Ideally, an RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. For example, a self-driving system should be able to take over the control from humans in the middle of driving and must continue to drive the car safely. To practically evaluate this type of generalization, we start the test agent from the middle of other independently well-trained \\emph{stranger} agents\' trajectories. With extensive experimental evaluation, we show the prevalence of \\emph{generalization failure} on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\\% failure rate during regular testing, failed on 81.6\\% of the states generated by well-trained stranger PPO agents. To improve"relay generalization,"we propose a novel method called Self-Trajectory Augmentation (STA), which will reset the environment to the agent\'s old states according to the Q function during training. After applying STA to the Soft Actor Critic\'s (SAC) training procedure, we reduced the failure rate of SAC under relay-evaluation by more than three times in most settings without impacting agent performance and increasing the needed number of environment interactions. Our code is available at https://github.com/lan-lc/STA.', 'bibtex': '@Article{Lan2023CanAR,\n author = {Li-Cheng Lan and Huan Zhang and Cho-Jui Hsieh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories},\n volume = {abs/2304.13424},\n year = {2023}\n}\n', 'references': ['2d9b75da46989bed3cffe8f51e050d674d5b5d15', '4dac673eb5c4849d3cbe710ab06c64c92b1457c7', '17609f260d7deb836702029521c7f120c61ad6a9', '43b8a2a4c64f80bc90f6c1f73bb77cd7bb749584', '2d84f9f1483a73acff0f349826c6bc0ac4025075', 'ae6e8f470bc28f14741a6d7b94afc29f52efc90b', '1a627d2a169d71563109546da590a7cceb0b349a', '93b2788fb1f2aed0e545d9f9d7dca1c05a63208a', 'd5ba07e4ef58bbbe8edee8c08ca02477e1b7af58', '28db20a81eec74a50204686c3cf796c42a020d2e', '1301e9d11b728268ed1ff3f1a9adc155308d5250', '5e7bc93622416f14e6948a500278bfbe58cd3890', 'f1697ce4dddb58533d7d3f937fed74807d46edb8', '9bb3d04c94a09e92375ae5377ab5187e1af3f6aa', '690e880ef82a9665db56c545880ae1d8307d58be', '20b9c2ea1a49ed7789b99ae4c84b1b517b65bff5', 'd81dd2dc0ee02d996763f3ea1703eaff681485d7', 'c520bf47db3360ae3a52219771390a354ed8a91f', 'c0dccd9be123057ec82a6747d8fec9cc34699a6d', 'ef2bc452812d6005ab0a66af6c3f97b6b0ba837e', '53ff8d142b3ce64ac9649450ddb7be12cfaefcd6', '1b19f433a3e8497e9d9bd67efb108521d16b5b85', 'caea502325b6a82b1b437c62585992609b5aa542', 'a622be547caf0b1223626de5e69377c20ae11265', '39b7007e6f3dd0744833f292f07ed77973503bfd', '4debb99c0c63bfaa97dd433bc2828e4dac81c48b', '4b0e8a4df3605d5e22c9eacc3cb360ff08eb8c4e', '811df72e210e20de99719539505da54762a11c6d', 'af10f3c1c0859aa620623f760c8a29e78f177f7f', '0af8cdb71ce9e5bf37ad2a11f05af293cfe62172', 'ebf0615fc4d98cf1dbe527c79146ce1e50dce9af', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '7e9c1e0d247b20a0683f4797d9ea248c3b53d424', '32ceb28e45a445df4d89df281bb0e3ab5aab1a2a', '9c4082bfbd46b781e70657f14895306c57c842e3', 'c8c16a56d2a9520197da9a1546f517db5f19b204', 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1', 'cd5a26b89f0799db1cbc1dff5607cb6815739fe7', '5296e2f5339bb9099d9c3805c37f77f62e3fec40', '42edbc3c29af476c27f102b3de9f04e56b5c642d', '1c46943103bd7b7a2c7be86859995a4144d1938b']}
{'paperID': '0dcf24bb23ce5bc17aab8138903af5f049a4db91', 'abstract': 'Learning on graphs, where instance nodes are inter-connected, has become one of the central problems for deep learning, as relational structures are pervasive and induce data inter-dependence which hinders trivial adaptation of existing approaches that assume inputs to be i.i.d.~sampled. However, current models mostly focus on improving testing performance of in-distribution data and largely ignore the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the prediction is overconfident on them. In this paper, we investigate the under-explored problem, OOD detection on graph-structured data, and identify a provably effective OOD discriminator based on an energy function directly extracted from graph neural networks trained with standard classification loss. This paves a way for a simple, powerful and efficient OOD detection model for GNN-based learning on graphs, which we call GNNSafe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a learning-free energy belief propagation scheme. For comprehensive evaluation, we introduce new benchmark settings that evaluate the model for detecting OOD data from both synthetic and real distribution shifts (cross-domain graph shifts and temporal graph shifts). The results show that GNNSafe achieves up to $17.0\\%$ AUROC improvement over state-of-the-arts and it could serve as simple yet strong baselines in such an under-developed area.', 'bibtex': '@Article{Wu2023EnergybasedOD,\n author = {Qitian Wu and Yiting Chen and Chenxiao Yang and Junchi Yan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Energy-based Out-of-Distribution Detection for Graph Neural Networks},\n volume = {abs/2302.02914},\n year = {2023}\n}\n', 'references': ['01de6d0c00e7e77050a90945246b2b4acde497a2', '31baf6130cc7837e0c67b6424bd6de11ae8a5904', '9b4f564e5d33625fa88fc4e1045e9d5681fa0cca', '3598a1932a5b4042f223f1fd2e9b3f93eb78e0be', 'c35c7240bad9b62394a63ee8cc3ae428fd204c74', '951f3f2449eb9a743543d67ab92e7769352f3413', 'ca9974ac55dacf8db6eb4a57f489756068797cab', '4818381d399636a4caefc24710b5c26a1e6e906c', '7f9760a76e9cf424da0b72d42f75594cefc4a329', '66ee16c1a274f1c9205b0ef4fbda0b4a8a481f81', '564e726cc687f325d70b46d0bb3be6863b362237', '86c4bb73bcc10a17faee13034a1742008cb001df', 'a72c85ccb30db1c76feafb3799481fa7599c8b98', '59a6a6c3110d8cfee9426ba7d8c11c37e58cfb43', '40848b41ed8c9c255ecd8a920006877691b52d03', '71a35aa42cd1ed6f213e58122154739dfd6340e8', '35b966347dae2f0d496ea713edf03a68211838a5', '9569b51674a032b3255e874e85f508a844a17183', '597bd2e45427563cdf025e53a3239006aa364cfc', '5771af144ce1d4d783a0af70c190a74e5123d0a0', 'ae9bf201f128cabaa4350b54ff6607525c736cd5', '97cd86d8d8c0f27cd3e64c6ca5cfdeb957ee39f4', 'c8c70d1a201f41af78b4e3f11810d0f8c6c452b3', '925182b91f51f8f2b747f7829e9d25ffc2729e5d', '4efb9a950f252138a30eeb942ed02663a3ea29d1', 'a25b63a6a0071d7d88ff4671c1fd40f320a08533', '3a7d5410c52213a53d3e1d8567a88a404684c045', '2d8c97db4bae00ff243d122b957091a236a697a7', '2da694d7f494265a8193f17dfc492c577ad4db1e', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', '5aea95e1ae78a66474051a330ded374e199b658c', '431ba9fae8fccad1665979d455c6307786e47318', '33998aff64ce51df8dee45989cdca4b6b1329ec4', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '6ff2a434578ff2746b9283e45abf296887f48a2d', '36eff562f65125511b5dfab68ce7f7a943c27478', 'e86f71ca2948d17b003a5f068db1ecb2b77827f7', '2da0ccacb4931f1e89cf7febdafd23a3cff079a4', 'fab4d19ed77dad7c437d885eceb8aa65fae5a783', '8ebc4145aef6a575cbaffcfeec56b20586db573a', '79c286bf03ed97fb94d33511f3355770dcee0aec', '43d2ed5c3c55c1100450cd74dc1031afa24d37b2', '306ddd8b7ea3ead125491efc3e8a9f738ce65b89', '19bb0dce99466077e9bc5a2ad4941607fc28b40c', '6064c3ab52c63ba525e1a5b7a0614ed7b177be6f', '125842668eab7decac136db8a59d392dc5e4e395', '650756338094ca801ee6fc9947970cb3c376410c', '301e14edd925fb191714ddfa13593e67c6e5b2fd', 'db72d07e5b1ae573ef59111a7b8c04e5b3252c12', '3efd851140aa28e95221b55fcc5659eea97b172d', 'b6b26564df790262abbe48fa18079d9610189b29']}
{'paperID': 'ef81bbfae3c77c177e3a5fd007c8b66a2ee7322b', 'abstract': 'Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf contrastive losses that suffice for classifying ID samples, but are not optimally designed when test inputs contain OOD samples. In this work, we propose CIDER, a novel representation learning framework that exploits hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: a dispersion loss that promotes large angular distances among different class prototypes, and a compactness loss that encourages samples to be close to their class prototypes. We analyze and establish the unexplored relationship between OOD detection performance and the embedding properties in the hyperspherical space, and demonstrate the importance of dispersion and compactness. CIDER establishes superior performance, outperforming the latest rival by 19.36% in FPR95. Code is available at https://github.com/deeplearning-wisc/cider.', 'bibtex': '@Article{Ming2022HowTE,\n author = {Yifei Ming and Yiyou Sun and Ousmane Amadou Dia and Yixuan Li},\n booktitle = {International Conference on Learning Representations},\n title = {How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?},\n year = {2022}\n}\n', 'references': ['ed0cbe81d615361e29196baec62de899c59463bf', '16ad01309dbfc406e3133bf0d62ea2be826423fe', 'c2d29e2b10572d229e83e8c0005f9b48223332eb', '8b1233107bff00355d4d1656795ec62d4f85e523', '7f9760a76e9cf424da0b72d42f75594cefc4a329', '3b316377f1d7cbcf3c907c4d8b08c05f4521b541', '8d761be2fa1a4dc60f5699d6c8542c6b7591595e', '84643a19b825f27343d95e1b7ddd76a7bafb366c', 'f19092561296244e1dafe7d799e7906e96a63773', '57119dde4f6c45b195d9768e00e441032da7a650', '0fe615dc0a422100e85cfb7e26c9306c481f6c75', '57eaad10369de402d3363c1d99c93810463eb03c', '7097137596f6755675f6aafcdd80969a747322ae', '35b966347dae2f0d496ea713edf03a68211838a5', '1cb29798801b315d6287aae1093f5432f54673dc', 'e1bb329621de73d08c47beae9b5439a1c244eb1a', 'bfc65dba18cdbe559f87dfe2cb8848452a2ee8d3', '94192bcdf3507e3543910c03b16bd06c5338fd47', 'c2eff53cc9db9eaca8d9ffe06f2d618b0e360c9d', '9a56ab8b1aba50dc2fea3cf4b531d30891a88ba9', '8bf6c69bae0956db13aa9129fedc69fdc1256dce', '38643c2926b10f6f74f122a7037e2cd20d77c0f1', '2cb80d51c771614b38924a24bf853e3a42dddabe', 'f7646cccbd6edbc148d08fea37e31bcd0592c992', 'a1b8a8df281bbaec148a897927a49ea47ea31515', '47cbd8cfd6fcba83d3b3714faef480260bc9d5de', 'ae9bf201f128cabaa4350b54ff6607525c736cd5', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '096033bd0d4bc867c7be1b8220d9afdc22c03cdc', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '1322719978980a831e1aee78aa80a141379c44dd', '925182b91f51f8f2b747f7829e9d25ffc2729e5d', '249f0a2ae3540dbe4f2a11806d2ac38581b9ad6b', '6507909a8f77c88144c3a67b9336bd1c85e84cac', '5df8b7279e0d80b6f418f7d5cb79b27cdba9ed16', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', 'f986968735459e789890f24b6b277b0920a9725d', '431ba9fae8fccad1665979d455c6307786e47318', '9dc915697768dd1f7c7b97e2c25c90b02241958b', 'd4f100ca5edfe53b562f1d170b2c48939bab0e27', '05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628', '547c854985629cfa9404a5ba8ca29367b5f8c25f', 'bd8f77b7d3b9d272f7a68defc1412f73e5ac3135', '4901eaa5a3b8cca41e4bb664ef0446d6118bd87c', '0b1157e308289bd09e798c207b38dfdeb8335c46', '6ff2a434578ff2746b9283e45abf296887f48a2d', 'fb4b700ba023c08e64c13f8030f40dcc901ac518', 'd094fb0af5bc6a26fa9c27d638c4a3a0725d8b5c', '884750937bb97e82c41316d80e5d104e0c0e4795', '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', '3433627f803953280b66ae1576d083fc9a68385a', '4543670c4b2d88a9b67525e0084044adef94ae76', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', '908091b4a8757c3b2f7d9cfa2c4f616ee12c5157', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'e90ec61e867579bc8bff26e686a59f70129e612c', 'c442d155c47981599ff9b13275058c55feb72c2e', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '95d1e89b4ed74c1d2924aacf1b0ae21ffd2a1cff', 'abstract': 'Time series classification is an important problem in real world. Due to its non-stationary property that the distribution changes over time, it remains challenging to build models for generalization to unseen distributions. In this paper, we propose to view the time series classification problem from the distribution perspective. We argue that the temporal complexity attributes to the unknown latent distributions within. To this end, we propose DIVERSIFY to learn generalized representations for time series classification. DIVERSIFY takes an iterative process: it first obtains the worst-case distribution scenario via adversarial training, then matches the distributions of the obtained sub-domains. We also present some theoretical insights. We conduct experiments on gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition with a total of seven datasets in different settings. Results demonstrate that DIVERSIFY significantly outperforms other baselines and effectively characterizes the latent distributions by qualitative and quantitative analysis. Code is available at: https://github.com/microsoft/robustlearn.', 'bibtex': '@Article{Lu2022OutofDistributionRL,\n author = {Wang Lu and Jindong Wang and Xinwei Sun and Yiqiang Chen and Xingxu Xie},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Out-of-Distribution Representation Learning for Time Series Classification},\n volume = {abs/2209.07027},\n year = {2022}\n}\n', 'references': ['ef018596c581fa37e83e9544b6412beeb15e31fa', 'cf12611abee206c9efe466781fe8998b06aba19d', '8d13ac2a2e427489ba6645567c6a02eef28eefa3', 'c6d7e13e064e9b7e972d03751045ae1fbbed26af', '7e8e9f5ddefb025154ebbcc37b5c86302a8aea9d', '763aee6af4ce9bfa6eb3dae148536c52780f736a', '9d7a6d0d92ec1ae3d84d7eaaa8c7bad13452de6b', '6bf04318d6e57463f7823b9770c5a6c19a7b47e9', '1109f18a41ad21635ae30952be89f56f52295f39', '6ef770d11e3e5918646b2f5a97c0bcc8bc9b9256', '53a16a2bd25c40401c7507ac8d70d61bbfb2e286', '648c13c77a94d46b2dbd75c8073cca4b8870bd9f', '266a35c51001be877cc970323ba8c65c7ac60811', '6ab82d3abe6c1b09ca9695f901ed927a0171a875', '085907c9b2bfbf39bcaf6fe3d16bd1dadcef5af5', '8d82973ef80e92cd64345feb3756ae361b49618c', '539e1fc503f525d3ef5b8a1976da04577279107a', 'bbae866c5edd477c1c39921436c47fd43d1f6e5c', '40848b41ed8c9c255ecd8a920006877691b52d03', '6972010d883e9746ecdf8147168caeccef6dcdb3', '09472ff0d3c3f975ef1fdc02cfb1605d3d4275fa', '03479dd8970c96058998956b7eadbac485a2a170', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '2d96c2f8c11ff95bd3fb5d9ba797beb63f01ab07', '31603b3339f4da5bdc6b7de4231bd1ddfb32a50a', '48eda0f2a611bc488987870f628dac82ae8dcfaa', '3621fff4a1c791901ea4a1359c10575193ec712d', '0d48655970e797e135e9dc8069edbb6a922da019', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', '193092aef465bec868d1089ccfcac0279b914bda', '41a4aa801da162ca777c14243e8e3a14e9bc2d45', '36e30516683032634975c53e60f3737b6e35ff80', 'f8d1cae82cdd4c6f4dcfefe5b2d1c0f0e68742fc', 'f7d4957127bb35b0d3cb1042a676ea60e259463d', '1c2efb418f79b5d29913e014a1dfd78865221c39', '1d033b30f38642e4b6dd146bb8b464bfb58aad96', 'da6e404d8911b0e5785019a79dc8607e0b313dc4', '1855fc5254f6c40c49df7fc15409e8278cabad3c', '4feef0fd284feb1233399b400eb897f59ec92755', '60a1e0a81a562f2efd56fe2c1d012b8cf5f47e1c', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '9c4f30fe94ce07a89eb9b1789b338064d5c44811', '12d0cf8ae5ffe1b89345e1dcead22be592d844b2', '1d5972b32a9b5a455a6eef389de5b7fca25771ad', '67216c49d031134a6554d050ab53440014263c20', '22a13f42511192cc5dbf7b4b1b8d72bcb15f1b75', '79c286bf03ed97fb94d33511f3355770dcee0aec', '4f605b3bb3ce574f4053f19264434baa522305b7', '7854760393966e63011c14eb81a29b1e5fb379cf', 'f4dacd1dae1615a6c89cffb5e06254aa9f3af1ef', '66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde', '5079ea296646fbbb0c1ceb8bbadf86c698c842ef', '4f12b08ad975c0def6da20d32536291becd48479', 'c1e10c15172b94d057f2bcdbcd6f3ddc766d8c62', 'e57b96b6be9666de5d87f314117ab15ccf902689', 'e0071927a83f29ab191c603f3a3bf9b6b1b35e96', '530d2b07898a610a3c7de19c01b71dc4b5a6dd5d', '8d56d4bc69a8c562434b9a129542bb79e9d6f1d6']}
{'paperID': 'c1f1d55ee54b6ea0b6052180e1aa88c2efd62481', 'abstract': 'We propose a generic module named Indirection Layer (InLay), which leverages indirection and data internal relationships to effectively construct symbolic indirect representations to improve out-of-distribution generalization capabilities of various neural architectures. InLay receives data input in the form of a sequence of objects, treats it as a complete weighted graph whose vertices are the objects and edge weights are scalars representing relationships between vertices. The input is ﬁrst mapped via indirection to a symbolic graph with data-independent and trainable vertices. This symbolic graph is then propagated, resulting in new vertex features whose indirection will be used for prediction steps afterward. Theoretically, we show that the distances between indirection representations are bounded by the distances between corresponding graphs, implying that unseen samples with very different surface statistics can still be close in the representation space to the seen samples if they share similar internal relationships. We demonstrate that InLay is consistently effective in improving out-of-distribution generalization throughout a comprehensive suite of experiments, including IQ problems, distorted image classiﬁcation, and few-shot domain adaptation NLP classiﬁcation. We also conduct ablation studies to verify different design choices of InLay.', 'bibtex': '@Article{Pham2023ImprovingOG,\n author = {Kha Pham and Hung Le and Man Ngo and T. Tran},\n booktitle = {International Conference on Learning Representations},\n title = {Improving Out-of-distribution Generalization with Indirection Representations},\n year = {2023}\n}\n', 'references': ['92394181881a9ff4063d9aedb3e4fd4ada466edb', '1b6e810ce0afd0dd093f789d2b2742d047e316d5', '8ccaf0c0fbd5e4079f36fa720cf23890be10dd66', '8c479e81ddaf55aba9044449b5be7b7bf2046b7e', 'fdacf2a732f55befdc410ea927091cad3b791f13', '8eaa6a82e1f94c1552e5c284061e25fee36ec427', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', '024a3f0ce83528914690a6bb010ba715643669d4', '43f2ad297941db230c089ba353efc3f281ab678c', '5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b', 'd2dc5081236a5e64260666e916f41c53879fc24b', '2873f78efd7adcb118a70f8ea3ca7fa1501e320a', '136c05cb8dd359fb8e0dc7947172a9ecb74ccbec', '4af09143735210777281b66997ec12994dbb43d4', '3880fa12f4a19e839f812819eda42a631278535f', 'd0bfd3cb732471a0843a39d2d047caf60a844466', 'c4744a7c2bb298e4a52289a1e085c71cc3d37bc6', 'fb507ada871d1e8c29e376dbf7b7879689aa89f9', '54fc23348ed840cb5f1fe2b41c80bfdcfc03631f', '046cd4c35f607a9ebfa2c41f02643eeded933245', 'c8efcc854d97dfc2a42b83316a2109f9d166e43f', '289fb3709475f5c87df8d97f129af54029d27fee', 'd93b0b37ee0e87a0e096ef803667b0798f465528', 'bfe284e4338e62f0a61bb33398353efd687f206f', '572a1f77306e160c3893299c18f3ed862fb5f6d9', '7e9c1e0d247b20a0683f4797d9ea248c3b53d424', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'c269858a7bb34e8350f2442ccf37797856ae9bca', '5f0625c30014c12f333eb518268647673d18f9f1', 'fafcaf5ca3fab8dc4fad15c2391c0fdb4a7dc005', '815c84ab906e43f3e6322f2ca3fd5e1360c64285', '21c99706bb26e9012bfb4d8d48009a3d45af59b2', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'c3823aacea60bc1f2cabb9283144690a3d015db5', 'be033f227bc5aeed0702d2beb944b1068b274cfa', 'c0373426c8e5579dcff60cc0bd930277822edc7d', 'f718309706172d6fb1e89f583927274f9a4cdf4f', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '3087b58cbfc6eb4a3076a180e21d6b872293f9a8', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'a2fb9bc2e3ec45fd1d7f20280c9c8503aa3de9c3', 'e62f7643f616aaad65ffd47155a53bfa325e455d']}
{'paperID': 'a64a192b01c57e18dafd1a8e826056cd3e97cedd', 'abstract': 'Outlier exposure (OE) is powerful in out-of-distribution (OOD) detection, enhancing detection capability via model fine-tuning with surrogate OOD data. However, surrogate data typically deviate from test OOD data. Thus, the performance of OE, when facing unseen OOD data, can be weakened. To address this issue, we propose a novel OE-based approach that makes the model perform well for unseen OOD situations, even for unseen OOD cases. It leads to a min-max learning scheme -- searching to synthesize OOD data that leads to worst judgments and learning from such OOD data for uniform performance in OOD detection. In our realization, these worst OOD data are synthesized by transforming original surrogate ones. Specifically, the associated transform functions are learned implicitly based on our novel insight that model perturbation leads to data transformation. Our methodology offers an efficient way of synthesizing OOD data, which can further benefit the detection model, besides the surrogate OOD data. We conduct extensive experiments under various OOD detection setups, demonstrating the effectiveness of our method against its advanced counterparts.', 'bibtex': '@Article{Wang2023OutofdistributionDW,\n author = {Qizhou Wang and Junjie Ye and Feng Liu and Quanyu Dai and Marcus Kalander and Tongliang Liu and Jianye Hao and Bo Han},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Out-of-distribution Detection with Implicit Outlier Transformation},\n volume = {abs/2303.05033},\n year = {2023}\n}\n', 'references': ['41e68a78f5bd266b1ae54d521ebd0be0e9314cd8', 'a5927a417272327d424a76b2d260f6c1a73bc71e', '10b2dc04a91b6e1cb3f24003a988fc07b57df09f', '16ad01309dbfc406e3133bf0d62ea2be826423fe', 'ca9974ac55dacf8db6eb4a57f489756068797cab', '8b1233107bff00355d4d1656795ec62d4f85e523', '4fe3f3e113334998114211f2bb9ff1659100fc14', '80ac4250b3cba55228684756acb55922042d7aaf', '7f9760a76e9cf424da0b72d42f75594cefc4a329', '7b2180d7fa0d65e8756401cb077bf3dea3f9b575', '7bdc1a737a8864b80c7abd5cca71c6514de25345', 'f19092561296244e1dafe7d799e7906e96a63773', '0d5406775fab3e71848908327fb5504df5f60f92', '0fe615dc0a422100e85cfb7e26c9306c481f6c75', '35b966347dae2f0d496ea713edf03a68211838a5', 'e1bb329621de73d08c47beae9b5439a1c244eb1a', '275675ea82a4b35c42bea2943649cabe6f6e43ab', '0c4b0409cb2ac16ad4e744d37bc5ca52cee1c1b4', '897e77a4721e542990981aad9d65580cc0937f57', '5771af144ce1d4d783a0af70c190a74e5123d0a0', '86c12bb6fb6fb2956d1147bd1b15a788b6d07f6e', '3621fff4a1c791901ea4a1359c10575193ec712d', 'd8c0aabe41ec6036a2404cb2bfaea2a7d6424ae3', '193092aef465bec868d1089ccfcac0279b914bda', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', '753b7a701adc1b6072378bd048cfa8567885d9c7', '2d8c97db4bae00ff243d122b957091a236a697a7', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', 'f986968735459e789890f24b6b277b0920a9725d', '36653f8705b56e39642bcd123494eb680cd1636b', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '9375729d21a344a5ccccd5f53556ddf90b957cd9', '6ff2a434578ff2746b9283e45abf296887f48a2d', 'b022f2a277a4bf5f42382e86e4380b96340b9e86', '1c4e9156ca07705531e45960b7a919dc473abb51', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'd094fb0af5bc6a26fa9c27d638c4a3a0725d8b5c', '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', '3433627f803953280b66ae1576d083fc9a68385a', '4543670c4b2d88a9b67525e0084044adef94ae76', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '73f76a40ed20aa3c6a8e27e4db4a8c102e7b4c6d', '0e4b0d177e550d365f456375781cd0e4f7a04979', '384ce792cf2b2afbe001f2168bfe7d5e7804c736', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '810f0c7a6f8e0e684cb0be4a7bd1675fc8a0bbb7', 'abstract': 'Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM), an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.', 'bibtex': '@Article{Kaur2022ModelingTD,\n author = {Jivat Neet Kaur and Emre Kıcıman and Amit Sharma},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization},\n volume = {abs/2206.07837},\n year = {2022}\n}\n', 'references': ['3ced5335b973fa9d4e537376c02c2df22dd5631c', '0e845ef0a3ae71bd32a6954fafe0702d0f0f033f', '4390d210bfd4cd7b646f13f287f44f9620a4f214', '04c9b1b0f83e5608e1f0c3ee0d331e74752f1fc1', '94001e6bdf94fd61be3fba6ab9e6e77fbd888867', 'de656b3de564933e1ef2351107e2369c0deae6b4', 'b249fe4e5e2bada6655ce5d61e7f50da5d471cb4', '085907c9b2bfbf39bcaf6fe3d16bd1dadcef5af5', '8f566001453bc6be0a935bf69ffd90d9db3af32b', '40848b41ed8c9c255ecd8a920006877691b52d03', '1e76e2fbf27198986271a672f462dc38d790d00f', '09472ff0d3c3f975ef1fdc02cfb1605d3d4275fa', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '2b5a8e6fd214e1471c5d61bf80e4ff9f774765b6', '0b40141779fafcedc28d83bd678807ddb5980df3', '5d0e2635a1ebe2c9347529975bc876d4286c9ab7', 'dddc3f4a6d2d668eb9a840ca8ff7c2d83366a507', '3621fff4a1c791901ea4a1359c10575193ec712d', '3538c520244b508945476f0814d2ba1e8f22307e', '976cb563bd7edfba984fbb6795026b31a4df3237', 'a1aaccbb59dda09352e6409f2628f152d79c00dd', '87f6a7c014ce206ac5b57299c07e10667d194b39', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'f855730cac5cc3d004d7d0a70c5cd0cfe9090af3', '8fe578a3daede41faea142eb414cb8c95d5adbf6', 'b611a8095630557229dc5fb6b07c272f1cd614da', '2f0b9e8e1aed4d4c112164d04f10ddbeb27da55f', '35ee6606ec99b5bf282a0c5f400edbd16a6e22d9', '1b27b9cfe0ce17950b6ea72f9ef8cf5a7459bccd', '3645848a932a8182533e5b054ca67d1072f360f9', '44fc8d79fb8e0f8c6c6f680179b5803a789c6227', 'a60540a8407fd117fd8e6857d4728e661f53dcc8', '8ad3bc604adc58c828c30e55e9adef0a81bf7e81', 'ba6ba7f488c1ece0803f4b9e1c83a3196d061610', 'f986968735459e789890f24b6b277b0920a9725d', '4c45944b2faeb6e19d80dfb9e536934167cb2a82', 'a588d38ec81c0337b445931eadf6f443aea13380', 'b39b45a59c27a0cb3214d5a84547f54722d40c69', '7e355a8f42becb6648efc4cf0c129a7f560789be', 'c43d954cf8133e6254499f3d68e45218067e4941', 'a90226c41b79f8b06007609f39f82757073641e2', '12d0cf8ae5ffe1b89345e1dcead22be592d844b2', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '8a29808865a0daf55e80a626a555c21f31c479f4', '1357de2a653b704066d2a2b0b2dfe12875bf8eb8', '1d5972b32a9b5a455a6eef389de5b7fca25771ad', '79c286bf03ed97fb94d33511f3355770dcee0aec', '225f78ae8a44723c136646044fd5c5d7f1d3d15a', 'c069629a51f6c1c301eb20ed77bc6b586c24ce32', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'f354310098e09c1e1dc88758fca36767fd9d084d', '7c15ee1018a87708815c359d920820d8742dd3c8', '6c3155e37db216929b3a3c7a3ecf6b1124cba56c']}
{'paperID': 'cf49af82a5f1ba5f2beba9f290e684b7b51b64e6', 'abstract': "The separation between training and deployment of machine learning models implies that not all scenarios encountered in deployment can be anticipated during training, and therefore relying solely on advancements in training has its limits. Out-of-distribution (OOD) detection is an important area that stress-tests a model's ability to handle unseen situations: Do models know when they don't know? Existing OOD detection methods either incur extra training steps, additional data or make nontrivial modifications to the trained network. In contrast, in this work, we propose an extremely simple, post-hoc, on-the-fly activation shaping method, ASH, where a large portion (e.g. 90%) of a sample's activation at a late layer is removed, and the rest (e.g. 10%) simplified or lightly adjusted. The shaping is applied at inference time, and does not require any statistics calculated from training data. Experiments show that such a simple treatment enhances in-distribution and out-of-distribution distinction so as to allow state-of-the-art OOD detection on ImageNet, and does not noticeably deteriorate the in-distribution accuracy. Video, animation and code can be found at: https://andrijazz.github.io/ash", 'bibtex': '@Article{Djurisic2022ExtremelySA,\n author = {Andrija Djurisic and Nebojsa Bozanic and Arjun Ashok and Rosanne Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Extremely Simple Activation Shaping for Out-of-Distribution Detection},\n volume = {abs/2209.09858},\n year = {2022}\n}\n', 'references': ['1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe', '9695824d7a01fad57ba9c01d7d76a519d78d65e7', '13a0d8bb38f739990c8cd65a44061c6534f17221', 'c57293882b2561e1ba03017902df9fc2f289dea2', '8b1233107bff00355d4d1656795ec62d4f85e523', '996445d847f06e99b0bd259345408a0cf1bce87e', 'ec4f3b701c6d97fc73d5afdede41a6510092320f', '7f9760a76e9cf424da0b72d42f75594cefc4a329', '1a250108e2a4d6c0d472b56bf2cbb2e07d2eedbb', 'e757488d2e8684e3da7b14fbb000b7e4a0bab001', '7b9d49b4c18a160512cbe9fc9fa5d8c40cd6b84e', 'f19092561296244e1dafe7d799e7906e96a63773', '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', '9d6acac70b2d1fdb861a08b00766ef263109cd7f', '35b966347dae2f0d496ea713edf03a68211838a5', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', 'e6c561d02500b2596a230b341a8eb8b921ca5bf2', 'e04a80263d252a3d8a382ba37a249b9345620570', '6efc9b997e57203e7e5405751c2ef7ee75c5e578', 'a0f0a94927c0013fa924ee43c8ddbace1d71e3fb', '13de3c06ef6dac1c296ada45df2be590f843edb7', '3055e31b007c93fb9291b3ecd5b3c5f94fe9225f', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', '95e920c2ba4ed19e462d42b2802536a5b35b796b', 'f986968735459e789890f24b6b277b0920a9725d', 'd55d1d035e91220335edff0fe8f5d249d8c4a00b', '2f201c77e7ccdf1f37115e16accac3486a65c03d', '9d55b5fc3f3b92324f4a9c46d5b66d11895ba565', 'dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4', '05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628', 'd65ce2b8300541414bfe51d03906fca72e93523c', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '57797e2432b06dfbb7debd6f13d0aab45d374426', '802168a81571dde28f5ddb94d84677bc007afa7b', '16aa01ca0834a924c25faad5d8bfef3fd1acfcfe', '6ff2a434578ff2746b9283e45abf296887f48a2d', '5694e46284460a648fe29117cbc55f6c9be3fa3c', 'e86f71ca2948d17b003a5f068db1ecb2b77827f7', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', 'f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6', '3433627f803953280b66ae1576d083fc9a68385a', '4543670c4b2d88a9b67525e0084044adef94ae76', 'eb42cf88027de515750f230b23b1a057dc782108', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '908091b4a8757c3b2f7d9cfa2c4f616ee12c5157', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '20ca62aae323c8778a978300c682ac8e1d160e74', 'abstract': None, 'bibtex': '@Article{Floto2023TheTV,\n author = {Griffin Floto and Stefan Kremer and M. Nica},\n booktitle = {International Conference on Learning Representations},\n title = {The Tilted Variational Autoencoder: Improving Out-of-Distribution Detection},\n year = {2023}\n}\n', 'references': []}
{'paperID': '48790c048f0b982e290e932eb320e0e1f544b641', 'abstract': "Though Self-supervised learning (SSL) has been widely studied as a promising technique for representation learning, it doesn't generalize well on long-tailed datasets due to the majority classes dominating the feature space. Recent work shows that the long-tailed learning performance could be boosted by sampling extra in-domain (ID) data for self-supervised training, however, large-scale ID data which can rebalance the minority classes are expensive to collect. In this paper, we propose an alternative but easy-to-use and effective solution, Contrastive with Out-of-distribution (OOD) data for Long-Tail learning (COLT), which can effectively exploit OOD data to dynamically re-balance the feature space. We empirically identify the counter-intuitive usefulness of OOD samples in SSL long-tailed learning and principally design a novel SSL method. Concretely, we first localize the `head' and `tail' samples by assigning a tailness score to each OOD sample based on its neighborhoods in the feature space. Then, we propose an online OOD sampling strategy to dynamically re-balance the feature space. Finally, we enforce the model to be capable of distinguishing ID and OOD samples by a distribution-level supervised contrastive loss. Extensive experiments are conducted on various datasets and several state-of-the-art SSL frameworks to verify the effectiveness of the proposed method. The results show that our method significantly improves the performance of SSL on long-tailed datasets by a large margin, and even outperforms previous work which uses external ID data. Our code is available at https://github.com/JianhongBai/COLT.", 'bibtex': '@Article{Bai2023OnTE,\n author = {Jianhong Bai and Zuozhu Liu and Hualiang Wang and Jinxiang Hao and Yang Feng and Huanpeng Chu and Haoji Hu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the Effectiveness of Out-of-Distribution Data in Self-Supervised Long-Tail Learning},\n volume = {abs/2306.04934},\n year = {2023}\n}\n', 'references': ['d57a90a2abafc55fcdf7dda2a84ac2fc05aa4d9a', 'b2eb73e269622329ac98e9938bb1402314281c9b', '10060c83e48b86eab95386da928de3dfd4201ac8', '26218bdcc3945c7edae7aa2adbfba4cd820a2df3', '13beab6bad06631d177e274e4d70b95c4b103423', '29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d', '2121c6910b5f187fdaecf65981ed76a6a668a559', '16aa9ddb681554daf075eb13ba1fe37aff5f151d', '6b1b8cbace3bcbfe9fd9bd48fae47bc6473abd79', '4b842ba29f6d244b9f456056a2d7efab9e4903a5', 'a01ac66f5f66a2b23152f631b920972e4407275c', '39ccbe81127e44bf3275b94d8b2311367d5c9548', '98be8ae73dc5b9666c298b09ae0ffe9a361197a9', '33c18b096ca4493a2c052b49c77468e2bc3f17c1', 'aff22383d605f9e3961364f850b93db6d986b843', '2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c', '2be5c2f23b997e1cb22479dc1d5d1735997ad4be', '758fb62a9ce0cabfce71b0020a662522ca8ceabf', '3bab355a4660edcc3f6e10df5e260dcf92f630d0', '022622e024890d6e044ac50e2da6b44c59bdf418', '5099d47408251626a4adc6a0f5e93678d8188732', '38f93092ece8eee9771e61c1edaf11b1293cae1b', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', '9a56ab8b1aba50dc2fea3cf4b531d30891a88ba9', '38643c2926b10f6f74f122a7037e2cd20d77c0f1', '297dbda128756e2816130a8de057f96760e4b89c', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '299847adf3ee558a760475ffa364facac3ebbb16', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '37d87f24841987ae64e871088efb6c1df6d405d4', 'dcc4c760c3f1cb17f953c487190b735030c33b78', '01cc664615c8b48c3341cab4452a60191fb1451d', 'bcfba69c2fadf2efea83be12fda2601f8d4681af', '97f4d09175705be4677d675fa27e55defac44800', '73c07e0a998576bb9d9409e5eed713788c0be037', '54036f43acc6c9b49b334270c7237217685f52fb', '2d8c97db4bae00ff243d122b957091a236a697a7', '9b15362b9a025071aa170f7ed81a761bc057c859', 'f986968735459e789890f24b6b277b0920a9725d', '41ed3608eae64fd181ca8b5e6d0eb63b210b588b', 'b36a5bb1707bb9c70025294b3a310138aae8327a', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '1f0bafe95728885034f5371420db2790e990971d', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7', 'be9a17321537d9289875fe475b71f4821457b435', '54d2b5c64a67f65c5dd812b89e07973f97699552', 'b1e4660c82951ba19a861afc8be5cf56cc46aaa8', 'fb885a2bcab4d361e6fbcd426ea617f0cab1ef63', '7fed3e00be2bb09510f5f7cad7ac106e6c94a359', '23feca8b7435f46c317c90172bb01eb1ed942fae', 'b65bc8bd4e8900c38eb09bc1cbccea2499a86627']}
{'paperID': '35af40853f48030c5050c1c3191f6e227ceb264b', 'abstract': 'Real-world machine learning problems often exhibit shifts between the source and target distributions, in which source data does not fully convey the desired behavior on target inputs. Different functions that achieve near-perfect source accuracy can make differing predictions on test inputs, and such ambiguity makes robustness to distribution shifts challenging. We propose DivDis, a simple two-stage framework for identifying and resolving ambiguity in data. DivDis first learns a diverse set of hypotheses that achieve low source loss but make differing predictions on target inputs. We then disambiguate by selecting one of the discovered functions using additional information, for example, a small number of target labels. Our experimental evaluation shows improved performance in subpopulation shift and domain generalization settings, demonstrating that DivDis can scalably adapt to distribution shifts in image and text classification benchmarks.', 'bibtex': '@Article{Lee2023DiversifyAD,\n author = {Yoonho Lee and Huaxiu Yao and Chelsea Finn},\n booktitle = {International Conference on Learning Representations},\n title = {Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement},\n year = {2023}\n}\n', 'references': ['517f2d4a0d2e4ee8c452ce53c455a4c3e7662a7b', '14a3aae8060338e3fbefc2af694890b019874d4f', '962466ca8a3bf432a2d45b656ab5dbcc9caf5b16', 'ab2a8ca21309859ed027928dc38e6915be0e6776', '535131d7218e26360c09cd8f9a2e7198ec0e3e6f', '216d093cb2ad81bf55c21dbce2217f2b9032e67b', 'd071797499892940876a50f518d9c74d8c4e4018', '0edbe195120c78777959dc8bff25d4c61305c9c8', 'a3ec0076139420a34d2079a029b0836b890e5d3c', '40848b41ed8c9c255ecd8a920006877691b52d03', '29877659966f5ca2f198712a313cc653789edef1', 'b405764900fd2ec3979b16633056e0e6434973a8', '71a85e735a3686bef8cce3725ae5ba82e2cabb1b', 'e4a46c64aafbef0406e9cfa90dd9c43e3e07598c', '00325cb5408da77827951abd3fa93ec3bd019608', '66488a38c3bae5d928bb22aa615fd0e64ccac62b', '5ce0ce49c082313d042fb864471af39ad04d26e5', '0b40141779fafcedc28d83bd678807ddb5980df3', '1b04936c2599e59b120f743fbb30df2eed3fd782', '99fbd94538d9568a04196e055d286ffae32cf58f', '299847adf3ee558a760475ffa364facac3ebbb16', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', 'b5461f9c5d65e87561e00848921ee797902dae14', '193092aef465bec868d1089ccfcac0279b914bda', '20ba55ee3229db5cb190a00e788c59f08d2a767d', '4ce2f55585f3156e332721b8ab4f449389dc2a3c', '207033829813aadc2f2dca8f93279352d39de759', 'df074eebeeaac85f87b48b0c1351c92f8442c1fb', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'ffb3886a253ff927bcc46b78e00409893865a68e', '676e40050453ddeb1387f8314478c0ac3681a8c6', 'f986968735459e789890f24b6b277b0920a9725d', '67a97032fd3ad81cda45e1e5d4a1a7d851494525', '2997b26ffb8c291ce478bd8a6e47979d5a55c466', '5b01eaef54a653ba03ddd5a978690380fbc19bfc', '04541599accc47d8174f63345ce9c987ef21685b', '540b399c768ab254be3df3c5b0cfd195cbd08f3a', '12a6492b48ab5c475615f7ba381b3dcd205041d6', '2064020586d5832b55f80a7dffea1fd90a5d94dd', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf', 'a32a61a6bf23d13a7088f1c77e694ab13bb6c58e', '1252ce88d42db2810de848e10f0a2d85f9bfdf7b', 'd4a196e67e9e47a9797670b7e4ae59f2330baca0', '802168a81571dde28f5ddb94d84677bc007afa7b', 'a90226c41b79f8b06007609f39f82757073641e2', 'e7eef2ac4136ec93bd306d2c9c353a13729a4553', 'eb7ee0bc355652654990bcf9f92f124688fde493', '1d5972b32a9b5a455a6eef389de5b7fca25771ad', '45373921f06a6efebefa6189d2dd80362ab0836e', '1c734a14c2325cb76783ca0431862c7f04a69268', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '1e3154b10b872c100b86181ac2931c8e26f67912', '71b7178df5d2b112d07e45038cb5637208659ff7', '46f74231b9afeb0c290d6d550043c55045284e5f', '7486e148260329785fb347ac6725bd4123d8dad6', '88843f4372f7c0b8c820a164b64060983df12049', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'a0456c27cdd58f197032c1c8b4f304f09d4c9bc5', '1150f9289c6151506e3f7cf0e6ebbcfd49f1dace', '9642a175637a400b425f0ac0cb6a2b067cc8fe6b', '08b67692bc037eada8d3d7ce76cc70994e7c8116', '8a85ef6a7ebcd8735b868bf9c4a77e6a3c195caa', '05e882679d61f4c64a68ebe21826251a39f87e98', 'ef0d827d5512de73d877048cf63b61c016cb81f1', '798d9840d2439a0e5d47bcf5d164aa46d5e7dc26', '1325bbd04a3e5a7e8137cf2edf9cbca7fc6fd55d', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', '62064218665ad89f0cb2a44f5b19f7703d9c7e71', '910688d01c01856dd20715907af44157de8d3d1d', 'ae3abde9f9df2cddd6d9c896e91a93fca5034480']}
{'paperID': '9a7a51cf95e5cb796847ec6d32c0b9ed95f1eec2', 'abstract': 'Recently, there has been a growing surge of interest in enabling machine learning systems to generalize well to Out-of-Distribution (OOD) data. Most efforts are devoted to advancing optimization objectives that regularize models to capture the underlying invariance; however, there often are compromises in the optimization process of these OOD objectives: i) Many OOD objectives have to be relaxed as penalty terms of Empirical Risk Minimization (ERM) for the ease of optimization, while the relaxed forms can weaken the robustness of the original objective; ii) The penalty terms also require careful tuning of the penalty weights due to the intrinsic conflicts between ERM and OOD objectives. Consequently, these compromises could easily lead to suboptimal performance of either the ERM or OOD objective. To address these issues, we introduce a multi-objective optimization (MOO) perspective to understand the OOD optimization process, and propose a new optimization scheme called PAreto Invariant Risk Minimization (PAIR). PAIR improves the robustness of OOD objectives by cooperatively optimizing with other OOD objectives, thereby bridging the gaps caused by the relaxations. Then PAIR approaches a Pareto optimal solution that trades off the ERM and OOD objectives properly. Extensive experiments on challenging benchmarks, WILDS, show that PAIR alleviates the compromises and yields top OOD performances.', 'bibtex': '@Article{Chen2022ParetoIR,\n author = {Yongqiang Chen and Kaiwen Zhou and Yatao Bian and Binghui Xie and Bing Wu and Yonggang Zhang and Kaili Ma and Han Yang and P. Zhao and Bo Han and James Cheng},\n booktitle = {International Conference on Learning Representations},\n title = {Pareto Invariant Risk Minimization: Towards Mitigating the Optimization Dilemma in Out-of-Distribution Generalization},\n year = {2022}\n}\n', 'references': ['dc659ff4b6f654d8c4ba355fb247d80726d623df', '5f09037109dceda8803baa527e8d1ed4095bee8f', '50447645baad0ad9f3a6c314a42abfe8ee6455fb', '5651cf8db4ffdc728aa89cf780a5573c37ecf805', '1420c75750c06c5eee473118389a6901847ad18b', '1b24fbc2189aca1d17f66eac7c8b09397eaf336f', '241f8a90da2a9ec13ca44be5b602585bde4f92b7', 'aa39c5a3080de756ad00648548e8f4faa2cfbf54', '0399be055f69e07a360dd4537c6f6304ff8c6ddc', '3ced5335b973fa9d4e537376c02c2df22dd5631c', 'b7fe3952a4d0abaa027f188ff39f2be3f1825e94', '2f6b5a30f3ef66402ccdf19564ae0eebd0029fc0', '6bf04318d6e57463f7823b9770c5a6c19a7b47e9', '216d093cb2ad81bf55c21dbce2217f2b9032e67b', '3076cfc34ed4cbcea88da074a0faa841cbf82036', '4390d210bfd4cd7b646f13f287f44f9620a4f214', 'd071797499892940876a50f518d9c74d8c4e4018', '525dd120c0b5808ddcbbf703677b46346fb0729b', '8f566001453bc6be0a935bf69ffd90d9db3af32b', 'bfc7bd9442c6a7ecbf0d4f2bf4e23a8861792c01', '34cce045b2106decb208e25197619628859fa3c0', '1fc4470c1766aea4c435015f5ef873b066f50121', '531beffcfca278108d8f89e1f9f5ed474907aa2d', '40848b41ed8c9c255ecd8a920006877691b52d03', '0f4374e62ae889dd2a35dc4c97b9a0510146fa87', '29877659966f5ca2f198712a313cc653789edef1', 'fbf50ed8e09b3268770029af30b01d31973f77d0', '00325cb5408da77827951abd3fa93ec3bd019608', '6e62a903dac8e643160bd1337d6cb8b09bf2f062', '6972010d883e9746ecdf8147168caeccef6dcdb3', '99bc80389f5957c7472906a5970e35a46281b469', '8352a4ec9a0cf24b7556945743c418f84f9ed9fb', '90e8767589109ff1bcd01ba847a497da6027a9cf', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '77e71aed1d8827ea65367933503ce24d6fbfb4ae', '2b5a8e6fd214e1471c5d61bf80e4ff9f774765b6', 'e14c3c0b4b2efb73cf47c54c45a2f23e45f6a7d1', '7bfaca28948164006002a3a71a38165d36af51c5', '23c84238f9b15d7fd87a95d3b16882a21b953d8a', '5d0e2635a1ebe2c9347529975bc876d4286c9ab7', '431717afcb02e83a1c77c08639199436d702dc4b', '1b04936c2599e59b120f743fbb30df2eed3fd782', '3621fff4a1c791901ea4a1359c10575193ec712d', '0702f398468c23c96e3313e6efc798c964fba289', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', '9f93e2de4c24acd3ffb10efae1c849a63830148d', 'a54b56af24bb4873ed0163b77df63b92bd018ddc', '6672941010dddfbe3228102e30de02f848930f36', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'acb9017f0d55ab0e8c8863b0b5dbdb373e4aee7b', 'b611a8095630557229dc5fb6b07c272f1cd614da', '1b27b9cfe0ce17950b6ea72f9ef8cf5a7459bccd', '14558cb69319eed0d5bfc5648aafcd09d882f443', '611fe6e34df07ea1b2104899e49642b4531b53e9', '2b0d7e51efd004fe3847f54863540c79312f3546', 'a60540a8407fd117fd8e6857d4728e661f53dcc8', '2d15a7546c16d5821ffa8f769eb7ec18e435e64d', '42f25f9513a86546a22626d6e1ea3b446c8fc994', '7a84a692327534fd227fa1e07fcb3816b633c591', 'e02ae07b45a2241f7ee1180b446ed7ba208c51e8', 'a588d38ec81c0337b445931eadf6f443aea13380', '4feef0fd284feb1233399b400eb897f59ec92755', 'b1e7f07965a53491690bd31fdab626bfac606eae', '0528ec31d633d1588a711c41b6adf624442df21f', '07b5093aace8e485e7d23b83edb6351618138127', '54ddb00fa691728944fd8becea90a373d21597cf', '5694e46284460a648fe29117cbc55f6c9be3fa3c', '12d0cf8ae5ffe1b89345e1dcead22be592d844b2', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '1a60d4122ef0ac6972ef9b4a3752ac1657de482c', '1d5972b32a9b5a455a6eef389de5b7fca25771ad', 'a2bf2e83df0c8b3257a8a809cb96c3ea58ec04b3', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '8221da402eb27e415d08d7a5b43517cc0fa2f9f1', 'aa7bfd2304201afbb19971ebde87b17e40242e91', 'b7ef79008d87bce38144b6f1a06e36870e1c2449', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', '844bc70828afab2d5be286934148a62c7a9e51df', '9642a175637a400b425f0ac0cb6a2b067cc8fe6b', 'a0117b72a4f68d0a134e24f674ca7fd0b42663b7', '191bb2e05e943903a05f6862e171f873a681793d', '5a064b89834622e238033a67e0da1f241db1dfe2', '249348e7da5b91ab09202f401831750cd0eb0a3b', 'acb6f28bbfb2d91832a5e11e734012cd95974976', '046de0df4562625a14998f96601a28475a05e4bb', 'e4350e816a350662ddb5f9ef92437aa8f3fd44f6', '9dc62fe526f67674512d749ad1880f8eaa5ca3de', 'df24c3011fc42b72195e876ce052a0a072a1d923', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '94b6f6822f364cf7b1a3a9984667c009e2ec6a65', 'abstract': 'Machine learning algorithms typically assume independent and identically distributed samples in training and at test time. Much work has shown that high-performing ML classifiers can degrade significantly and provide overly-confident, wrong classification predictions, particularly for out-of-distribution (OOD) inputs. Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be generated and it is important to know when to trust the generated output. We present a highly accurate and lightweight OOD detection method for CLMs, and demonstrate its effectiveness on abstractive summarization and translation. We also show how our method can be used under the common and realistic setting of distribution shift for selective generation (analogous to selective prediction for classification) of high-quality outputs, while automatically abstaining from low-quality ones, enabling safer deployment of generative language models.', 'bibtex': '@Article{Ren2022OutofDistributionDA,\n author = {Jie Jessie Ren and Jiaming Luo and Yao Zhao and Kundan Krishna and Mohammad Saleh and Balaji Lakshminarayanan and Peter J. Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Out-of-Distribution Detection and Selective Generation for Conditional Language Models},\n volume = {abs/2209.15558},\n year = {2022}\n}\n', 'references': ['9da634823416e96417530f0a2197b9a4936eee3e', '8b1233107bff00355d4d1656795ec62d4f85e523', 'c69f9a5185b4c29525bedb2dcc79d20b42c14cc6', 'b3848d32f7294ec708627897833c4097eb4d8778', '1706a87cf650750df326891b09da53899e85354e', 'dfd104dd0ff28b1bde2fbd4c4d6d3ccb4761f639', '8331f4363d65235a8344e6a0c9b21fa3ab4c1d5e', '3397f25209666d30a8b797932e3197cc826fba18', '4c7d664761c359cffd20c9d555031271ec67ab3c', '0921322cf6ea34d1852f13cb67eeac9d1f863518', 'a9b04a3e0cf5766df9b3af8c442f2d85ac5e2c7e', '6e84d6788bdd1f99a0ed322cf35ae7b2fb81aa66', '1b4a54670bb4fe15bcb0d06de0391d5b6d10ace2', '35b966347dae2f0d496ea713edf03a68211838a5', '30b99ae0682d42a2010be401dd1d8f7baca9bb5f', '9e67b9758520e49016ab66bafb974d2e1ed762d1', '94192bcdf3507e3543910c03b16bd06c5338fd47', '14fc61fdc8f2205ff96ff6dc9c4c881e4063db8c', '49b477049668505ecc5d74be1f9117437e63d14e', '298f67a9719e74192296fa2428776c46161a256a', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', 'dbeeca8466e0c177ec67c60d529899232415ca87', '97f08c1ae8ca5ddf5948c66bfbbc0546ac154807', '01508f386eb2ca5181fde7bb6da4920e250d7498', 'f4061bd225b3be5b3f5b18eb1a229ce991efefeb', 'f9700e31a1d0ae34d4571ab056dfb268c1543349', '6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6', '395de0bd3837fdf4b4b5e5f04835bcc69c279481', '3cfb319689f06bf04c2e28399361f414ca32c4b3', '925182b91f51f8f2b747f7829e9d25ffc2729e5d', '1eb7f46b1a0a7df823194d86543e5554aa21021a', '8d89f85b5f8a1d65b4e93a7ebb793618641c3ece', 'cf4aa38ae31b43fd07abe13b4ffdb265babb7be1', 'a01fb557abc65ec5c37c28ca18298f27aa0dba72', '2d8c97db4bae00ff243d122b957091a236a697a7', 'ce89ee7aaeeea2c9d474707690f3ea9d948776a3', '305b2cf37e5dece81e95c92883d5a6e28ac93b22', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', '4e346eb1628df6a12c1a121f862fb3a16c6fec60', '54a13bcc9613dcaa76fb25fbe96572f376cfcca9', '8691706ad0cf5e83969658b2e6bfffdc379440c9', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '547c854985629cfa9404a5ba8ca29367b5f8c25f', '2ed7cc027367295b1a7d7cd49406acfa5c580138', '668db48c6a79826456341680ee1175dfc4cced71', '802168a81571dde28f5ddb94d84677bc007afa7b', '6ff2a434578ff2746b9283e45abf296887f48a2d', 'c6850869aa5e78a107c378d2e8bfa39633158c0c', 'feb420a4ac7c5719d51480053cd3e8669d5f2062', 'd1505c6123c102e53eb19dff312cb25cea840b72', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '4543670c4b2d88a9b67525e0084044adef94ae76', '25ca4a36df2955b345634b5f8a6b6bb66a774b3c', '595aed9aa694565348794782a3ebf8344ced529a', '32d8a10c47096fefe17a68f3166059f172ad094e', '0e4b0d177e550d365f456375781cd0e4f7a04979', '3e60cba99b4e8a45f2e3ba3df462ac949a720833', '0b1470014bdbaa80ba63da0491d9db6c7d4febcc', '22e7424448e17e3357e03db73ddf7ce2c39b48f6', '63aa5073ce839c26b3d505200e5ef78d875f91c0', '37eb32915b7767685ec3c9e9728ca9a50a379b8d', '25890fe723d290ac5d535212b64382e22f8a66f6', '9405cc0d6169988371b2755e573cc28650d14dfe']}
{'paperID': 'f9512b7c6129e0243726742cd833532482b2b11b', 'abstract': 'Neural networks are widely used in Natural Language Processing, yet despite their empirical successes, their behaviour is brittle: they are both over-sensitive to small input changes, and under-sensitive to deletions of large fractions of input text. This paper aims to tackle under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. We develop a novel technique for formal verification of this specification for models based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. Using this method we can efficiently prove, given a model, whether a particular sample is free from the under-sensitivity problem. We compare different training methods to address under-sensitivity, and compare metrics to measure it. In our experiments on the SNLI and MNLI datasets, we observe that IBP training leads to a significantly improved verified accuracy. On the SNLI test set, we can verify 18.4% of samples, a substantial improvement over only 2.8% using standard training.', 'bibtex': '@Article{Welbl2020TowardsVR,\n author = {Johannes Welbl and Po-Sen Huang and Robert Stanforth and Sven Gowal and Krishnamurthy Dvijotham and M. Szummer and Pushmeet Kohli},\n booktitle = {International Conference on Learning Representations},\n title = {Towards Verified Robustness under Text Deletion Interventions},\n year = {2020}\n}\n', 'references': ['07398e448180ad75c44d30f23a65289d40ff6f52', '4690190d6c110f7525f7250e1acf4a4eab42519f', 'f3b89e9a2b8ce1b6058e6984c3556bc2dded0938', '46f055fff654f936e40aeac1a3abb082beab0edc', '7ad8c18994108a630c4564400f6137bf4d8b7818', '67b72e427187b1113c787f9265926322e3d123e8', '43a4a354b67ab6d5531355a368094815d2d2593d', '3de72d2b1ead0b9c7af5804252024128312b9cfe', 'fac1c95993e86f92c7adeec7f72e06503e4190d5', '75339d34bdac0d21a41461228ec6088eecdf857a', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', '8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6', '2410923ed90b099e3f5565b63e789f10bf70ec4c', '1d8f4f76ac6534627ef8a1c24b9937d8ab2a5c5f', '9db631435f7f79646a4e0a1841fbeb3340e44261', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', '74e9053d6f44f4507bd40bbea999ee65f0cbefb2', '2b110fce160468eb179b6c43ea27e098757a56dd', '804fb9542f4f56e264dd2df57c255a9a2011c00f', '8d35663a80199b173d8cbd12dbf2300a9f86a021', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '514e7fb769950dbe96eb519c88ca17e04dc829f6', '765bdcf27ebc1eb03a14f1e47aefa4dda1e03073', '8472e999f723a9ccaffc6089b7be1865d8a1b863', '4b23012689e0f17912fb38d4984775e567cff8d6', '7b8fee37c685d6fd38455e2c06eef988fa86d522', '3502b5ef1afb16f76bcae33db17179195bbcdaae', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', '9f92a0ccc8b039a83bd5ba5482facb5829c712aa', '73300838d524d062e8341b242765fb6efaf48f43', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '2cd8e8f510c89c7c18268e8ad51c061e459ad321', 'e96506ee4baab43fa81cf1870cf7befb4a71fec7', '2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45', 'f04df4e20a18358ea2f689b4c129781628ef7fc1', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'de794d50713ea5f91a7c9da3d72041e2f5ef8452', '908119b1aee9774f3a2ed9ecaf42c441f038178c']}
{'paperID': '3242bf8767179c13c7322ccfdbe18c66c1e25a99', 'abstract': 'Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify DARTS to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of DARTS that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling.', 'bibtex': '@Article{Zela2019UnderstandingAR,\n author = {Arber Zela and T. Elsken and Tonmoy Saikia and Yassine Marrakchi and T. Brox and F. Hutter},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Understanding and Robustifying Differentiable Architecture Search},\n volume = {abs/1909.09656},\n year = {2019}\n}\n', 'references': ['78dbb9334215194020437c2c2cfdfce478f30060', '954d607ec77d2893e64105021f6b1010c3dd5e2e', '6eb3a62cd365e4f9792eedca43c90595e1a862ba', '35a59bd09974c7fc78cf681f77f7301e180fd23c', '6beadd6d5191e5546d1334d0a2f1d8bd1ab80e30', '7bac3d11824fabe0dc3ac2fee9bfb667e82fba9c', 'f4838839719cf96951ade45a221700341f57c4d7', '40fa2e2a58c0245a35fb363e6694d978ef51adb2', 'f323407464c4cd492d3fc1afd7170eab08f44d9b', '3f0a2de309f21a957b4741dd68007eb08d9b12e3', 'b9e942942306d1d4b7a5640d8ed3c0cdcdc34078', '93436a26d744e0417e21df10abdfce2cc74b1e58', '45b7b5514a65126d39a51d5a68da53e7aa244c1f', 'c1f457e31b611da727f9aef76c283a18157dfa83', '15561ab20c298e113b0008b7a029486a422e7ca3', 'dcc808993310a8a64fdd5efa9e46d0022ff12c27', '3662def6e3909868f92461694a586a626e725416', 'f322a04e51e50a5896a2d28da0beba12a0a49d1b', 'fe9b8aac9fa3bfd9724db5a881a578e471e612d7', 'c7ec956cb69ee85269067bdaa3c06a8aa63f04ac', '0e0ee672ebd9ec0019c414d1c0524f3bb888dd6d', '856451974cce2d353d5d8a5a72104984a252375c', '8a1ce657dd41a4f49990a4769000dc8049b83404', 'eb35fdc11a325f21a8ce0ca65058f7480a2fc91f', 'd0611891b9e8a7c5731146097b6f201578f47b2f', '84e65a5bdb735d62eef4f72c2f01af354b2285ba', '2cad68ca9cea089324b67ed96b1175f7a655c521', 'ecc76c03d6a3ae4233097ef8bcc9d04d8b3c9bec', 'f108b65fe0003e387e1cd7e50f537af0531818e4', '0359739027d44f2baa0ae7e99fca8e3400b8181f', 'b6583fe9c9dc52bb129aff4cefc60519349f3b4c', '6cd5dfccd9f52538b19a415e00031d0ee4e5b181', '67d968c7450878190e45ac7886746de867bf673d', '04fe6b11280c79b91c060934be66856877e532c6', '8ec5896b4490c6e127d1718ffc36a3439d84cb81', '197c8988ef21d0b58d363c21bafe1900c3089e3e', '66edb2a8d33db2d133d3a3c8c032a06a95c6cd3b', '1ced31e02234bc3d1092ffb2c7442ffbd51cb309', 'c2fb5b39428818d7ec8cc78e152e19c21b7db568', 'e2820bffe5b42cb7d88b7f65c12171c62ab4aae2', '7d53f0c87c8ab0de6f3e74515e3ffaf3fab40c62', '5b0a44014c24f9b584904bf223530a3b9fa9853f', '6609c558425c9e1848944049c6e302c69eeb2842', 'd03c916d49268d48fde3b76a68e64af7761835e7', 'e5ca0f1d79a5245ee5ddf6af80c01829bcac7340', '6b61cbbdce584fe8ee97184d8abb416daf96721a', 'f91154c0d159a3f2dd3638915db32c5914544273']}
{'paperID': '4bf832104a47c380eb4413b20a9d5bf06649684f', 'abstract': 'The information bottleneck method provides an information-theoretic view of representation learning. The original formulation, however, can only be applied in the supervised setting where task-specific labels are available at learning time. We extend this method to the unsupervised setting, by taking advantage of multi-view data, which provides two views of the same underlying entity. A theoretical analysis leads to the definition of a new multi-view model which produces state-of-the-art results on two standard multi-view datasets, Sketchy and MIR-Flickr. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to traditional unsupervised approaches.', 'bibtex': "@Article{Federici2020LearningRR,\n author = {M. Federici and Anjan Dutta and Patrick Forr'e and Nate Kushman and Zeynep Akata},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Robust Representations via Multi-View Information Bottleneck},\n volume = {abs/2002.07017},\n year = {2020}\n}\n", 'references': ['1ec049a29d369294a5a5ffdfb67e872dce899dac', 'd55902d896a1a890e8f5cf1be4768c5bf0d74ec2', '41fcef711faca9013fd0980a9f6ec1d23c9c76c8', '97f4d09175705be4677d675fa27e55defac44800', '9b09d296059909490096e34e9df2d95314787ad5', '1cae417456711c4da184f5efcd1b7464a7a0661a', '4aea3547974399a32d7aa7c007b10bd665e93fab', '96112f58cae6cbb85528ab1cb01550d106dc4ae8', 'fa7af221e7773033fed816f0fe29209cb4ed4283', '2b2cf76246466c266f9c9fe9bbfd43d918514e55', '77be85f6c3c465ef8e17d3ec6251794cf4ff5940', 'af3825437b627db1a99f946f7aa773ba8b03befd', '787c11692e202173367929af6a96f4ccdcdbd2e7', 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', '287547fc81364e64d196abb8d891ade3f6599a5a', 'bb17ff968ae1dbd772ac337f469d1ca915cb4c03', '8976e91ccae57a20c29f3c9d88bf45b19973c952', '9206cca6ced47353cf11f524c68d432d8496e171', '4d7574c0c4aca70e5811a8e33906f0106d6b76e6', '5ea2cdab68c69d7aef5a004495783ae7628193f2', '69569f5d9fa03f410f5e83299e974ad683523cdc', 'a90226c41b79f8b06007609f39f82757073641e2', '590c9a1ff422b03477f7830b20609f212c85aa13', '42e9055ec712ec9c7f0a79d963ea034a72dc7fa8', 'fa3e6f9361d75b9ccbad9baeec1e9025c2b0f6db', 'e8db8b3ae77c09e0b882560b06fbfb4b4690792e', '6a97d2668187965743d1b825b306defccbabbb4c', '9aea99b194a3473829cef62d71ec466a09cb462e', '0f899b92b7fb03b609fee887e4b6f3b633eaf30d', '415229903f91a1f3fc7404f5e5997fde025c221d', 'e40f36cea9a7620165543d5c77a3b898dbd83b7f', '64bfeb1ddd35838706e4fffc469234cc2f215631', 'eb42cf88027de515750f230b23b1a057dc782108', 'c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6', 'b3f09ea2a8cc1d82c2b27d71dd8f7451d178beaf', 'e2257e3f56ccb12875a57bc0a8cca1d9d7e93ec6', '5726c7b40fcc454b77d989656c085520bf6c15fa', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'e33cbb25a8c7390aec6a398e36381f4f7770c283', '56ac97a7d53702a0941c7bdebe41c682d9831627', '80e9e3fc3670482c1fee16b2542061b779f47c4f', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'f79131806747fce087d0fe73d0867cc621547b2a', 'e0293e4a293ff1a0137c6fcfd3be3274c16b9959', 'aae4efb3d412d585ea0dec03f933397c93caf989', 'c76c62c5ab6c076a80f925d277ef04dd36f6bf9c', '16d70e8af45ca0ae2c1bb73f3be6628518d40b8f', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '3121c41844faf48bbf61cb037d36045742f04091', '9405cc0d6169988371b2755e573cc28650d14dfe', 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c', '4f8d648c52edf74e41b0996128aa536e13cc7e82', 'f6a883e5ce485ab9300d56cb440e8634d9aa1105']}
{'paperID': 'b1e6716d068ee0d98aa213ad496ec189c10f9250', 'abstract': 'We propose an approach to training machine learning models that are fair in the sense that their performance is invariant under certain perturbations to the features. For example, the performance of a resume screening system should be invariant under changes to the name of the applicant. We formalize this intuitive notion of fairness by connecting it to the original notion of individual fairness put forth by Dwork et al and show that the proposed approach achieves this notion of fairness. We also demonstrate the effectiveness of the approach on two machine learning tasks that are susceptible to gender and racial biases.', 'bibtex': '@Article{Yurochkin2019TrainingIF,\n author = {Mikhail Yurochkin and Amanda Bower and Yuekai Sun},\n booktitle = {International Conference on Learning Representations},\n title = {Training individually fair ML models with sensitive subspace robustness},\n year = {2019}\n}\n', 'references': ['8d4e3ec650e8509f2ff001462f53064a3071c86a', 'f397df630e0708d411e76309b85f56440b853b86', 'f7f5a101985e66c7440deb9286f7c4602294f29b', '2c59645915e054d4434f2e2e007ce82a8c65b55a', 'c4afa2b3eda95a1194313394901e0e96e24cefaa', 'c8541b1dc813f3a638d7acc79e5f972e77f3c5a7', '70e28eb8ee40cf5caa704ac7f87940c0818ba28e', 'acd6de3ac2a3d9449aae51b87fbb03f6f0020954', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', 'c657b3fc93a24349117bf87296ea2b9b780706cc', 'c7330852a07170cd0e6990f5fbde5fca12b6ccd6', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '567e1d6f83f792f5532a60a020c5f22ac39ecf99', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'f7c0b4333f914546b144ba15ea38d0e8f29bb622', 'e2a85a6766b982ff7c8980e57ca6342d22493827', '37f5d47019f467c74acff22a38ffd4b98bdcb5d4', '352056cf4cf7e44fb8ebf863c4b632b759a39344', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', '6e77765dd3250fc671c413b44554087bad43ad92', 'ed6297433cfc580837e87592f550cc96296c7d0a', '5966d7c7f60898d610812e24c64d4d57855ad86a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'ccf6a69a7f33bcf052aa7def176d3b9de495beb7', 'f701b58e41d928cdcd8d733b638fd65a73623b72', '8a344f58fbb83871bf3e46b110557b645bdee862', '819167ace2f0caae7745d2f25a803979be5fbfae', '86c37cd1109ce5b465116695b7705444a45185cf', '51dcc0c6c8ea27f0a5a3071fb8c4b32004cd55d8', 'd450b0f12ae0437048e4047a630c31d902002d0c', '948dfbb55a93aa9585056a4b4dd3cd6553b236a9', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'f37e1b62a767a307c046404ca96bc140b3e68cb5', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '583b55367f787eb0c4e295707b642e63547b9806', 'adaa0523a5c9d5f92aa2009a51226391d8e62380', '43ed402570125c14cb2cbd1d158a55a1cebd0248', '42810f584d17fc7c355bec2d35caf5fc1b13f0a7', 'cdcf7cb29f37ac0546961ea8a076075b9cc1f992', 'e4350e816a350662ddb5f9ef92437aa8f3fd44f6', '1d174f0e3c391368d0f3384a144a6c7487f2a143', '8d56d4bc69a8c562434b9a129542bb79e9d6f1d6']}
{'paperID': '02b1607af35b48f0bd716367caf6a7428b969369', 'abstract': 'Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.', 'bibtex': '@Article{Hendrycks2019AugMixAS,\n author = {Dan Hendrycks and Norman Mu and E. D. Cubuk and Barret Zoph and J. Gilmer and Balaji Lakshminarayanan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty},\n volume = {abs/1912.02781},\n year = {2019}\n}\n', 'references': ['2ce8e6bf9687a479f4aa8858e0e4a75e4885c143', '87f6a7c014ce206ac5b57299c07e10667d194b39', 'cccc4fa0dba2eb11cecb382f0ffea8289263af84', '934d7bffdba0b560a80a518b99a791a16b3e198c', '1eb7f46b1a0a7df823194d86543e5554aa21021a', 'c703618d1f97a2d2184a09bbd73520034a19ef56', 'c3d846a3c51dc6423381257b95a4b821e778dce0', 'ed17929e66da7f8fbc3666bf5eb613d302ddde0c', '3562fefb64cd63ac1a6a0adbaa83ae73dd674243', '8c92054c26fb4c6dd7435bc99fbb8af3323eae1b', '9b2dba32fa1837216602259a707637ca54c6575c', '49b64383fe36268410c430352637ed23b16820c5', 'aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '1f5066018662b7c7d13a57611e6f118b2871d39f', '2d8c97db4bae00ff243d122b957091a236a697a7', 'd3eef9744324abc397c82b112b026aad3ec56708', '54fc23348ed840cb5f1fe2b41c80bfdcfc03631f', '6effa092456e30e7e54954fd28b755e0a75b52b8', '18063ed998c99bfef92fad8418610b97f863d878', '6f4afaa1ec7528c59aba86def531df6c524229b2', 'f723eb3e7159f07b97464c8d947d15e78612abe4', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', '80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332', 'e41bbabd84bfbce6ef66825c1a2d7eb869bd1202', '4feef0fd284feb1233399b400eb897f59ec92755', '2788a2461ed0067e2f7aaa63c449a24a237ec341', 'eb35fdc11a325f21a8ce0ca65058f7480a2fc91f', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'd65ce2b8300541414bfe51d03906fca72e93523c', '0d57ba12a6d958e178d83be4c84513f7e42b24e5', '802168a81571dde28f5ddb94d84677bc007afa7b', 'a93ddecb919b50f6072ae6447430786e002d4857', 'f6e0856b4a9199fa968ac00da612a9407b5cb85c', '6ff2a434578ff2746b9283e45abf296887f48a2d', '5694e46284460a648fe29117cbc55f6c9be3fa3c', 'b022f2a277a4bf5f42382e86e4380b96340b9e86', '1c4e9156ca07705531e45960b7a919dc473abb51', 'a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb', '3d2c6941a9b4608ba52b328369a3352db2092ae0', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '1aad4230be997ec83acd6b7c41d581eb8dd033ca', '0f84a81f431b18a78bd97f59ed4b9d8eda390970', '03cd6f2297637a322bdd4519b8cee331ef42984b', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '0302bb2d5476540cfb21467473f5eca843caf90b', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '1c5b068ce6dff86bf152312b99f3360456a00faf', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'df24c3011fc42b72195e876ce052a0a072a1d923', 'f6a883e5ce485ab9300d56cb440e8634d9aa1105']}
{'paperID': 'c68cd22de315a14587120e98bb02fdcf51edec46', 'abstract': 'Previous work shows that adversarially robust generalization requires larger sample complexity, and the same dataset, e.g., CIFAR-10, which enables good standard accuracy may not suffice to train robust models. Since collecting new training data could be costly, we focus on better utilizing the given data by inducing the regions with high sample density in the feature space, which could lead to locally sufficient samples for robust learning. We first formally show that the softmax cross-entropy (SCE) loss and its variants convey inappropriate supervisory signals, which encourage the learned feature points to spread over the space sparsely in training. This inspires us to propose the Max-Mahalanobis center (MMC) loss to explicitly induce dense feature regions in order to benefit robustness. Namely, the MMC loss encourages the model to concentrate on learning ordered and compact representations, which gather around the preset optimal centers for different classes. We empirically demonstrate that applying the MMC loss can significantly improve robustness even under strong adaptive attacks, while keeping state-of-the-art accuracy on clean inputs with little extra computation compared to the SCE loss.', 'bibtex': '@Article{Pang2019RethinkingSC,\n author = {Tianyu Pang and Kun Xu and Yinpeng Dong and Chao Du and Ning Chen and Jun Zhu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness},\n volume = {abs/1905.10626},\n year = {2019}\n}\n', 'references': ['e4a81b32bdc0adbf599bb8e98e53ec1adfa91878', '3e4a49b86c9c34e27e00a0f250b3d82f269cf153', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', 'd33deae7f654b07ac8a5c437a4fa018c29e6af17', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', '144a8fb6e9d573f7ffb0768846f94d7859811d44', 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5', '988a378f640eb7fb681f977d6cb1e0c830c07b4c', '676e40050453ddeb1387f8314478c0ac3681a8c6', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '41071dbbbcbb27af3fec70de045f19c28535f5b7', 'de49430578bb3f8de3e610423255662c45f17610', '20f85256555ad612148e52f9363e52f9d661728b', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', '13b2bc8101a2a7a0c95412c48f40ef95e798e9fb', '9db631435f7f79646a4e0a1841fbeb3340e44261', 'b3f83e8416010e9c3a705a0b6390d268e5ddf5c0', '29524f145db94cab2336da99f157e869d805dead', '804fb9542f4f56e264dd2df57c255a9a2011c00f', 'ca5642f522cd2cd44948c7e9f337c91e5f26fdcf', 'f78a911f516625d6b7b76a9a33c1eb14613341c4', '8d35663a80199b173d8cbd12dbf2300a9f86a021', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', 'b6b24dfaf4c9e498ca9b9ee9f82d8d0c5bdb77e9', '40addc9c5d6ab5668fe347806e94546e80e66595', '7005fe514458b538b7516b41af5f5e1971154070', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '9dc915697768dd1f7c7b97e2c25c90b02241958b', 'd4f100ca5edfe53b562f1d170b2c48939bab0e27', 'f60070d3a4d333aa1436e4c372b1feb5b316a7ba', 'afa0d49c1399c752d6f4665d75ecec640c000468', 'f220ef68612e68a5708001f2b596d742b941e773', '4b23012689e0f17912fb38d4984775e567cff8d6', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '8e37a3b227b68953f8067215828dc8b8714cb21b', '160a03c2890f3ef5436c25ef9b1758faa13807a0', '9ab7319dbe80549ba80e3320d0546d741a7a5791', '6cacda04a541d251e8221d70ac61fda88fb61a70', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '255d2c2af6d7abbbebfc03dab51cd8574ad3558e', '5fb1988aca0d8797daaf97dbfd56f54f7b84d230', '99cb08c76c120599abd1d1637e32aaf577f38d39', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', 'bd8f77b7d3b9d272f7a68defc1412f73e5ac3135', '21063765fc3dc7884dc2a28c68e6c7174ab70af2', '4cfd770ccecae1c0b4248bc800d7fd35c817bbbd', '1f4f8206c1d459f60324f06967173b906a12ef6a', '637c25f7e8f37226f829cd9264d4eeea50f75e7b', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '6fd9e3cb0cf23c8ef4aa7065d9be407c45250bff', '77f0a39b8e02686fd85b01971f8feb7f60971f80', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '819167ace2f0caae7745d2f25a803979be5fbfae', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '5aa26299435bdf7db874ef1640a6c3b5a4a2c394', '4d376d6978dad0374edfa6709c9556b42d3594d3', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '4543670c4b2d88a9b67525e0084044adef94ae76', '91bdaf3f1226e4065c4296d5c362906ceadfc631', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '46f30e94dd3d5902141c5fbe58d0bc9189545c76', '8e6c6086ea725737aa6081a57ea68d43a24ca3b9', '7114418f520880dd202108f20fe8e6ce70db68bd', '5d90f06bb70a0a3dced62413346235c02b1aa086', '7fb1364bc382c67901f3e3eb12901874570788ca', '735d4220d5579cc6afe956d9f6ea501a96ae99e2', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '6946ae0c23257586c12d01675e05167d74cb89fa', 'abstract': 'We provide a framework for incorporating robustness -- to perturbations in the transition dynamics which we refer to as model misspecification -- into continuous control Reinforcement Learning (RL) algorithms. We specifically focus on incorporating robustness into a state-of-the-art continuous control RL algorithm called Maximum a-posteriori Policy Optimization (MPO). We achieve this by learning a policy that optimizes for a worst case expected return objective and derive a corresponding robust entropy-regularized Bellman contraction operator. In addition, we introduce a less conservative, soft-robust, entropy-regularized objective with a corresponding Bellman operator. We show that both, robust and soft-robust policies, outperform their non-robust counterparts in nine Mujoco domains with environment perturbations. In addition, we show improved robust performance on a high-dimensional, simulated, dexterous robotic hand. Finally, we present multiple investigative experiments that provide a deeper insight into the robustness framework. This includes an adaptation to another continuous control RL algorithm as well as learning the uncertainty set from offline data. Performance videos can be found online at this https URL.', 'bibtex': '@Article{Mankowitz2019RobustRL,\n author = {D. Mankowitz and Nir Levine and Rae Jeong and A. Abdolmaleki and J. T. Springenberg and Timothy Mann and Todd Hester and Martin A. Riedmiller},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Reinforcement Learning for Continuous Control with Model Misspecification},\n volume = {abs/1906.07516},\n year = {2019}\n}\n', 'references': ['03367b0458e08105475dba32dbacee1fe0b7f311', '896e5529de1da1e4494033404721b70339bb9557', '12c0751b4f51ed833172a713b7e32390032ead93', '655cfe96e20675183dc8c2acbab659bce54fd6f5', 'd37a34c204a8beefcaef4dddddb7a90c16e973d4', 'b47512c1444374add267ed7232b689037972da01', 'eb37e7b76d26b75463df22b2a3aa32b6a765c672', '0c284ef8a3366b4912d59b6af7e70cbb0d070f80', 'cab81775baae7ba2d056ebbc60437f2e03358ca3', 'd72e69eacd4afeac33f71d07c484686084e55b9a', 'a8ef08940341381390d9a5672546354d0ce51328', '4e43d0365e4e922123de54c5e9a430bbced4a817', 'a9a3ed69c94a3e1c08ef1f833d9199f57736238b', '0af8cdb71ce9e5bf37ad2a11f05af293cfe62172', 'b7473703c1e56b1e48b9d69eb055d95dd8ba9e83', 'cf90552b5d2e992e93ab838fd615e1c36618e31c', 'd0352057e2b99f65f8b5244a0b912026c86d7b21', '96a067e188f1c89db9faea1fea2314a15ae51bbc', '3ed67ded2b4d3614b38798b3f17a8e69803d0980', '8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a', '3c3861c607fb79f3fbf79552018724617fc8ba1b', '1464776f20e2bccb6182f183b5ff2e15b0ae5e56', '3a472aeb7312383822da385f03141f2decb3d124', '4a026fd65af4ba3575e64174de56fee093fa3330', '1c4927af526d5c28f7c2cfa492ece192d80a61d4', '6640f4e4beae786f301928d82a9f8eb037aa6935', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', '199a292bb72d57cae4a4455249f438a4f9ac96b8', '484ad17c926292fbe0d5211540832a8c8a8e958b', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', '9eae0c6ca4a52fc5e6b6f9eb111ab6fdbecdf9a6', 'e27fb84fccd3d9724df8ce35fe14149da5de3251', 'a86e7b223be9005d30ad806803c09fedfc804624', '6db16608fccddef51202af84112b34cfebfbe20a', 'ab4a1c4dfe23b3a1e3d077df467452cc68f64de8', '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d', '87bd79e8444d6604dc2b2d8342f3e80cdf3468b1', '6e7891253247b01b346e92fb81c2ba589ca8e428', '1d75b5275f24c422b6f73dd77750b402d997d060', '97efafdb4a3942ab3efba53ded7413199f79c054', '427ce4ce5f64295f39c8714de6ff09fb3bb7d187']}
{'paperID': 'd5fc1374bfb839a65e928c8554ec09421739c2b7', 'abstract': 'Since deep neural networks are over-parametrized, they may memorize noisy examples. We address such memorizing issue under the existence of annotation noise. From the fact that deep neural networks cannot generalize neighborhoods of the features acquired via memorization, we find that noisy examples do not consistently incur small losses on the network in the presence of perturbation. Based on this, we propose a novel training method called Learning with Ensemble Consensus (LEC) whose goal is to prevent overfitting noisy examples by eliminating them identified via consensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC outperforms the current state-of-the-art methods on MNIST, CIFAR-10, and CIFAR-100 despite its efficient memory.', 'bibtex': '@Article{Lee2019RobustTW,\n author = {Jisoo Lee and Sae-Young Chung},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Training with Ensemble Consensus},\n volume = {abs/1910.09792},\n year = {2019}\n}\n', 'references': ['aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '5d99d520410efec67a515e3bcc20ce8868e13b6d', '568675640f381fbc13998015c2854298877b7aa0', '66169adc068ef55f17ce1b1b51efb8778673ecfc', 'a2b5d224895d96bfe2e384e2dcf1ebd136ac3782', '6c4f5c95004cdfde3b58d543b8cb25d6b91ebed6', '4f686309f5a34d5a5c687539b71bac0bafd8476f', '622727f595542afa3caf8802927d880818ddb17a', 'e5a95a679774e069e1e36d96f92bac6b93027118', 'e061d23b68e7d4aac5aece4794c044c80e638dca', '0abea3322379506b017784309d9bee2e375e3e5d', '780d18205252a4d3fc1ab4b4a2db55a370d0105e', '2b1a3d7e6045dc6b544a548b372c1f8492b85967', '306a2e8ca31fdcc148618d37074785c290f96375', '5de5848dc3fc35e40420ffec70a407e4770e3a8d', 'e644a409b4a4c6eaedffe27efbc5c76280b34c61', '5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf', '96f4d4fc345698b9b44f034c0d63b704772c8386', '0ebb83e5c28719c7b5cb5bc482e62f835fb0d116', '802168a81571dde28f5ddb94d84677bc007afa7b', '54ddb00fa691728944fd8becea90a373d21597cf', 'bc550ee45f4194f86c52152c10d302965c3563ca', 'd2e4587744a89bad95fea69e08842cad6c8ff0dd', '91d331d2bdd5fc86400c40c497bcb4c741c652be', 'b5c26ab8767d046cb6e32d959fdf726aee89bb62', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '4b675d8f63888d7d6d7d77a0834efa5eaded64c5', '77e3c48aa10535276e7f570a3af594ba63de7d65', 'ad84f49b2cd1b85a6d7df2304144a093f5b610a8', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '34f25a8704614163c4095b3ee2fc969b60de4698', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4', 'f6a883e5ce485ab9300d56cb440e8634d9aa1105']}
{'paperID': 'f324002c4d22abf7ed5f24699151719f29625d08', 'abstract': 'For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound – a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the “all-layer margin.” Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. We present three concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin and demonstrate that our algorithm improves test performance over strong baselines in practice.', 'bibtex': '@Article{Wei2020ImprovedSC,\n author = {Colin Wei and Tengyu Ma},\n booktitle = {International Conference on Learning Representations},\n title = {Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin},\n year = {2020}\n}\n', 'references': ['c563c0e06684f42bc5d76dfc7304581c11312393', 'bcfba69c2fadf2efea83be12fda2601f8d4681af', '3f46ac38812f9f0f99ff1edddd85d71a84da4497', '0204871837acb118871e8d1bb59407da73142333', 'ec7c8b82009fee95ee807da79045c83373378fef', 'b2162301e7c2fc90f5eb62471b4f23ece8da1b8d', 'c3d846a3c51dc6423381257b95a4b821e778dce0', '6b81ffcb4b461f7f657cfb4d9b076cdc595b1077', '816db3b923a870f02391759c214a024a8d2c548c', 'fa2bfdf70e43b84a632b679a8f35999816245919', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '2236e2263cab1a9e4ba71fdb7cedc7092bfb6c2f', '9c985db0a4a255a06e0fbf2ce147ea741720f9f0', '1438fab07a94351eaabcbe92983fbefaa208b2f3', '3a606480406886742572a956e221e986c65d94c1', 'e3c6d32182992310b802ec0a2c261358e72487ae', '47149cbfee9dac6c092462bbe13cf18cf9cc9314', 'cd34714b819ca75bba63fea36aff54d66f458d1b', '67a97032fd3ad81cda45e1e5d4a1a7d851494525', '1b9c6022598085dd892f360122c0fa4c630b3f18', '804fb9542f4f56e264dd2df57c255a9a2011c00f', '4a5a17d7849b91a3af583c7b99403844e1a5cdb1', '06f35a25c12d33a93578711eccf7cceab4e66d54', 'e837dfa120e8ce3cd587bde7b0787ef43fa7832d', 'a9022d8ffb5e417458fba9a280f90c1b08cb6c73', '033f7570be9877c5a4bcbb71f6aec8f95cee3608', '8b4b861583f698e89c8cd9e198aad86809a71de7', '018a844cd7a496aed84f166e4b02ff547c3b5d16', 'f220ef68612e68a5708001f2b596d742b941e773', '11adc8bd35bd897502f9b5452ab7ac668ec9b0fb', 'f657e68dde470641ba1a6cd7ab755e6359a32840', '4fc3ee440c2b0f66255a9e6966cee871ee0cc6da', 'd53fb3feeeab07a0d70bf466dd473ec6052ecc07', '9753967a3af8e1db1e2da52a9bb3255bd1ce5c51', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '4e8917e73e02c76d55ded62e43541d44684a4c8a', 'b6583fe9c9dc52bb129aff4cefc60519349f3b4c', '54ddb00fa691728944fd8becea90a373d21597cf', '4cfd770ccecae1c0b4248bc800d7fd35c817bbbd', '8ec5896b4490c6e127d1718ffc36a3439d84cb81', '1c4e9156ca07705531e45960b7a919dc473abb51', '0f7c85357c366b314b5b55c400869a62fd23372c', '02480b5d060eb4cb2228ac7e824fda22b29c3e9e', 'eb42cf88027de515750f230b23b1a057dc782108', '91bdaf3f1226e4065c4296d5c362906ceadfc631', '34ad20cf5f0043b47de601e0f77e6fb4a7d824c6', 'ae6e206c8c2994e04c3fdc5bd97d81fdd0f27493', 'd37fc9e9c4fedc32865b08661e7fb950df1f8fbe', 'f5616db9e1e377ef580bbe5c05b6e969b16dc222', '009f35c0e453f2435efd8d8ef8086b76b294967a', 'a5c8ca9177054499e43898ac73d1c1b6e2a45928', '6388150296152c8173fae995e30c80a86d7cf1f7', '2599131a4bc2fa957338732a37c744cfe3e17b24', 'dace63396b81af8f3b34112d8ab744380ab024e7']}
{'paperID': '3a2ef5c27e7140f819a4c99444b7d4fd533dca59', 'abstract': 'We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a "manifold" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.', 'bibtex': '@Article{Lai2019RobustSR,\n author = {Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\n volume = {abs/1904.00152},\n year = {2019}\n}\n', 'references': ['10965afaf73892bc989c9242442ec01cf22eb365', '68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6', '5db790198b9acf4e5efe350acdd814238fcacaa7', 'ddbc185ed132f09673ef048aab0a10a25ee6d27e', 'f35ef556eff448479dad4f139eb747c5e2911384', 'dbc7401e3e75c40d3c720e7db3c906d48bd742d7', '98af3dc5af084967adcfeda40b58b0012f98e1e9', 'b3acb6f183b5f4b651f53c0eec5cb5c805224ac1', '388645c44061f6e88fff0ecdad2f622936207d67', 'bd32166541a656a4420c5c1ffc5701d09eedf2aa', '8a6acba7fb2aad1299fcf35701417e063d410ed4', 'd66d93fafe61e878b8da5a8d0ecee442d44b5be0', '1003462fd2d360542ea712430b13cf0c43fc7f9d', 'da0ae07cafd76210fed537e4a776cb52b3c50174', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '2b75ba7f75170b73d913c515cc0deefef6c88f5f', 'acd87843a451d18b4dc6474ddce1ae946429eaf1', '65f233478263b77b48dc435d73bf3647f4ae4027', '1d4ec24a6da3be62dc5d7efbae2a101c63f187e8', 'edf73ab12595c6709f646f542a0d2b33eb20a3f4', '2f85b7376769473d2bed56f855f115e23d727094', '10a498003e9204f5fc1328e706510a37e514d8c7', '355692eb86b06a0a23af45c106cfb02c95bf380e', '77f0a39b8e02686fd85b01971f8feb7f60971f80', '1c06870e1ecc63e120e45a2283ca4b72c153e867', 'a4cec122a08216fe8a3bc19b22e78fbaea096256', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', 'd47312e1019450097adaf50347120fc6db6c5323', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', 'eea4ca46542125e02cd7b6de60f28c3710b3f7a3', 'dcd9f5d61bc9a70c40be84a8d78fbee822ffcd9e', '9c7da2c47c66afe9a723af75908f0b5c45832e61', '67d77348984b1793ebc65be0684a50bae90bb39e', '437a48a476268ec4bfa40b7446a3ff95291a4fe0', '168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74', '73fa4d95d635d6b914c967e41b5dd3321751672a', '07f87b91ddabb7f71f43f20ab1b7d572904fe801', '01625cba9f8a783994377d4f35aa765242faab4f', 'a56d20496e084f99823493eebf28a88a60f92e17', '71d1ac92ad36b62a04f32ed75a10ad3259a7218d', '13fb2d7aef8b6352e99c3ccd268fc139c34cf657', '0439f9ea8dabf18581322f33aa25424851251d21', '73f76a40ed20aa3c6a8e27e4db4a8c102e7b4c6d', 'ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2', '0f2cd9d0e82d4868f7c9c0fb06fcedd7237a1ac7', '3f5dfbd571ef719a2c5dc1ed63ca6a3702f7861e', 'a7c828184693a453a6c2867dee233ed054b2012e', 'bf206bad6a74d27b40c8ea77ee54e98e492fb7f9', '38b3a4447a47a6a6ed1869f3da03352c487f8fe3', 'da6fa08473e1e7dd84abfb0533e5d17d7aa7d422', '15e46987e753a7f01f3d0f3b23e5a948366f94e8', '061146b1d7938d7a8dae70e3531a00fceb3c78e8', 'dcb207ce848b358aeb2e4698c9ea1ad273ce98db', '68f52ec7b398998f21034cf19972fc54dc057cad', '5833aa3ab163bf57a969b74ff3c3a66babe19fa1', '1f6921bd11f8a37e56049e68fc584417bb10c7ba']}
{'paperID': 'c2c5d4650beb72a0e00a4c384064f4ab2ddef1dc', 'abstract': 'Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018a) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high biases due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or the value function estimation is perfect. In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that our method yields significant advantages over previous methods.', 'bibtex': '@Article{Tang2019DoublyRB,\n author = {Ziyang Tang and Yihao Feng and Lihong Li and Dengyong Zhou and Qiang Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},\n volume = {abs/1910.07186},\n year = {2019}\n}\n', 'references': ['035c298e77b21968425eb44598c0516b1efbe4be', '25a7b8c2e110a1bcd5c42ab5de55a0c08b0b8846', '4708b886a622a821f64d1be30638b78a7dbeb64e', '875280d96b2f138902061ae6409249ee4ded0da3', '197cd4b7cd242243b5e8fc48ee08828882c930a1', '922c32ccc83b9054df2bbef7e81e20e8edca1605', 'dc4ec37102afb166b96abc268ae3dc15e230d776', 'e81ea45d8bec329fdb11fd84990852f620895d6f', 'dbba10eb7fb37626ea554cf7f64ef514ddd642be', '4382b7abb3bda846cb999ce60f6858ecb56acd4e', 'd7b32bf240bb3b2f2a98d87ea21b669226e0f9d8', '1bd66197d692581f4ac507107184ca9110b61d7d', '5fd8f4d7bcea08e15229a6d0b6ba1bc11d44f348', '00d6c4aa261fb3fb4e8e76997bce91f9336f250f', 'ba847beb3ed679ff56d0414c04748de88ed46af9', '1f81de57298677c98736d8b6f6736a279c75cc35', '2c2bed7759d817b023732b6e9b5d11911ea49dd5', '117f12f75cfe4d5705d7beabbe961b2ced0d7025', 'd402ec2bd0ab0e5399b71f75076d589df914a4e3', 'ec8a2f6cfe72309f5f1608d22ec28778d3ee976a', '2fdb536da39a014c598ea67b0db88431fcd852a8', 'fc71c22a316ee4d8cbf27fabf5b4c0c9041c43cc', '15b15083809a1a951d63821a21cbbc12af8dddef', '139a945f22e53a6c2eab19105f3ab0c7a67f68bd', 'b006aefa9c8be8c33138c4c5352f4d2d44378be9', '5ccf7658018981bf492d0c8d66277d22ebaac815', 'd6554953543050bba991c58dc637f10fc2474e6e', '28886248acf75e7b1cca167669161d9d214ed662', '7e348097aba114157dfbd649bca0c59b981deb59', 'b2db5541059288472ca246acdca6ead949326864', '8a14ac38f66996913c4d7f3a3141294a602fd8f3', '47ba51a5913a6f2be8ab874a5f2d246735c89614', '2edf95e2c4323783731c388b33ba79bb72faa5d8', 'a82db864e472b5aa6313596ef9919f64e3363b1f', '8090121ad488b4af27bc59bf91b62e9c6a6f49c6', '4217bf521e0ffb4b2a36673478caa29c223a2b84', 'e5309d90535f7f798ef8482e2450f78900a53651', '97efafdb4a3942ab3efba53ded7413199f79c054', 'b907404e5676c338f25592e582abd91a4b2ef2c2', '142ec0ed30a36cd247cc14c2f303eaf5eb63df53']}
{'paperID': 'dc0def651f7ff53d7b3d764924b5c2c28024cdd7', 'abstract': 'Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models. We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided. Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness.', 'bibtex': '@Article{Zhao2020BridgingMC,\n author = {Pu Zhao and Pin-Yu Chen and Payel Das and K. Ramamurthy and Xue Lin},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness},\n volume = {abs/2005.00060},\n year = {2020}\n}\n', 'references': ['aa647f298e43192a479d5c70b4378a58ca1e2333', 'f91d9aec51826e85050029ca15f0ba391534c83e', '549180d76bc15ec3fcb9fa0e34d06bf7173faa6b', 'bd4336b6015d4d680a27c25a0ed296df5692ddf1', '42658c812d60d26a0bdad91b4d81e8620b994bf6', '5ae786deaa875613e85ed2df0dbeec4301109f74', '633ccadcde3bfca87f91bfe5ef4aa297fb2da2f4', '77030ffbd1119bf98614459bc087a8a59ce5697d', '71f212b84e8f784bb2c17bf9e2415b0b780f2e73', '67c6a88a877b195e99b655197dc75f54ca2fca1a', '1d8d6c71a9dd22c8e5f82e42afb8fec3c31e1fc8', '28fdcbe150096451521240371fc5978c4f124afe', '5a40a8d27d37948dae9ab62337dc6ec492caa611', '98ebd263571748a30eb99d9b9d58bc0519be09ea', '5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1', '821fd5bed14d6d06c25fbf44123fd7be382f7b4e', '289f58af35a5a7509cd8addeb450d45a2dc86bad', 'e5b4a134836f376fc368fb8cdb194c8ca2a8828e', '1b9c6022598085dd892f360122c0fa4c630b3f18', 'ba4883eafede39be6494a80c8b999abc46fa6b5e', 'b3f83e8416010e9c3a705a0b6390d268e5ddf5c0', 'ab2f8be8cd0474b3ab2c77b1bfeee25f25cad076', 'efd7b7aafeb83b8a8d6fd90a35d6fb6a62f5f695', 'c6d4ef1a98b1b9e5875b79efd3ef2a73403a0884', '2c90d366126a3ccd3c43e47891730650003059da', 'f6195d8dc6aad8231e97b563246f2585842bc68b', '2c20e7220269b28fb1935a83d0e7f2db330aa691', '900d130f25f453948fedf7df5faf4ab710fb5e94', '8f1dc314680ab0e2b780c093546395b499bb2b67', '9ab7319dbe80549ba80e3320d0546d741a7a5791', '33472f3b55747e8aa36f4db57070de1e0e321218', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '2612541a89857949bc512b6fb2ad7f0c153cb97c', 'c2a1cb1612ba21e067a5c3ba478a8d73b796b77a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '78aa018ee7d52360e15d103390ea1cdb3a0beb41', '77f0a39b8e02686fd85b01971f8feb7f60971f80', '53b047e503f4c24602f376a774d653f7ed56c024', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'eb42cf88027de515750f230b23b1a057dc782108', '0ad3053eb32ecc5a13d9a6aa868b5c86c5663372', '990a02f20529f5ce3b382f1d54648afaab391179', '3866fe85f6d7d1f38e6d6d4f112b51e03ab0c68b', 'cead00c7b3384653ce8013139a5d4543d96fcd07', '08f7ac64b420210aa46fcbbdb0f206215f2e0644', 'f2a0fbba89f0d18ea0abd29639d4e43babe59cf3']}
{'paperID': '6189bf5f4c851ad0217a782509f8818aca4c7ff4', 'abstract': 'Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding the behavior of a given model and for obtaining safety guarantees. However, previous methods are usually limited to relatively simple neural networks. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous work. We resolve these challenges and develop the first verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of words in sentiment analysis.', 'bibtex': '@Article{Shi2020RobustnessVF,\n author = {Zhouxing Shi and Huan Zhang and Kai-Wei Chang and Minlie Huang and Cho-Jui Hsieh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robustness Verification for Transformers},\n volume = {abs/2002.06622},\n year = {2020}\n}\n', 'references': ['46f2c91124681b9a2653249efc058453c476dd75', '8974985efa379dd03be434c6a2958e6563b0b6ab', '07398e448180ad75c44d30f23a65289d40ff6f52', '4690190d6c110f7525f7250e1acf4a4eab42519f', '2527626c11a84f15709e943fbfa2356e19930e3b', '5aec474c31a2f4b74703c6f786c0a8ff85c450da', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', '3608b958ced869c674877e3cba03f7b04befa775', 'd8d4cb8e8893b3ddb940134083582566d45da22e', 'b66a943dd745c0868d03144d60b7cd2aeb3c2ba7', 'e0c6abdbdecf04ffac65c440da77fb9d66bb474c', 'dd9d4111e218628954047be0f315fa61be108806', 'cb60351c4e2f9f244bcbed2eb44cdce86a029b4b', 'ef1ca41cc6ed1b34cfe3d34a6150912cfc5aed3b', '210fcca62c05e84d660330fbe1c2648ab0984154', '5473175211aff4a8a099c44d1a57802d1b7ecf9e', 'ad640ea420d49a969a995632aed22c21251891df', 'f20de741e2d4ece239261de010d6d9beca3b26b9', '44d7b5a66faeaccd7d3e2c36a3b51f4f13f73f4e', '7ad8c18994108a630c4564400f6137bf4d8b7818', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', 'f7f1b5437b9c97d67fae59142b242ce079d7cb08', '98cc371f4e3a39b5c69b4e8980a5990f9011f223', '97faeefa771e8cc8e55159e2bd03e6f5eef249a8', '75339d34bdac0d21a41461228ec6088eecdf857a', 'd21fde0f55ee0285c66334d37b8920c867959784', 'a8cb3df07c340f47fc0f225282e46757a93ca0d0', '2410923ed90b099e3f5565b63e789f10bf70ec4c', '53dcd6068586d50169877d145df550ff3f568221', '9db631435f7f79646a4e0a1841fbeb3340e44261', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', 'd2df6969b185a4017048f996d0e7cd1859c24e67', '8d35663a80199b173d8cbd12dbf2300a9f86a021', 'fdfbabfd7b3712a4c2cb864ca0ca5c201dfee5a1', '1db9bd18681b96473f3c82b21edc9240b44dc329', 'fa12574c228542151ccd7d4e3f42cc4896cd274a', '514e7fb769950dbe96eb519c88ca17e04dc829f6', '9de69a46e6c619255eeffbfbb6c7b7163690eb48', '4b23012689e0f17912fb38d4984775e567cff8d6', '91f4ebdfb4618e9a7bbcefc8b64e2f7d6e176545', '3502b5ef1afb16f76bcae33db17179195bbcdaae', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '255d2c2af6d7abbbebfc03dab51cd8574ad3558e', '333416708c80d0c163ca275d1b190b1f2576fa5f', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', '1439e05971a053c2368e6dee6d484b43c833d43c', '51a55df1f023571a7e07e338ee45a3e3d66ef73e', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '687bac2d3320083eb4530bf18bb8f8f721477600', '42371cfea5154e77a5057cafde7dc00a6446ea16', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'bb92676f9ec13783ac664c268191f20944718f95']}
{'paperID': '7ad9822c1de2b61708b803e8a0a548718cefdeb5', 'abstract': 'Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as "outliers" that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual\'s information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy can improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection.', 'bibtex': '@Article{Du2019RobustAD,\n author = {Min Du and R. Jia and D. Song},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy},\n volume = {abs/1911.07116},\n year = {2019}\n}\n', 'references': ['3b1941105317edaef6ac5995089d6d916e5fb483', 'b5c93d5438fcd0f21b5cb0b962e392545671e71c', 'fa11df1157486731afe86af7d6d9254a82d4dbd6', '520ec00dc35475e0554dbb72f27bd2eeb6f4191d', 'f9313ada269360c9faa74385d966122e5a20e69a', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', 'fefeded74334e5eafa47c5df6de2837fe3b7502d', '18cfd4b9e35fb12fbebedb0fdc3f7811090372bf', 'fb6409e86fbd27f65b6fbbf7a448a1be958bd206', '64ed3a3af683ddf2619173dda156e741c1fb277f', 'c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7', '573fd2ce97c70bb29097e8efb28a27af791225ca', 'e9a986c8ff6c2f381d026fe014f6aaa865f34da7', '04c159a99bd8083b2b609d9b39611a57a4114553', 'e3ab07e76fd97a6014332a8a0ffa09af02696a8e', '8963b5c1a754741d6599b95e47818fe3b79a6a51', '0023582fde36430c7e3ae81611a14e558c8f4bae', '36445111c9f9eb6763fedab5294aca792519f925', 'e6f273cbb59d882c7690413dba22dbcbd7c9b64e', 'b65fc8f5e7329f0476bc7280f0ef6b91a8c8484b', '168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74', 'f8558a09553fc35415b271019be9f7d44354073e', '01625cba9f8a783994377d4f35aa765242faab4f', '852094207ef6083d807a5215028e46e50685acbb', 'd612d7c21d4130a457968273d79c2c2f6946953d', 'c1e25bb0c3d0c3a8c3b2bf8e9735a9aadc175b45', '9cc44c3c3b57cbb95b356bde75985e12ae3ced67', '70fda5147aedd42c64143a464117b5ffde18a2e4', '8c23ea0ed7badd70a8e26dcea73f2d673cc0c74d', '27222787908c3a1c6fb6c4b5cb5ef8b2542f1b3c', '2618dbd8bc6bc401fbc202342c00cd2ffefcbe4f', '59bb8d6c3eec8f925710db4d2488e2a23167d3e8', 'ba10c37a6a24276f5e67a22a71d0d511c01cf5e1', '53f2bf254c530c4412a8892896422511bc2cee45', '96bc069f7734c062cd9a15ba686ef3a444bfb914', '9f9829038c1374f34143e77dd42ea469e8c145ac', '7e2fd9c6205613c4858b36ebb4e1b655d915c099']}
{'paperID': '6113062a0d646080aa8ec726908fba9406f4eebb', 'abstract': 'Deep reinforcement learning has achieved great success in many previously difficult reinforcement learning tasks, yet recent studies show that deep RL agents are also unavoidably susceptible to adversarial perturbations, similar to deep neural networks in classification tasks. Prior works mostly focus on model-free adversarial attacks and agents with discrete actions. In this work, we study the problem of continuous control agents in deep RL with adversarial attacks and propose the first two-step algorithm based on learned model dynamics. Extensive experiments on various MuJoCo domains (Cartpole, Fish, Walker, Humanoid) demonstrate that our proposed framework is much more effective and efficient than model-free based attacks baselines in degrading agent performance as well as driving agents to unsafe states.', 'bibtex': '@Article{Weng2020TowardER,\n author = {Tsui-Wei Weng and Krishnamurthy Dvijotham and J. Uesato and Kai Y. Xiao and Sven Gowal and Robert Stanforth and Pushmeet Kohli},\n booktitle = {International Conference on Learning Representations},\n title = {Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control},\n year = {2020}\n}\n', 'references': ['00d2739b1b3b4d2fa8ffc090c68225a5e3dd96ef', '6ff50528f3d7c72772f8c0e3f8398f9dd8e06575', '553e6c8a79a24abc554782bfe2c5a0ecb31bbb61', '56136aa0b2c347cbcf3d50821f310c4253155026', 'd355e339298fc2ab920688c1709d4ba6476a2bc6', 'a8ef08940341381390d9a5672546354d0ce51328', '27dfecb6bb0308c7484e13dcaefd5eeebba677d3', '6a5704ac5fdacb7121a0c02a9be4de2bdc5a40fc', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', 'bb430ec2f25e4a1513073a2a4098cbb942c2e3e0', '236b40f3144b95cd84779484c8269092122920aa', 'c8c16a56d2a9520197da9a1546f517db5f19b204', '62e11a7ad4096521b8ba1a30c1994648197330d1', '024006d4c2a89f7acacc6e4438d156525b60a98f', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '936a67aad36a9d9a7799237f0499d2f588d6e8ba']}
{'paperID': '6e635f131c37d481f5b8778c5823a35201876150', 'abstract': 'Defenses against adversarial attacks can be classified into certified and non-certified. Certifiable defenses make networks robust within a certain $\\ell_p$-bounded radius, so that it is impossible for the adversary to make adversarial examples in the certificate bound. We present an attack that maintains the imperceptibility property of adversarial examples while being outside of the certified radius. Furthermore, the proposed "Shadow Attack" can fool certifiably robust networks by producing an imperceptible adversarial example that gets misclassified and produces a strong ``spoofed\'\' certificate.', 'bibtex': '@Article{Ghiasi2020BreakingCD,\n author = {Amin Ghiasi and Ali Shafahi and T. Goldstein},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Breaking certified defenses: Semantic adversarial examples with spoofed robustness certificates},\n volume = {abs/2003.08937},\n year = {2020}\n}\n', 'references': ['8a67aa574b0c6ee161ecc286506d79f44c43b74d', 'ff22e140a0423f1cf0595d213f36402668084014', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', 'fb38fc75f58cf8e171d59b868b1afbddbb9a28eb', '1e8a10b781e17234267200880a4dc99c41b0e29c', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', '7c0a73771778fa8362c3e3abe7734dc9b1de3c76', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '263f845e25c1df3dffbadbee4f17a03fd0cea5dc', '43a4a354b67ab6d5531355a368094815d2d2593d', '052252e5d5523e9b6488f0f6022bf3621f44e0be', '750fd4f2a6139387b4f6245d3fd1013a8c8cf702', 'de49430578bb3f8de3e610423255662c45f17610', '769da7360c0c9c199645d9523ec576ad576ea4b3', '75339d34bdac0d21a41461228ec6088eecdf857a', '20f85256555ad612148e52f9363e52f9d661728b', '5023544ad6fa49b35526a62f22207e43c4db870d', '48d88124dd89d988386ae452516dbba6c4a96e85', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', '0314e777333a63aca5735ea136c74e113aa8801d', 'afa0d49c1399c752d6f4665d75ecec640c000468', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '99cb08c76c120599abd1d1637e32aaf577f38d39', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', 'd2c733e34d48784a37d717fe43d9e93277a8c53e']}
{'paperID': '161446f6c04f0451bf77cfc8550a2e654e056571', 'abstract': "Current artificial neural networks (ANNs) can perform and excel at a variety of tasks ranging from image classification to spam detection through training on large datasets of labeled data. While the trained network usually performs well on similar testing data, certain inputs that differ even slightly from the training data may trigger unpredictable behavior. Due to this limitation, it is possible to generate inputs with very small designed perturbations that can result in misclassification. These adversarial attacks present a security risk to deployed ANNs and indicate a divergence between how ANNs and humans perform classification. Humans are robust at behaving in the presence of noise and are capable of correctly classifying objects that are occluded, blurred, or otherwise distorted. It has been hypothesized that sleep promotes generalization and improves robustness against noise in animals and humans. In this work, we utilize a biologically inspired sleep phase in ANNs and demonstrate the benefit of sleep on defending against adversarial attacks as well as increasing ANN classification robustness. We compare the sleep algorithm's performance on various robustness tasks with two previously proposed adversarial defenses, defensive distillation and fine-tuning. We report an increase in robustness after sleep to adversarial attacks as well as to general image distortions for three datasets: MNIST, CUB200, and a toy dataset. Overall, these results demonstrate the potential for biologically inspired solutions to solve existing problems in ANNs and guide the development of more robust, human-like ANNs.", 'bibtex': '@Article{Tadros2020BiologicallyIS,\n author = {Timothy Tadros and G. Krishnan and R. Ramyaa and M. Bazhenov},\n booktitle = {International Conference on Learning Representations},\n title = {Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks},\n year = {2020}\n}\n', 'references': ['aa2343a94a6ae7cc4e6434801abf03c9feb6bb5f', '988a378f640eb7fb681f977d6cb1e0c830c07b4c', '54fc23348ed840cb5f1fe2b41c80bfdcfc03631f', '1b9c6022598085dd892f360122c0fa4c630b3f18', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '651adaa058f821a890f2c5d1053d69eb481a8352', '03a507a0876c7e1a26608358b1a9dd39f1eb08e0', '1b225474e7a5794f98cdfbde8b12ccbc56799409', '2c20e7220269b28fb1935a83d0e7f2db330aa691', '8dce99e33c6fceb3e79023f5894fdbe733c91e92', '6114cdf58aa606ceaa998ed883c01c692207d473', 'a8c6ad02756387456422d4bd1727c0932e8c9b04', 'eb4d7688cd03f3863a175149f5fa293140f9df30', 'a93ddecb919b50f6072ae6447430786e002d4857', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '975b6ec05f04662a967af8c7504b7f552a0ee0bd', '2e67d919815a073d1dbc6db3153697578257a28d', '846aedd869a00c09b40f1f1f35673cb22bc87490', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '819167ace2f0caae7745d2f25a803979be5fbfae', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '1534867c76db0315c7f50ebc14099940dd860a93', '1cb3a0ff57de3199fe7029db7a1b8bac310fa5f8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '4543670c4b2d88a9b67525e0084044adef94ae76', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'c201cff713db94cd98fcd1fb1aea2b00e0299307', 'fd3c816a35e8cc14330eeaf5e471c24827a36331', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '3a8cc04d591c9e1c69f8bc9df9bdc5d23cd13ae5', '83897de64130d56de390eaf2c17e157946284f47', '86c6461f6dfb3e990f0ecf76bbdac58c271be6ba', 'f15be3dfc68f65090e9a81def42ccfc066bcf558', 'bc45769a57087ffceab785bc624b99acf0aa5c7a', '77c53d8d539f2b48f7b71d4e2f8041a43b0e1800', 'c0ac11ca3fd3a63bde616e76d70a44aa859ba3c0', '59d9318f07331ec15e54fe2a4218bc4a5c247a38', '0186ae43e8889d5bbc19e3f331d91eedec0ff586', '162d958ff885f1462aeda91cd72582323fd6a1f4', '6f8bbdee59216c5506d06558b0629972fcff290c']}
{'paperID': '509f652d67a793a614633d4a5c236592d80b6522', 'abstract': 'Deep neural networks (DNNs) have great expressive power, which can even memorize samples with wrong labels. It is vitally important to reiterate robustness and generalization in DNNs against label corruption. To this end, this paper studies the 0-1 loss, which has a monotonic relationship with an empirical adversary (reweighted) risk~\\citep{hu2016does}. Although the 0-1 loss has some robust properties, it is difficult to optimize. To efficiently optimize the 0-1 loss while keeping its robust properties, we propose a very simple and efficient loss, i.e. curriculum loss (CL). Our CL is a tighter upper bound of the 0-1 loss compared with conventional summation based surrogate losses. Moreover, CL can adaptively select samples for model training. As a result, our loss can be deemed as a novel perspective of curriculum sample selection strategy, which bridges a connection between curriculum learning and robust learning. Experimental results on benchmark datasets validate the robustness of the proposed loss.', 'bibtex': '@Article{Lyu2019CurriculumLR,\n author = {Yueming Lyu and I. Tsang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Curriculum Loss: Robust Learning and Generalization against Label Corruption},\n volume = {abs/1905.10045},\n year = {2019}\n}\n', 'references': ['6c97556edbc192896cc55395f8f21fe0ff148580', 'c307baff85987544b0b85821f4a5c02b889994e9', '568675640f381fbc13998015c2854298877b7aa0', '622727f595542afa3caf8802927d880818ddb17a', '77a1dcdb52b5b76ae17fb4f181840797e58f7fed', '1e1855ca80e8ac3de0e169871f320416902e9ad1', 'e061d23b68e7d4aac5aece4794c044c80e638dca', 'c4c4bc0367ec099f1e00a7700332cd0bf393aa55', '306a2e8ca31fdcc148618d37074785c290f96375', 'a4a8e91995ae8c8b203dd857bdc0915facddeebe', '492874f957e11a535c46c242ea0e595c0f068785', '5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf', '07b5093aace8e485e7d23b83edb6351618138127', 'bc550ee45f4194f86c52152c10d302965c3563ca', '54ddb00fa691728944fd8becea90a373d21597cf', '91d331d2bdd5fc86400c40c497bcb4c741c652be', '37acbbbcfe9d8eb89e5b01da28dac6d44c3903ee', '93d8d45fe8101545ae6d9fab3dbb38f904ff7b4e', 'dd05ce86cacf50e3aee70d633040241923deb120', '21d255246cd7ddba24a651fd716950f893ea8eb2', '44606e1209a47d1fcf88b90e306db9e4b84fa2c5', '9f59d0a003558066d2ff4fc1c77f461b4d233663', 'cf9daa503798c3d4e04131bc7bf01544666265d1', 'a049555721f17ed79a97fd492c8fc9a3f8f8aa17', '8de174ab5419b9d3127695405efd079808e956e8', 'f5fb14e99bab28ec06d9a4f77875a37ceb73c489', '5a43cc163f4e570b28617ad8a40872ad9189349c', '1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435', 'e82c817c75061797f2ec111610ae4ce8d8c20794']}
{'paperID': 'ff22e140a0423f1cf0595d213f36402668084014', 'abstract': 'Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training. In this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass. CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks. We conduct large scale experiments on MNIST and CIFAR datasets, and outperform all previous linear relaxation and bound propagation based certified defenses in $\\ell_\\infty$ robustness. Notably, we achieve 7.02% verified test error on MNIST at $\\epsilon=0.3$, and 66.94% on CIFAR-10 with $\\epsilon=8/255$. Code is available at this https URL (TensorFlow) and this https URL (PyTorch).', 'bibtex': '@Article{Zhang2019TowardsSA,\n author = {Huan Zhang and Hongge Chen and Chaowei Xiao and Bo Li and D. Boning and Cho-Jui Hsieh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Towards Stable and Efficient Training of Verifiably Robust Neural Networks},\n volume = {abs/1906.06316},\n year = {2019}\n}\n', 'references': ['86fcf7c9166b5151c7e84ece7ea137bf2bd6b740', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '5677b0ca32651b80aa3a210d691bfc93bf9fcc82', '5473175211aff4a8a099c44d1a57802d1b7ecf9e', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'ad640ea420d49a969a995632aed22c21251891df', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'e749e8c947550485eddf864f8efeb870b894e4ce', '7c6123cd7052c9c8bb2ed5e11818b2ba7f06e79c', '63b4347cd65c00efae69726996752a2b1c869fa5', '7ad8c18994108a630c4564400f6137bf4d8b7818', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', 'f7f1b5437b9c97d67fae59142b242ce079d7cb08', '1a83564d61aebde360c0be4834cf6eb4c472c1bd', '98cc371f4e3a39b5c69b4e8980a5990f9011f223', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', '750fd4f2a6139387b4f6245d3fd1013a8c8cf702', 'de49430578bb3f8de3e610423255662c45f17610', '74dbcc09a3456ddacf5cece640b84045ebdf6be1', 'f749b2576c062a4fd16bb668d76d1e1084ad704e', '5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1', '75339d34bdac0d21a41461228ec6088eecdf857a', 'f0c5991dbb130fa6b5de011cf7a04f6ed815ef68', '20f85256555ad612148e52f9363e52f9d661728b', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', '9db631435f7f79646a4e0a1841fbeb3340e44261', '8d35663a80199b173d8cbd12dbf2300a9f86a021', 'f7bb1636ced9036b3d0edafc7d82ad43164d41a3', '8b9127bee0f7d109da2672ba06d0f39a5a60335a', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', 'a18ada04d93981178234d9c8907fb99ea92fddcb', 'd5577abcc1fbf57d66017e3b5b2211a82022842c', 'd3c071dbbb4520ed5875f7e064a9da87240534db', '77685c77a1fa39890006fe13f43738aac49a2c51', '1cf361d02f5ad84567e48754f1a8f895653bc701', '4b23012689e0f17912fb38d4984775e567cff8d6', 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '6f61d15a31d6d051aeee3bf6d1482d332e68ebfe', '255d2c2af6d7abbbebfc03dab51cd8574ad3558e', '99cb08c76c120599abd1d1637e32aaf577f38d39', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'bb92676f9ec13783ac664c268191f20944718f95', 'e225dd59ef4954db21479cdcbee497624b2d6d0f']}
{'paperID': 'df26b25512f5e8ff8f2242f9b0aa40e7572f2f43', 'abstract': 'In recent years several adversarial attacks and defenses have been proposed. Often seemingly robust models turn out to be non-robust when more sophisticated attacks are used. One way out of this dilemma are provable robustness guarantees. While provably robust models for specific $l_p$-perturbation models have been developed, we show that they do not come with any guarantee against other $l_q$-perturbations. We propose a new regularization scheme, MMR-Universal, for ReLU networks which enforces robustness wrt $l_1$- and $l_\\infty$-perturbations and show how that leads to the first provably robust models wrt any $l_p$-norm for $p\\geq 1$.', 'bibtex': '@Article{Croce2019ProvableRA,\n author = {Francesco Croce and Matthias Hein},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {Provable robustness against all adversarial $l_p$-perturbations for $p\\geq 1$},\n year = {2019}\n}\n', 'references': ['91a05cb84f1c7dbb0354da2ff11ae92549152435', '25f99278a54417a2dcb64c549416e138373082d6', 'daf8cd0f2c159d022477914bfacee9ff6da70c8b', '49b64383fe36268410c430352637ed23b16820c5', '530859552c04fa4dae8f77882d7590e2df1a557e', '43a4a354b67ab6d5531355a368094815d2d2593d', '9eda74b1219572287e489f84134bb935f139c4e7', 'ecba2826cd7a51d4d8b9820591ff0fa6b41d66a6', '12b6a7eb4014338fba3edef956be32775d271c78', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', 'de49430578bb3f8de3e610423255662c45f17610', '75339d34bdac0d21a41461228ec6088eecdf857a', '20f85256555ad612148e52f9363e52f9d661728b', 'fd7789de401811fd8692466b8d49230e7184655f', '9db631435f7f79646a4e0a1841fbeb3340e44261', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', 'e3b17a245dce9a2189a8a4f7538631b69c93812e', '0314e777333a63aca5735ea136c74e113aa8801d', 'afa0d49c1399c752d6f4665d75ecec640c000468', '9de69a46e6c619255eeffbfbb6c7b7163690eb48', '4b23012689e0f17912fb38d4984775e567cff8d6', '6afc5e895d2fa5a816cb47311dfc1768c8bf86c9', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '255d2c2af6d7abbbebfc03dab51cd8574ad3558e', '99cb08c76c120599abd1d1637e32aaf577f38d39', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', '9375729d21a344a5ccccd5f53556ddf90b957cd9', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '73f56fd3fdcbff8131b8a70ebcc05264cecc3d99', 'a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb', '1534867c76db0315c7f50ebc14099940dd860a93', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '4f9f7434f06cbe31e54a0bb118975340b9e0a4c9', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'c56e758ba18066a8cdc333f15dfdb7ea6af4d283']}
{'paperID': '4136cbc5f7f1fa34b91bf7bd335b173afaaf68d6', 'abstract': 'Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide certified l2 radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize. In our experiments, we show that our method can be applied to modern deep neural networks on a wide range of datasets, including Cifar-10, ImageNet, MNIST, and SVHN. For all tasks, MACER spends less training time than state-of-the-art adversarial training algorithms, and the learned models achieve larger average certified radius.', 'bibtex': '@Article{Zhai2020MACERAA,\n author = {Runtian Zhai and Chen Dan and Di He and Huan Zhang and Boqing Gong and Pradeep Ravikumar and Cho-Jui Hsieh and Liwei Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius},\n volume = {abs/2001.02378},\n year = {2020}\n}\n', 'references': ['d6dcbf0bb628658df27abd297c03147115e476e4', '1822e737dfbc933c95ab44f1dd123756120dfaa4', 'ff22e140a0423f1cf0595d213f36402668084014', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '63c022ae3b385d1d49c119142bfabb5cdb5ec90b', '6d12401822a24b2ff5542a7fa72158d891960c62', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', 'd33deae7f654b07ac8a5c437a4fa018c29e6af17', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '93314b89c218c02cc1a32cad7071215693599907', '63b4347cd65c00efae69726996752a2b1c869fa5', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', '750fd4f2a6139387b4f6245d3fd1013a8c8cf702', 'b862efa06baea0b032214675eb3c3645d5d69d46', '75339d34bdac0d21a41461228ec6088eecdf857a', '20f85256555ad612148e52f9363e52f9d661728b', 'd21fde0f55ee0285c66334d37b8920c867959784', '9db631435f7f79646a4e0a1841fbeb3340e44261', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', '1cf361d02f5ad84567e48754f1a8f895653bc701', '9a089c56eec68df722b2a5a52727143aacdc2532', '4b23012689e0f17912fb38d4984775e567cff8d6', '9ab7319dbe80549ba80e3320d0546d741a7a5791', '6114cdf58aa606ceaa998ed883c01c692207d473', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'ebab687cd1be7d25392c11f89fce6a63bef7219d', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '1534867c76db0315c7f50ebc14099940dd860a93', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '6e71a24fc0bba4a712b89dd9ff87a452230c2c4b', 'bb92676f9ec13783ac664c268191f20944718f95', '59d9318f07331ec15e54fe2a4218bc4a5c247a38']}
{'paperID': '2a88cc8cc9562b4addec03ba16b35cb4d3baaa43', 'abstract': 'Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the “best of both worlds”, i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in our evaluations). We evaluate EMPIR across a suite of 3 different DNN tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. Our results indicate that EMPIR boosts the average adversarial accuracies by 43.6%, 15.3% and 11.9% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs.', 'bibtex': '@Article{Sen2020EMPIREO,\n author = {Sanchari Sen and Balaraman Ravindran and A. Raghunathan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness against Adversarial Attacks},\n volume = {abs/2004.10162},\n year = {2020}\n}\n', 'references': ['b4dc59552f815283f371e6aa024a4ee7dd2d101d', '676e40050453ddeb1387f8314478c0ac3681a8c6', '9a1093af92d315def21b90918faf08665157051a', '99ce59c65f1bdf1ae2592e5d4f939b42c084e193', '921cb84f4b7ea75a2632c7e7ceadb213b4d7d8fd', 'ac8e45a0451ac578f17f631fc2663ee4b98b83a9', '64520a1f652ebd04565354fbb8a6281606c9724d', 'eccd39db20a0caae8dbcf7f64d04280e62cabf65', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '6f61d15a31d6d051aeee3bf6d1482d332e68ebfe', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', 'd86c9623d56e469aec73a76758a829891b0b2a09', 'd2e4147eecae6f914e9e1e9aece8fdd2eaed809f', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '8b053389eb8c18c61b84d7e59a95cb7e13f205b7', '819167ace2f0caae7745d2f25a803979be5fbfae', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'a5733ff08daff727af834345b9cfff1d0aa109ec', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '9c0e19677f302f04a276469fddc849bd7616ab46', '88843f4372f7c0b8c820a164b64060983df12049', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'a0456c27cdd58f197032c1c8b4f304f09d4c9bc5', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'd1ee87290fa827f1217b8fa2bccb3485da1a300e', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '764eff31d9596033859895d9513b838d2c57a6fb', 'abstract': 'Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. However, there exists a simple, yet easily overlooked fact that adversarial examples are only defined on correctly classified (natural) examples, but inevitably, some (natural) examples will be misclassified during training. In this paper, we investigate the distinctive influence of misclassified and correctly classified examples on the final robustness of adversarial training. Specifically, we find that misclassified examples indeed have a significant impact on the final robustness. More surprisingly, we find that different maximization techniques on misclassified examples may have a negligible influence on the final robustness, while different minimization techniques are crucial. Motivated by the above discovery, we propose a new defense algorithm called {\\em Misclassification Aware adveRsarial Training} (MART), which explicitly differentiates the misclassified and correctly classified examples during the training. We also propose a semi-supervised extension of MART, which can leverage the unlabeled data to further improve the robustness. Experimental results show that MART and its variant could significantly improve the state-of-the-art adversarial robustness.', 'bibtex': '@Article{Wang2020ImprovingAR,\n author = {Yisen Wang and Difan Zou and Jinfeng Yi and J. Bailey and Xingjun Ma and Quanquan Gu},\n booktitle = {International Conference on Learning Representations},\n title = {Improving Adversarial Robustness Requires Revisiting Misclassified Examples},\n year = {2020}\n}\n', 'references': ['dcfb420412d76600eb124625f62fb28499af1e8c', '2fb43a4c5cab8215d510fc585ca81fb5ee8a3abb', 'b2125f338e736497bb01168b922a6ab63cc27d75', '1067c814c5d517cd50af176f3c919493fa799c0f', '2172552b917ef3757b0af47d17fce18586d56cba', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '63c022ae3b385d1d49c119142bfabb5cdb5ec90b', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', '3f7bc67330b3eff749459568e7995f0017dfe645', '6be44364db3a46ab5fcf8172910650b210cc5c39', 'ac644a74a0ebc8cfbe1b0af8120004909828d283', 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '7bc43b9f701823c04c7ee3ed780a34b041ee4b07', '93314b89c218c02cc1a32cad7071215693599907', '508dad538d5c63eb6c07fd10794510357a951a58', '99ce59c65f1bdf1ae2592e5d4f939b42c084e193', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', '1b9c6022598085dd892f360122c0fa4c630b3f18', '804fb9542f4f56e264dd2df57c255a9a2011c00f', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', 'f7bb1636ced9036b3d0edafc7d82ad43164d41a3', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '2f201c77e7ccdf1f37115e16accac3486a65c03d', '651adaa058f821a890f2c5d1053d69eb481a8352', '92be0550fa3801d7a4bd71ea205373c22f5848a9', 'a18ada04d93981178234d9c8907fb99ea92fddcb', 'ca9c1224636b0a7dd37340a4691c34a9914b5af8', 'ac8e45a0451ac578f17f631fc2663ee4b98b83a9', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '63a010c69f00e65c946a68b546bbd42cbed03564', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', '405b6ff2ea2ec9a7c7d6b18ac951dc778892ffcf', 'da1231a3a7536010ddb6ef5e163a785d03974af1', '0a77313fa10a864e14f538c73d417d7b4d6f320e', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '1c4e9156ca07705531e45960b7a919dc473abb51', 'a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb', '53b047e503f4c24602f376a774d653f7ed56c024', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'd6b93b766dff2066cebbc897c9ec7fbc44848ad7', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '4f9f7434f06cbe31e54a0bb118975340b9e0a4c9', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '54d2b5c64a67f65c5dd812b89e07973f97699552', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'e5693e6b6d7551a82b64cd4ca0e2071c3dfef4c9', 'e225dd59ef4954db21479cdcbee497624b2d6d0f', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '000829516229b87cafff052fe310604c85236671', 'abstract': "Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training examples.", 'bibtex': '@Article{Chan2019JacobianAR,\n author = {Alvin Chan and Yi Tay and Y. Ong and Jie Fu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Jacobian Adversarially Regularized Networks for Robustness},\n volume = {abs/1912.10185},\n year = {2019}\n}\n', 'references': ['cdfee4355cae3299b06f3f98718df9bb64a899bf', '2c1006c856fefdbd6cd710e840e57153f2d6cd04', 'd6dcbf0bb628658df27abd297c03147115e476e4', '91a05cb84f1c7dbb0354da2ff11ae92549152435', '81bcbae2f547c6915dc14e7b0c3ff9ea6cff7d4f', 'daf8cd0f2c159d022477914bfacee9ff6da70c8b', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', 'f6a201eed70e8b48e2f60d97c98cfc8fe3b7b175', 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5', '988a378f640eb7fb681f977d6cb1e0c830c07b4c', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '0c1a245a67a5136d85de77d3ba38a900ab0579ab', '41071dbbbcbb27af3fec70de045f19c28535f5b7', '7ad8c18994108a630c4564400f6137bf4d8b7818', '43a4a354b67ab6d5531355a368094815d2d2593d', '20f85256555ad612148e52f9363e52f9d661728b', '1b9c6022598085dd892f360122c0fa4c630b3f18', 'ba6bc847a6ece8448ff492d0639a93d736427903', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '5e3e36049ed4156f01b0133d56352af8f5020b4a', 'ac8e45a0451ac578f17f631fc2663ee4b98b83a9', '302207c149bdf7beb6e46e4d4afbd2fa9ac02c64', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '255d2c2af6d7abbbebfc03dab51cd8574ad3558e', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '78aa018ee7d52360e15d103390ea1cdb3a0beb41', '0e3cc46583217ec81e87045a4f9ae3478a008227', 'e8b8a7778ace2a02f8db6fe321a54520c6b283ca', 'a4cec122a08216fe8a3bc19b22e78fbaea096256', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'dc048a657951340b312504450a09a07b630152d7']}
{'paperID': '0d94293388417458eae73632e33840a772375900', 'abstract': 'Transfer learning, in which a network is trained on one task and re-purposed on another, is often used to produce neural network classifiers when data is scarce or full-scale training is too costly. When the goal is to produce a model that is not only accurate but also adversarially robust, data scarcity and computational limitations become even more cumbersome. We consider robust transfer learning, in which we transfer not only performance but also robustness from a source model to a target domain. We start by observing that robust networks contain robust feature extractors. By training classifiers on top of these feature extractors, we produce new models that inherit the robustness of their parent networks. We then consider the case of fine tuning a network by re-training end-to-end in the target domain. When using lifelong learning strategies, this process preserves the robustness of the source network while achieving high accuracy. By using such strategies, it is possible to produce accurate and robust models with little data, and without the cost of adversarial training. Additionally, we can improve the generalization of adversarially trained models, while maintaining their robustness.', 'bibtex': '@Article{Shafahi2019AdversariallyRT,\n author = {Ali Shafahi and Parsa Saadatpanah and Chen Zhu and Amin Ghiasi and Christoph Studer and D. Jacobs and T. Goldstein},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adversarially robust transfer learning},\n volume = {abs/1905.08232},\n year = {2019}\n}\n', 'references': ['c92be891c5f8f0f60b6de206364f9a744612d1e8', 'aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '821fd5bed14d6d06c25fbf44123fd7be382f7b4e', '1b9c6022598085dd892f360122c0fa4c630b3f18', '804fb9542f4f56e264dd2df57c255a9a2011c00f', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '651adaa058f821a890f2c5d1053d69eb481a8352', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a', '1c4e9156ca07705531e45960b7a919dc473abb51', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '0c908739fbff75f03469d13d4a1a07de3414ee19', '4d376d6978dad0374edfa6709c9556b42d3594d3', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '081651b38ff7533550a3adfc1c00da333a8fe86c', 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', 'a25fbcbbae1e8f79c4360d26aa11a3abf1a11972', '34f25a8704614163c4095b3ee2fc969b60de4698', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '5ecd39a7797978bf0fd5682f93eddb195ad4a1ea', 'abstract': 'This paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data. We identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO). We show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution and this behaviour of the VAE can lead to extreme errors in the learned representation. This is a key hurdle in the effective use of representations for data-efficient learning and transfer. To address this problem, we propose to augment the data with specifications that enforce insensitivity of the representation with respect to families of transformations. To incorporate these specifications, we propose a regularization method that is based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. For certain choices of parameters, our formulation naturally leads to the minimization of the entropy regularized Wasserstein distance between representations. We illustrate our approach on standard datasets and experimentally show that significant improvements in the downstream adversarial accuracy can be achieved by learning robust representations completely in an unsupervised manner, without a reference to a particular downstream task and without a costly supervised adversarial training procedure.', 'bibtex': '@Article{Cemgil2020AdversariallyRR,\n author = {A. Cemgil and Sumedh Ghaisas and Krishnamurthy Dvijotham and Pushmeet Kohli},\n booktitle = {International Conference on Learning Representations},\n title = {Adversarially Robust Representations with Smooth Encoders},\n year = {2020}\n}\n', 'references': ['77d76c5fb1d7467518744c7f14e4de3ac39e589e', '813995ec740921c88360fccee4a49befbb1e51c3', 'e8d2ad861e4d107ae2c0d1b7bb053d06022dfe1c', '287547fc81364e64d196abb8d891ade3f6599a5a', '8ea9093542075bd8cc4928a4c671a95f363c61ef', '89317846e89556ef183433fbded801088b110a86', '23b8ae10566785d4befacedf26631e80afa1af9f', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '4d7574c0c4aca70e5811a8e33906f0106d6b76e6', '0a2605ca2c38fe45ac87b1d196a322857d8cb912', '8e5d0c73eb29e3da8d6d3a0c8560b23680122bb2', '29831b8830e278c8c28e45c8e9c41c619c89f86a', 'a90226c41b79f8b06007609f39f82757073641e2', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'fcf43325529c8b1cc26aeb52fd5d7e532abb0a40', '1db6e3078597386ac4222ba6c3f4f61b61f53539', 'c8c04ed972d38e2326a53d322a6f2d7e0f8218c1', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', '484ad17c926292fbe0d5211540832a8c8a8e958b', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '267754c911d6e8adadae50c1e7f3ab5be4244e52', '72d32c986b47d6b880dad0c3f155fe23d2939038', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'c7870b78c57e0bd2e9fb6907c0702e28eb87e239', 'f4ba954b0412773d047dc41231c733de0c1f4926', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992']}
{'paperID': '00d4f1c5f11ecbbc1a033e6675934703f09016f8', 'abstract': 'Neural ordinary differential equations (ODEs) have been attracting increasing attention in various research domains recently. There have been some works studying optimization issues and approximation capabilities of neural ODEs, but their robustness is still yet unclear. In this work, we fill this important gap by exploring robustness properties of neural ODEs both empirically and theoretically. We first present an empirical study on the robustness of the neural ODE-based networks (ODENets) by exposing them to inputs with various types of perturbations and subsequently investigating the changes of the corresponding outputs. In contrast to conventional convolutional neural networks (CNNs), we find that the ODENets are more robust against both random Gaussian perturbations and adversarial attack examples. We then provide an insightful understanding of this phenomenon by exploiting a certain desirable property of the flow of a continuous-time ODE, namely that integral curves are non-intersecting. Our work suggests that, due to their intrinsic robustness, it is promising to use neural ODEs as a basic block for building robust deep network models. To further enhance the robustness of vanilla neural ODEs, we propose the time-invariant steady neural ODE (TisODE), which regularizes the flow on perturbed data via the time-invariant property and the imposition of a steady-state constraint. We show that the TisODE method outperforms vanilla neural ODEs and also can work in conjunction with other state-of-the-art architectural methods to build more robust deep networks.', 'bibtex': '@Article{Yan2019OnRO,\n author = {Hanshu Yan and Jiawei Du and V. Tan and Jiashi Feng},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On Robustness of Neural Ordinary Differential Equations},\n volume = {abs/1910.05513},\n year = {2019}\n}\n', 'references': ['79a13802beabf29155cc8b554a140fa215d77722', '5f073839a55f387af71d789f1f7924e08bcd7df5', '98fc7a351bbb07fb3a304508e1a5ffcab03babba', '31156009c49a88b5f0fb37437512eec570310d24', '41071dbbbcbb27af3fec70de045f19c28535f5b7', 'b51cdc040f98831b47e0848ee1f382e2e4d3ff57', '8afa6dd9f9ac46462a1fb70a757c4ae1cd45bbf6', 'ef0a6e167c95927dac6d61d96e2d08da413868ac', '25e433197844c239742f67fbb4171e913e0b9fe2', '449310e3538b08b43227d660227dfd2875c3c3c1', '1b9c6022598085dd892f360122c0fa4c630b3f18', 'ce1ff1a833f38e411c0ecdbf84426cfea8842646', 'ee95231783167baa4785a642e8ef563a572c5d63', '9e09ef5f2e0519c7a7df01e6e1a7d9306441e179', 'be4a4f7f65d397a4e07dc83b95da6b414e0634e2', '9a089c56eec68df722b2a5a52727143aacdc2532', 'b36a5bb1707bb9c70025294b3a310138aae8327a', '1cb7ea5de0f8f4d90e4369db4a161ca8594026dc', 'f657e68dde470641ba1a6cd7ab755e6359a32840', '9ab7319dbe80549ba80e3320d0546d741a7a5791', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', 'e3f8c8253767b6fe0891025cdf772cd286337921', 'aad34665649953fa4bbacdc6eff4edb5408df6b3', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'd0156126edbfc524c8d108bdc0cf811cfe3129aa', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '163047737ca73fdb3e5147eaee64329abe04ed1d', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '4f2703652205bb4e146aee4dd48c78ed68b83ff3', '7b3a7733077e8f632d42bd0718943671b006e461', '02227c94dd41fe0b439e050d377b0beb5d427cda', 'cebeb3afc6d68217bc9f3f143a3aedf3622980b6', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '7c83832f00579685ce454d0f2d61758db2b635d6', 'abstract': 'Adversarial training has been demonstrated as one of the most effective methods for training robust models to defend against adversarial examples. However, adversarially trained models often lack adversarially robust generalization on unseen testing data. Recent works show that adversarially trained models are more biased towards global structure features. Instead, in this work, we would like to investigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize well for unseen shape variation. To learn the robust local features, we develop a Random Block Shuffle (RBS) transformation to break up the global structure features on normal adversarial examples. We continue to propose a new approach called Robust Local Features for Adversarial Training (RLFAT), which first learns the robust local features by adversarial training on the RBS-transformed adversarial examples, and then transfers the robust local features into the training of normal adversarial examples. To demonstrate the generality of our argument, we implement RLFAT in currently state-of-the-art adversarial training frameworks. Extensive experiments on STL-10, CIFAR-10 and CIFAR-100 show that RLFAT significantly improves both the adversarially robust generalization and the standard generalization of adversarial training. Additionally, we demonstrate that our models capture more local features of the object on the images, aligning better with human perception.', 'bibtex': '@Article{Song2019RobustLF,\n author = {Chuanbiao Song and Kun He and Jiadong Lin and Liwei Wang and J. Hopcroft},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Local Features for Improving the Generalization of Adversarial Training},\n volume = {abs/1909.10147},\n year = {2019}\n}\n', 'references': ['76199eeda71c903cc4b53691655e35f71909d8b2', '63c022ae3b385d1d49c119142bfabb5cdb5ec90b', '5fbb606b313e4b8e57496db027025fb549543401', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '717425092f2e7a2a00c98e8e1778036e9b5a8b4e', '04ee17ea05341aadc8643a21d21746cb67993a9d', '43e78439c6e37918133da064f2eed872b3b99ac5', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'e749e8c947550485eddf864f8efeb870b894e4ce', '508dad538d5c63eb6c07fd10794510357a951a58', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', '428c2e5992d6ed3186c087cba0fdba2ab6a468b2', '804fb9542f4f56e264dd2df57c255a9a2011c00f', '2f201c77e7ccdf1f37115e16accac3486a65c03d', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '8b9127bee0f7d109da2672ba06d0f39a5a60335a', '651adaa058f821a890f2c5d1053d69eb481a8352', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'f538dca4def5167a32fbc12107b69a05f0c9d832', '99cb08c76c120599abd1d1637e32aaf577f38d39', '99e5a8c10cf92749d4a7c2949691c3a6046e499a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '1c4e9156ca07705531e45960b7a919dc473abb51', '53b047e503f4c24602f376a774d653f7ed56c024', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'e33cbb25a8c7390aec6a398e36381f4f7770c283', 'be9a17321537d9289875fe475b71f4821457b435', '4282a344671189e17c9c9e00e329fe2d0fa71769', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '036251ce21ef0750898bc4ddab42cb9f5d7bdded', 'abstract': 'We analyze the effect of quantizing weights and activations of neural networks on their loss and derive a simple regularization scheme that improves robustness against post-training quantization. By training quantization-ready networks, our approach enables storing a single set of weights that can be quantized on-demand to different bit-widths as energy and memory requirements of the application change. Unlike quantization-aware training using the straight-through estimator that only targets a specific bit-width and requires access to training data and pipeline, our regularization-based method paves the way for "on the fly\'\' post-training quantization to various bit-widths. We show that by modeling quantization as a $\\ell_\\infty$-bounded perturbation, the first-order term in the loss expansion can be regularized using the $\\ell_1$-norm of gradients. We experimentally validate the effectiveness of our regularization scheme on different architectures on CIFAR-10 and ImageNet datasets.', 'bibtex': '@Article{Alizadeh2020GradientR,\n author = {Milad Alizadeh and A. Behboodi and Mart van Baalen and Christos Louizos and Tijmen Blankevoort and M. Welling},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {Gradient $\\ell_1$ Regularization for Quantization Robustness},\n year = {2020}\n}\n', 'references': ['cdfee4355cae3299b06f3f98718df9bb64a899bf', 'd77123b54dcc8014949584ab624e97298617bcad', 'ed1f55fb7ba0e3196913027840c4e23155e2e80c', '47a1edfb88f5b4a7ba1e9f6aed327f67f942f6d6', '63521e29aacc8c07bcb8476389f9b0cc247802bd', '2735dd87f42f60dd8f50def5ae51bbbf95318235', '2aa2fe58f441fbecc183965bf79f4ef7fb2dd4a7', 'f789425a7af1d012675118d7d10cd50afad09074', '16e6514ebf46ffdb5d11ba347fe366673fc0549c', '3d8b62c060f8444907e7c975c6ae590373b51ed4', 'fb2edf25484c9e9e5f94b719c55dc1faf7591bfa', '665eff85f159c290e059056ac099a0a6e1f54725', 'f93ae1a0b9e40b138da8c25855b6c68af4ee201a', 'b7ebb7aa28c275c329500e5597c2c399ac7e037a', '59d0d7ccec2db66cad20cac5721ce54a8a058294', '15698db8c6b08891c8ab8b75a2738cad04c7b25b', '013efe3ff541e518c51f08d1b62a62e0c57c0b14', 'edf73ab12595c6709f646f542a0d2b33eb20a3f4', '57713f4b3b9fbe6ceddfa8c26b4e1b99ad50a9b7', 'b649a98ce77ece8cd7638bb74ab77d22d9be77e7', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'a5733ff08daff727af834345b9cfff1d0aa109ec', '643da4c4de1954daeac571a82367241db012a8bf', 'b7cf49e30355633af2db19f35189410c8515e91f', 'eb42cf88027de515750f230b23b1a057dc782108', '62c76ca0b2790c34e85ba1cce09d47be317c7235', '52a978e6de72fd3ee25b36170cd08cfe8005b7ba', '61eabd7fa6c4d8fb14aa5ca5f8086238ef9f1e05', '30ffd4a8e479d04b1dea5749eac4a466dccde64b']}
{'paperID': '832baf7476fa4c98ad326ae9190d613d78e6eb13', 'abstract': 'Deep convolutional networks often append additive constant ("bias") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of "batch normalization"). Recent state-of-the-art blind denoising methods (e.g., DnCNN) seem to require these terms for their success. Here, however, we show that these networks systematically overfit the noise levels for which they are trained: when deployed at noise levels outside the training range, performance degrades dramatically. In contrast, a bias-free architecture -- obtained by removing the constant terms in every layer of the network, including those used for batch normalization-- generalizes robustly across noise levels, while preserving state-of-the-art performance within the training range. Locally, the bias-free network acts linearly on the noisy image, enabling direct analysis of network behavior via standard linear-algebraic tools. These analyses provide interpretations of network functionality in terms of nonlinear adaptive filtering, and projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology.', 'bibtex': '@Article{Mohan2019RobustAI,\n author = {S. Mohan and Zahra Kadkhodaie and Eero P. Simoncelli and C. Fernandez-Granda},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust and interpretable blind image denoising via bias-free convolutional neural networks},\n volume = {abs/1906.05478},\n year = {2019}\n}\n', 'references': ['9a058fd787914ffbf90f607a5c62b271a8a87ea7', 'f2ef09c15eeb7d197c8ec23fb85ce3bf4be6bd63', 'e26e0c78e346c167e635e8f3b8b419e283542a70', '635fe55f70feffca01fec7e07e4968b98dba672f', '5694e46284460a648fe29117cbc55f6c9be3fa3c', '0c00a328fa7cd56ee60338c54e89bd48310db80b', 'dc00e2c47411d6d257c979963e4dd2f7d97d5d03', 'eb04068416ade86de63cf9d9939e14d0bc9b96f9', 'a4cec122a08216fe8a3bc19b22e78fbaea096256', '6364fdaa0a0eccd823a779fcdd489173f938e91a', '4d376d6978dad0374edfa6709c9556b42d3594d3', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '4513104e69cf6b01f3e70bb157ae107ed81624e0', 'dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71', '33efd0cd7db2c9f44b2c2e150dafff14a96cd877', 'e07416eabd4ba6c69fa473756bb04ae7161177be', '742b3dce4a954001bb31bc7fa739c93fcbccd289', 'eae2e0fa72e898c289365c0af16daf57a7a6cf40', '9a1ed876196ec9733acb1daa6d65e35ff0414291', 'bfeaf424a2ea6ca4702d545c6e959e2caeb68e9b', '85a1725bfd3b4a2d3fe9a7272d66ebf03c016fed', '8cc9918add61ee5ef3b848aba9646169cc5e364e', 'cb2ffb67def387b584a4bca8a6d40e1c7be44f72', '5620050230ba1b45689cfc5e3efa064219a66a2e', '4e5c5fe6039aba0a86df0e8fc64c81563422a245', '04f7769cf0908101af532b02428928baccb2c4d3', '85791491919e1f740f0e882366046acbe56fb14c']}
{'paperID': 'e2d09159e54a53bb3b02e9265c0e63631d8d7d8a', 'abstract': 'Formal verification techniques that compute provable guarantees on properties of machine learning models, like robustness to norm-bounded adversarial perturbations, have yielded impressive results. Although most techniques developed so far requires knowledge of the architecture of the machine learning model and remains hard to scale to complex prediction pipelines, the method of randomized smoothing has been shown to overcome many of these obstacles. By requiring only black-box access to the underlying model, randomized smoothing scales to large architectures and is agnostic to the internals of the network. However, past work on randomized smoothing has focused on restricted classes of smoothing measures or perturbations (like Gaussian or discrete) and has only been able to prove robustness with respect to simple norm bounds. In this paper we introduce a general framework for proving robustness properties of smoothed machine learning models in the black-box setting. Specifically, we extend randomized smoothing procedures to handle arbitrary smoothing measures and prove robustness of the smoothed classifier by using $f$-divergences. Our methodology achieves state-of-the-art}certified robustness on MNIST, CIFAR-10 and ImageNet and also audio classification task, Librispeech, with respect to several classes of adversarial perturbations.', 'bibtex': '@Article{Dvijotham2020AFF,\n author = {Krishnamurthy Dvijotham and Jamie Hayes and B. Balle and Zico Kolter and Chongli Qin and A. György and Kai Y. Xiao and Sven Gowal and Pushmeet Kohli},\n booktitle = {International Conference on Learning Representations},\n title = {A Framework for robustness Certification of Smoothed Classifiers using F-Divergences},\n year = {2020}\n}\n', 'references': ['07398e448180ad75c44d30f23a65289d40ff6f52', '4690190d6c110f7525f7250e1acf4a4eab42519f', '89750a16c0645ab8a8f7ba6f5848b7f98a4ea8eb', '361c7858fa8f55928dc6358bc25d18fe3316d735', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'a1fb7236d104ae0343c1a09e3590ee2283483240', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', '20f85256555ad612148e52f9363e52f9d661728b', '7d45ac8d29160103cd0bba76aa99b0f60f23a1cd', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'f8c61521ac186443aae4082616821a55780d32a9', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', 'e177365c0dd642341469ba2f045405646c5dbb79', '34038d9424ce602d7ac917a4e582d977725d4393', 'a7b7e17fbbc22664451ad0e01e8e012d934ac2c0', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '70e4a4d752a881fd0e8382489d5d22f449537567', '55a4b14badcdcb4ae75e3729241fedbade728056', '8d2820ac17ff3cedf59f173b16b98872848bf3ad', '91ab3ca9fe3add7ce48dc2f97bb9435db1a991b9', 'e008ec108171c4eb88c65edf589ebee31e4359c8', '77ce3697fc01e0bebba1dfe81cedb712b0b604a0']}
{'paperID': 'bc4bcc5c62092349ad0ef82af958b16e0c3ec856', 'abstract': 'While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, some issues remain to be tackled to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efficient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which instead of learning a completely shared set of weights, represents the parameters for each task as a sum of task-shared and sparse task-adaptive parameters. With our Additive Parameter Decomposition (APD), the task-adaptive parameters for earlier tasks remain mostly unaffected, where we update them only to reflect the changes made to the task-shared parameters. This decomposition of parameters effectively prevents catastrophic forgetting and order-sensitivity, while being computation- and memory-efficient. Further, we can achieve even better scalability with APD using hierarchical knowledge consolidation, which clusters the task-adaptive parameters to obtain hierarchically shared parameters. We validate our network with APD, APD-Net, on multiple benchmark datasets against state-of-the-art continual learning methods, which it largely outperforms in accuracy, scalability, and order-robustness.', 'bibtex': '@Article{Yoon2019ScalableAO,\n author = {Jaehong Yoon and Saehoon Kim and Eunho Yang and Sung Ju Hwang},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {Scalable and Order-robust Continual Learning with Additive Parameter Decomposition},\n year = {2019}\n}\n', 'references': ['42a7f3e90aa4929d7f01b6124b87f82f7735aa88', '4a954b3e72a61968ab235076bcc242aca3a05520', '2b877889ac31b73d1ede70b00eb4c7118ef8eca2', 'ae7619604821adce52c28daa2aed14f5a191d975', '73d5c3cff5a777169dc62d0b351ddcfda503cd33', 'aab368284210c1bb917ec2d31b84588e3d2d7eb4', 'd5bb3faa48b83469da1a01ef267886e71f4a931a', '3aa673abd49f0837ed2fbf5cffcada9ba6f48693', 'd475f695dedd94e96771fdaa1e5c075fd01d11cf', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', 'ac7c04a668bdb0e3eff168e65cb85689b4f7ef57', 'ed04dc53d0091052feb3e7d6b7678fd7b218d416', '118fae4b4d07453561f1eded88654f812c7c61ec', '59a922212153d3407e658109f36c11a34ee7d283', '0a9a3380090d994e867026388f64a85481cdb700', '2e55ba6c97ce5eb55abd959909403fe8da7e9fe9', '8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a', '53c9443e4e667170acc60ca1b31a0ec7151fe753', '815c84ab906e43f3e6322f2ca3fd5e1360c64285', '0d3bb75852098b25d90f31d2f48fd0cb4944702b', 'eb42cf88027de515750f230b23b1a057dc782108', '8d49d34fff05285cb9a148261caff57775eb4453', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'd3c04a424fff21d3d12ff8b0543734cf244d5f67', '005b668ef278941f584df96f2aca1ca88f056470', '22fe619996b59c09cb73be40103a123d2e328111', '18e5ae17ea0c934dcad242b860e5f7a8ad6dd049', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '947a4b2e31dcaeffa8d86bc8d6888665ec33c5f6', 'abstract': 'It is well-known that classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$k$ predictions are more relevant. In this work, we aim to derive certified robustness for top-$k$ predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. We derive a tight robustness in $\\ell_2$ norm for top-$k$ predictions when using randomized smoothing with Gaussian noise. We find that generalizing the certified robustness from top-1 to top-$k$ predictions faces significant technical challenges. We also empirically evaluate our method on CIFAR10 and ImageNet. For example, our method can obtain an ImageNet classifier with a certified top-5 accuracy of 62.8\\% when the $\\ell_2$-norms of the adversarial perturbations are less than 0.5 (=127/255). Our code is publicly available at: \\url{https://github.com/jjy1994/Certify_Topk}.', 'bibtex': '@Article{Jia2019CertifiedRF,\n author = {Jinyuan Jia and Xiaoyu Cao and Binghui Wang and N. Gong},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing},\n volume = {abs/1912.09899},\n year = {2019}\n}\n', 'references': ['5812dae376cc07b955244a8e1ce11c3e4b9775ac', '50c5763d2d35f2c4eaa5cebea310faf2cf0a10dc', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '4a416cb1cac9fb4fecad49af1ff07e2523c99cfe', '63b4347cd65c00efae69726996752a2b1c869fa5', '7ad8c18994108a630c4564400f6137bf4d8b7818', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', 'ecba2826cd7a51d4d8b9820591ff0fa6b41d66a6', 'f0ded4902d7f9c111e50047f8c9494effb7282d1', 'c1f76891bdfa07d9a61ad11a15de13b139b20d2a', '98cc371f4e3a39b5c69b4e8980a5990f9011f223', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', '750fd4f2a6139387b4f6245d3fd1013a8c8cf702', '75339d34bdac0d21a41461228ec6088eecdf857a', '33f68f93914704ba49b48b508e5bb93369f4e79f', 'bfb0d179916c000d54f27e7a9ea18b6269963e74', '20f85256555ad612148e52f9363e52f9d661728b', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', 'fd7789de401811fd8692466b8d49230e7184655f', 'd21fde0f55ee0285c66334d37b8920c867959784', '291ac97d951026dfaa080bcbd46bdb9bde94ad4c', '9db631435f7f79646a4e0a1841fbeb3340e44261', '797e841a06e2f57163b86c24942b1e043fd3ca3e', '06b98537324dbf11c7de2040e519b4d110f5d622', '8d35663a80199b173d8cbd12dbf2300a9f86a021', '8b9127bee0f7d109da2672ba06d0f39a5a60335a', 'f7bb1636ced9036b3d0edafc7d82ad43164d41a3', '2f201c77e7ccdf1f37115e16accac3486a65c03d', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '0bd8c29a206c46dccca63c010a95734018c98d2e', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', 'a18ada04d93981178234d9c8907fb99ea92fddcb', '1cf361d02f5ad84567e48754f1a8f895653bc701', '9de69a46e6c619255eeffbfbb6c7b7163690eb48', '9a089c56eec68df722b2a5a52727143aacdc2532', '4b23012689e0f17912fb38d4984775e567cff8d6', '91f4ebdfb4618e9a7bbcefc8b64e2f7d6e176545', 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd', '59ea59d73eea51f80b60ba6ea47dac0197029336', 'c02596cee34919daeaab1beddd813f23d429973a', '69092affc3461a38eb05cf7982f104eb30b0492c', '45a710be199c8eb43f465c88fc4b343267c35d38', 'ee8bc379985788544e44cf63887cf75a03e08b64', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '63a010c69f00e65c946a68b546bbd42cbed03564', '99cb08c76c120599abd1d1637e32aaf577f38d39', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', '333416708c80d0c163ca275d1b190b1f2576fa5f', '013efe3ff541e518c51f08d1b62a62e0c57c0b14', '9f92a0ccc8b039a83bd5ba5482facb5829c712aa', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', '061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'd31655b3f82038c513eed0e4a84c8f1c89d5bfdb', 'c770e4a0564e46e23f4684a76b885b995add053a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'd4180fc8e2a9d9f69f815e33b9f937197f649f96', 'f10e974d32a8b20289bb6de74d96e90402b9ce2d', 'd7b7e7b7771f8d0f666798a214b7437b63b6e3dd', 'bb92676f9ec13783ac664c268191f20944718f95', 'e225dd59ef4954db21479cdcbee497624b2d6d0f', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'a05f5a5c9fe1d8a44f5960571cc6f4fbb75d0d36']}
{'paperID': '70a6f1820eec8152f4af826d9adf61f442a24743', 'abstract': 'Deep networks were recently suggested to face the odds between accuracy (on clean natural images) and robustness (on adversarially perturbed images) (Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently higher sample complexity (Schmidt et al., 2018) and/or model capacity (Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view of that, give a classification task, growing the model capacity appears to help draw a win-win between accuracy and robustness, yet at the expense of model size and latency, therefore posing challenges for resource-constrained applications. Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? This paper studies multi-exit networks associated with input-adaptive efficient inference, showing their strong promise in achieving a “sweet point" in co-optimizing model accuracy, robustness, and efficiency. Our proposed solution, dubbed Robust Dynamic Inference Networks (RDI-Nets), allows for each input (either clean or adversarial) to adaptively choose one of the multiple output layers (early branches or the final one) to output its prediction. That multi-loss adaptivity adds new variations and flexibility to adversarial attacks and defenses, on which we present a systematical investigation. We show experimentally that by equipping existing backbones with such robust adaptive inference, the resulting RDI-Nets can achieve better accuracy and robustness, yet with over 30% computational savings, compared to the defended original models.', 'bibtex': '@Article{Hu2020TripleWB,\n author = {Ting-Kuei Hu and Tianlong Chen and Haotao Wang and Zhangyang Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference},\n volume = {abs/2002.10025},\n year = {2020}\n}\n', 'references': ['53e36eb4ead1999146601db500d85faed219e8a9', 'ed1f55fb7ba0e3196913027840c4e23155e2e80c', '1eff01027877843f1b492c4abecdbbc112497d29', 'ebde1d64f5a77f36258ac6c23f6285050968e3f9', '0b3de4589f96926621822dc07a9e3c25a5c338df', '170dd6ea32861684e9fa4cc45816e5f6c2f44ea2', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '4ae3fbba225eb647733e733c66836b0fc1abe4a0', '7bc43b9f701823c04c7ee3ed780a34b041ee4b07', '2aa2fe58f441fbecc183965bf79f4ef7fb2dd4a7', '6aa8ca81c755843bb814f01b4f2a25fc8f3b781d', 'a3143eaa68040d366848a9c324b29d3f56f97a5d', 'e8b378731c5df7b09efdc6d14dc03a26e9f56a4b', '449310e3538b08b43227d660227dfd2875c3c3c1', '1b9c6022598085dd892f360122c0fa4c630b3f18', 'f37a8472b00f4a00a91abb41e5ab764d5a5076a8', '804fb9542f4f56e264dd2df57c255a9a2011c00f', 'dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4', 'ca9c1224636b0a7dd37340a4691c34a9914b5af8', 'f37ea0b173dd0403a5028c12746082d31dff60bb', '9a089c56eec68df722b2a5a52727143aacdc2532', 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', 'eccd39db20a0caae8dbcf7f64d04280e62cabf65', '80b35b033bbb7585f97a458fd245e873fe5c75ed', '9806871bdcf0a9f926f6b4aebd20ee4580d69f00', '2adb616a77fe28b49be2a2d66cccf2d7400e4a04', '9da734397acd7ff7c557960c62fb1b400b27bd89', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', '125ccd810f43f1cba83c6681836d000f83d1886d', 'abf38db4775bec89c950013030d8eda56a89d32a', '7944d0b061610b1c67ad15efdf192681e60d0129', '896de8418884f4aab1ae4a60027500c9e8baffc3', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '7601b995303f953955004db7b9b8b206c0e02ff8', 'd3cb9bad655197b52932978dd8186b36c512bf92', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '819167ace2f0caae7745d2f25a803979be5fbfae', 'd5b4721c8188269b120d3d06149a04435753e755', '1ff9a37d766e3a4f39757f5e1b235a42dacf18ff', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8']}
{'paperID': 'f4ebde1ba8944f136a654b0bcd468afec23556f7', 'abstract': 'Recently $\\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \\cite{zhai2019a} has shown to be surprisingly efficient and effective. This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal}, {\\em independent}, or {\\em sparse} components of high-dimensional data. Our studies reveal additional good properties of the $\\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but also robust to outliers, and resilient to sparse corruptions. We provide preliminary statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.', 'bibtex': '@Article{Zhai2020UnderstandingLD,\n author = {Yuexiang Zhai and Hermish Mehta and Zhengyuan Zhou and Yi Ma},\n booktitle = {International Conference on Learning Representations},\n title = {Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness},\n year = {2020}\n}\n', 'references': ['23e4f9ee3c7f6d201081cc895ddb5f33476b9058', 'bbf8d32cfd460ddb89cba5d86d8dfb15e9979e17', '15411d45ac13f432c3505c3ce825dfa00d88f8a6', 'a31df1ada97f656fc16516c75132b0cfe2b3ebc0', '68923afec7f5c04e2f399f3d31094bc27a82392b', '01218257f5871297755041551f25d77fed6b4c95', '0cfcb1908fee5395e159b9d5ab1f270009444cf9', '67123f08ba8ffa75126cb68fd6860e5d34494815', '078dfd1ae7a550b3f1d37de0887a4f1ce36aaa3f', '0cef691880a3a2b15ea3d0740882921527d05c77', '1ef68840e963e6168a9d22785bc14cede6b54b2a', 'a10a39d2811a804fcead1667c54c4465c3314484', '9868cfbc9860eb79a854be2550e1e9774ed53dc2', '4e8d5da88bc2bb85bdc483b4cba316baed2b5999', 'f99d0bd1f1288fe35d46f719125844bfbaf544e3', '01625cba9f8a783994377d4f35aa765242faab4f', '59882b92d0183163e897a671b8c9298f89df5df3', 'c8831d7d318b8d59f9b958d250a58f253f08bd8a', '3596cc61267bdae97bb1df9b01a8e9a1b8dca450', 'd5eec41043d91964879c4c745c7165f823967f29', 'a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b', '9e7b0395d7b34e9d34cca779afd0c10da6e135b5', 'c346bf16bcf34d432df0f30942b94cc6fdd4c3e2', '0a072cbdee54b83c8df43a431065f009d2cd2e70', '92281d5002178003bd7060fc66677a3471cdaa4b', '41fef1a197fab9684a4608b725d3ae72e1ab4b39', '0c9bb579d8ad6ac987f7a16b66ddace671fc57c5', 'e07416eabd4ba6c69fa473756bb04ae7161177be', 'a11d6a90126d97d61d85098c8731c6f6d781e5ca', '4387ec06ce5df65eb7c8221652cb9fd317c40bda', '577d19a115f9ef6f002483fcf88adbb3b5479556', '2805537bec87a6177037b18f9a3a9d3f1038867b', '0899a6b62251ebb4af1ed35f0c6f9d63bed8c8e9', '6f7ef8ed60553b37bdf335876ac4e0adf400d1a8', '8012c4a1e2ca663f1a04e80cbb19631a00cbab27', 'b850d9d1529898ec4013deb1c976d081599c3a95', '70237128f7b4e4feda451457a9dc979446678776', '6aa8b611d67af1d32bcb3b7af8ca82510f6b1949', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '193092aef465bec868d1089ccfcac0279b914bda', 'abstract': 'Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.', 'bibtex': '@Article{Sagawa2019DistributionallyRN,\n author = {Shiori Sagawa and Pang Wei Koh and Tatsunori B. Hashimoto and Percy Liang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization},\n volume = {abs/1911.08731},\n year = {2019}\n}\n', 'references': ['4ce2f55585f3156e332721b8ab4f449389dc2a3c', '77568c594470f9aa029f92774e2c12ab0451d9bb', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'bd63a18ae679659b61a297e5ea8c5fee7f8b11b5', 'bcfba69c2fadf2efea83be12fda2601f8d4681af', '42ed4a9994e6121a9f325f5b901c5b3d7ce104f5', '54036f43acc6c9b49b334270c7237217685f52fb', 'b661520bf0061b7d96ccf12016e351dd3a6ee780', '00192b5adb3dea34a60ca7ebe58b48fc44e80efd', 'a1fb7236d104ae0343c1a09e3590ee2283483240', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', 'f986968735459e789890f24b6b277b0920a9725d', '2997b26ffb8c291ce478bd8a6e47979d5a55c466', '18858cc936947fc96b5c06bbe3c6c2faa5614540', 'f8d8e52ec60a7da7609c79cb3a19635385e180aa', 'afdc57412a7dadf2a392a16276f05df757d5b98b', '04fd269c96f11235fbbb985bb16dacedaa3098fd', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '9757ef31095227cb289af22b0a4010eda754d100', 'd8a6999ab3b0395477e34e7e2fdb10e122e1cc56', '8501e330d78391f4e690886a8eb8fac867704ea6', '5ded2b8c64491b4a67f6d39ce473d4b9347a672e', '4b1c6f6521da545892f3f5dc39461584d4a27ec0', '901335712430a194b6e15d817685e5ecc72a15c1', '07b5093aace8e485e7d23b83edb6351618138127', '54ddb00fa691728944fd8becea90a373d21597cf', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', 'd42b11ce90c9c69a20ed015b73dc33e0e4100a7b', 'ed6297433cfc580837e87592f550cc96296c7d0a', '7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c', 'f701b58e41d928cdcd8d733b638fd65a73623b72', 'e96506ee4baab43fa81cf1870cf7befb4a71fec7', '1f0bafe95728885034f5371420db2790e990971d', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '23ffaa0fe06eae05817f527a47ac3291077f9e58', '51dcc0c6c8ea27f0a5a3071fb8c4b32004cd55d8', '0f7c85357c366b314b5b55c400869a62fd23372c', '948dfbb55a93aa9585056a4b4dd3cd6553b236a9', '4d376d6978dad0374edfa6709c9556b42d3594d3', 'a2bf2e83df0c8b3257a8a809cb96c3ea58ec04b3', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1', '93f028b791bb7d0d4f0bccae89e7d7873a809572', '104f4de69582d2d2011d5c5135cf80b2233744f2', '5d83a8514d419c3a2a62bc9754225968934288a0', '50d7ecd6901d6759a6bd9da7f2fc8f346e073577', 'c069629a51f6c1c301eb20ed77bc6b586c24ce32', '583b55367f787eb0c4e295707b642e63547b9806', 'adaa0523a5c9d5f92aa2009a51226391d8e62380', '52125dd0e436a5fe3ced11587aa4f37a01c51a87', '6e71a24fc0bba4a712b89dd9ff87a452230c2c4b', 'e1932c4db44cc2ceab12a347c6067b139d040abb', '96167ed3ebc9a2c3270f6ae96043e6f086eed4de', '96c6bc559b79d8fd518f431c707e8b44ce3bc4de', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', '235723a15c86c369c99a42e7b666dfe156ad2cba', '9642a175637a400b425f0ac0cb6a2b067cc8fe6b', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '68d12857f76645a86417139eb6078db1ba76a7bf', '9405cc0d6169988371b2755e573cc28650d14dfe', 'a295f76c2afb7f79a970ccf086f16168d976bb93', 'e4350e816a350662ddb5f9ef92437aa8f3fd44f6', '4c38051f439b5e7316dbd7fc42fb40249ce6835d', '34f25a8704614163c4095b3ee2fc969b60de4698']}
{'paperID': '2bc747f61085bcd4c0cfa868ac6cfc92b0c1ce37', 'abstract': None, 'bibtex': '@Article{Lu2021ARMOUREDAR,\n author = {Kangkang Lu and C. Nguyen and Xun Xu and K. Chari and Y. Goh and Chuan-Sheng Foo},\n booktitle = {International Conference on Learning Representations},\n title = {ARMOURED: Adversarially Robust MOdels using Unlabeled data by REgularizing Diversity},\n year = {2021}\n}\n', 'references': []}
{'paperID': '48b7c4785693ba582f65b266756ba1946cef4bcb', 'abstract': None, 'bibtex': '@Article{Müller2021CertifyOP,\n author = {Mark Niklas Müller and Mislav Balunovic and Martin T. Vechev},\n booktitle = {International Conference on Learning Representations},\n title = {Certify or Predict: Boosting Certified Robustness with Compositional Architectures},\n year = {2021}\n}\n', 'references': []}
{'paperID': '7427493f3233580b28c949baace2461a92f37706', 'abstract': 'We study the problem of learning Bayesian networks where an ǫ-fraction of the samples are adversarially corrupted. We focus on the fully-observable case where the underlying graph structure is known. In this work, we present the first nearly-linear time algorithm for this problem with a dimension-independent error guarantee. Previous robust algorithms with comparable error guarantees are slower by at least a factor of (d/ǫ), where d is the number of variables in the Bayesian network and ǫ is the fraction of corrupted samples. Our algorithm and analysis are considerably simpler than those in previous work. 1 We achieve this by establishing a direct connection between robust learning of Bayesian networks and robust mean estimation. As a subroutine in our algorithm, we develop a robust mean estimation algorithm whose runtime is nearly-linear in the number of nonzeros in the input samples, which may be of independent interest.', 'bibtex': '@Article{Cheng2021RobustLO,\n author = {Yu Cheng and Honghao Lin},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time},\n volume = {abs/2105.05555},\n year = {2021}\n}\n', 'references': ['4c260343764c29db7d4039cfa14d88cf3169e11a', '1db7952118ebdc1d36ea20d9a2b74429a190994d', '18c20ed73e5a3424c91519664682130031a339af', '6a5126b6f980763e47d63c74526f11db5673cb45', '5c7869bdcdcd0dd2ee45918792706e39ed1280dd', '9726d843000d55e24800bbc37d6392f821598c09', '0257850fef6a0c87eccb75942e05f3a8d051f2b1', 'e1d853d3c696e6ffc89bd02c3398aee29623687a', '6af68fe0e76d08966803959998dc34ee738d9391', 'aa7768d1bfe128a7c70deeadf351644b7aa9ab75', '8599e1701c0598e27623419ef0b931c3e71dbac4', 'c8662d9009dc0b4462a2d48c2ba1f6a51f09e70d', '8904cdbe394593cdef96405b238fe5d3a4bdc03a', 'ad901015a61585f967d87e6acb847be9d18d7df3', 'c49f1881dcb84113fe3cd8c9dbe539ad1111e2f0', 'f403d6c5c79d235c9d021e9e65ab691141e88a4c', 'e405c59d9e13c4d72050535f00cd3696ac004740', 'a4f12ff9e04ff40c320a40f67f2e9340479c8d54', 'bfbaf90374e4a33910435ab13c1d18f3b4835a4a', '6b2ddc7eb3eff1adf2d69d9ef400dd5c4f6339ff', '925edae268cfdc7c6d75202da6963f2bf5770166', '2a6de51d86f13e9eb7efa85491682dad0ccd65e8', 'e7b392cb8c7b22cbd551fdd0be70eeb640b140f4', 'b501495a8467a93d559d6b3dda6184b0098b322f', '842ba35a690282acde75643bb1935a9b8b630b1e', '45ec4a51a3c821039872dbbe0cb91087b92a106f', 'd5c26e47890f90d7b9ff64f9b395ff4210349b1b', '026d83264f1590718a247c533fa912db34c7bc09', '144987426b7d5bc9027f8d2dd9620ecb6966401f', 'bd16fa2d0425e367385f00e6a924d37950361b82', 'd32496881afebd48ab360b81c017d08466d5fcab', 'e290d38eca3620965a2257aa12606b4a954e90d6', '1e70c6c8128ddd94e59095c652e5513eb521d4ae', '4406c54f40e0f73db2180704d454951649df32f2', '4c9abd36ab90c38432a21ff8957313648526ed1b', 'c7baa5d9d6e244586eb90929526fa7d4589d8ed2', 'dce7e2c8b4187069aa7062b56bce77a2a26eeeda', 'd98d0d1900b13b87aa4ffd6b69c046beb63f0434', 'b18c3b77cbe45cdb8869e44f3f8fa7289afa3858', 'bc4d9febd19e30f376e4d26deeeb75047bde24d4', 'a49a75990571ce0983bd045af8d2127bb6e3e9a8', 'b52b4d46aa2ff7775ec36fc0f3d3fcc02ea237f1', '27ae01b74d54035c28abff93b7e0dd0382139acf', 'd183977b599bd8b8dc711083d16717db55142ba6', '61a65e0d94160135b96cf7b484344449b53f621d', 'db95a993109f14558f3410ab8f6c1a1a5534a5e3', '683fe3bbf2b2e628cf40d90e35fb39effc63b7e9', 'f24d15247fe922982c7e3478db36c94825619395', '2aef78ad47756436deecf6b3578d65dc37a61753', 'c87d57da3b1f2b467ef4995d30df832ee2281107', 'b83151fcd9264ce85feeeeb9707b3655ecbe6dc9', '746546336375a9129b856d44f0740e6609024212']}
{'paperID': '656cec38e5295b94a17cf82e9ee9dd7b38296b49', 'abstract': 'Local robustness ensures that a model classifies all inputs within an $\\epsilon$-ball consistently, which precludes various forms of adversarial inputs. In this paper, we present a fast procedure for checking local robustness in feed-forward neural networks with piecewise linear activation functions. The key insight is that such networks partition the input space into a polyhedral complex such that the network is linear inside each polyhedral region; hence, a systematic search for decision boundaries within the regions around a given input is sufficient for assessing robustness. Crucially, we show how these regions can be analyzed using geometric projections instead of expensive constraint solving, thus admitting an efficient, highly-parallel GPU implementation at the price of incompleteness, which can be addressed by falling back on prior approaches. Empirically, we find that incompleteness is not often an issue, and that our method performs one to two orders of magnitude faster than existing robustness-certification techniques based on constraint solving.', 'bibtex': '@Article{Fromherz2020FastGP,\n author = {Aymeric Fromherz and Klas Leino and Matt Fredrikson and Bryan Parno and C. Pasareanu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Fast Geometric Projections for Local Robustness Certification},\n volume = {abs/2002.04742},\n year = {2020}\n}\n', 'references': ['72647fbd50bd6ae42d253116bc62504bb136f8d7', 'dd45dc3767230b70197420e1523dc0f1d7930f80', '3c742ebff7df12e3fd0194023975e2211e4ba67a', '064ec3a5e121cf997c3b57b23867c756cd141c04', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '5ca131d97019ff1e40a92fa1c9c4c5179632744a', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', 'ecba2826cd7a51d4d8b9820591ff0fa6b41d66a6', 'de49430578bb3f8de3e610423255662c45f17610', '33f68f93914704ba49b48b508e5bb93369f4e79f', 'd21fde0f55ee0285c66334d37b8920c867959784', '2410923ed90b099e3f5565b63e789f10bf70ec4c', '9db631435f7f79646a4e0a1841fbeb3340e44261', 'd2df6969b185a4017048f996d0e7cd1859c24e67', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', 'edce5208900a2702eee5b2452781909a4c8aa77a', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '333416708c80d0c163ca275d1b190b1f2576fa5f', '9f92a0ccc8b039a83bd5ba5482facb5829c712aa', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', '819167ace2f0caae7745d2f25a803979be5fbfae', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'ed7c7c079c8c54d3b82e016cc52a7a2c3a61f237', 'c77cd64f26228442ffff9219bfd870c83c8747c0', '89803ae9052541794bf34673101d88c02c15b19f', '641864f095a113c8a0e4f8e58647c4b22f5ef478', 'c1626528298f39c11834a66b34e21f645e46690c']}
{'paperID': '73dd7ff60ba551fcf1b7b13fdf65ae29d6e2f37c', 'abstract': 'Adversarial attacks against deep networks can be defended against either by building robust classiﬁers or, by creating classiﬁers that can detect the presence of adversarial perturbations. Although it may intuitively seem easier to simply detect attacks rather than build a robust classiﬁer, this has not bourne out in practice even empirically, as most detection methods have subsequently been broken by adaptive attacks, thus necessitating veriﬁable performance for detection mechanisms. In this paper, we propose a new method for jointly training a provably robust classiﬁer and detector. Speciﬁcally, we show that by introducing an additional “abstain/detection” into a classiﬁer, we can modify existing certiﬁed defense mechanisms to allow the classiﬁer to either robustly classify or detect adversarial attacks. We extend the common interval bound propagation (IBP) method for certiﬁed robustness under (cid:96) ∞ perturbations to account for our new robust objective, and show that the method outperforms traditional IBP used in isolation, especially for large perturbation sizes. Speciﬁcally, tests on MNIST and CIFAR-10 datasets exhibit promising results, for example with provable robust error less than 63 . 63% and 67 . 92% , for 55 . 6% and 66 . 37% natural error, for (cid:15) = 8 / 255 and 16 / 255 on the CIFAR-10 dataset, respectively. and shown that such can be The effectiveness of the proposed versus SOTA robust classiﬁcation methods is corroborated by empirical tests on MNIST and CIFAR-10, against large perturbations.', 'bibtex': '@Article{Sheikholeslami2021ProvablyRC,\n author = {Fatemeh Sheikholeslami and A. Lotfi and J. Z. Kolter},\n booktitle = {International Conference on Learning Representations},\n title = {Provably robust classification of adversarial examples with detection},\n year = {2021}\n}\n', 'references': ['0930f7234a861c34050685070e117c295ebb184a', '71ea8f105803703893b5c2d01f0c9508643b6554', '58c143069444c7dff4be53531a47efefc40be497', '6d4a87759917132913319960389f17fa1fe8b630', '9f862165c7b82eb4779484ff5ca2235a2fbaba91', '604dc3c7ad3736e58d7fd8a5839f8d8ba63e63b6', '1c1003f06c6cf0f61dfe0fcc0346c23d3c681fae', 'ff22e140a0423f1cf0595d213f36402668084014', '89fc3d8af739f3c41ff72a1bd0acf0571a8c533e', '95239030dcb8d9213b77cf04b03f2e14a900e08b', '449b8ee671510b7ba749074e053815a3a655b0be', '4f8c49cd3eccc2417c78e0b310698d9f603aa2e9', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', '37a9e8ce40c010216bad8a009fca168be2332dde', '5677b0ca32651b80aa3a210d691bfc93bf9fcc82', '5473175211aff4a8a099c44d1a57802d1b7ecf9e', '95ea893315923ff14d7632f7a7e5d07cdf800ea7', '676e40050453ddeb1387f8314478c0ac3681a8c6', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'bc5a73a755d43f8579770c0408b4b89ee5ae4fbe', '43a4a354b67ab6d5531355a368094815d2d2593d', 'de49430578bb3f8de3e610423255662c45f17610', '75339d34bdac0d21a41461228ec6088eecdf857a', '8d35663a80199b173d8cbd12dbf2300a9f86a021', '63ed798f1847d8a952cbeb75d3b286f16c144914', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '9de69a46e6c619255eeffbfbb6c7b7163690eb48', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '333416708c80d0c163ca275d1b190b1f2576fa5f', 'b0d675ebfbaf4b94bc517192bc5a6ea5890f60e8']}
{'paperID': '3797437ea7990e99cbba6e94d402650a842ba738', 'abstract': 'Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation accounts for the improved corruption robustness and increased shape bias is only a byproduct.', 'bibtex': '@Article{Mummadi2021DoesES,\n author = {Chaithanya Kumar Mummadi and Ranjitha Subramaniam and Robin Hutmacher and J. Vitay and Volker Fischer and J. H. Metzen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Does enhanced shape bias improve neural network robustness to common corruptions?},\n volume = {abs/2104.09789},\n year = {2021}\n}\n', 'references': ['5414cb3aedbbfd5550e1f8b1aa5e27d04003700f', '022622e024890d6e044ac50e2da6b44c59bdf418', '46803220e5f6b636b65ef0ff87c0c9c4b95dec31', '86974ebbf9f7d5797e8ea919a347c3dbbe9a1f17', '20ba55ee3229db5cb190a00e788c59f08d2a767d', '6673237e21e9bb0605704f4ba3ef7ef680558ab2', '87f6a7c014ce206ac5b57299c07e10667d194b39', '1b78742903fbf51a544fd40d4e2395cf8df044fd', 'f3b76f7a1042972009c37302ea00daa30238934b', 'db787640c9b42416ff8d7015546e667e58267177', '934d7bffdba0b560a80a518b99a791a16b3e198c', '4974595550d47d3d444cf37fed1cee592eb50fcd', 'c703618d1f97a2d2184a09bbd73520034a19ef56', 'ae0a728d10e4c61a66021bc02b01e7bdd13030b9', 'ed17929e66da7f8fbc3666bf5eb613d302ddde0c', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '4ae0c4a511697e960c477ea3e37b3e11bf3e0e02', '8c92054c26fb4c6dd7435bc99fbb8af3323eae1b', '49b64383fe36268410c430352637ed23b16820c5', 'caa2704320e3742bc611c30092acd7a7eb87d5d4', '810ae452a3a1f673ea241bd540f9551b2996ed5b', '23cfb692c55ab178a818d4a86d5d417981358086', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', '54fc23348ed840cb5f1fe2b41c80bfdcfc03631f', 'c67855ff840c9f8d256486158c63a242d912e41e', 'd08b35243edc5be07387a9ed218070b31e502901', '39c940bfe6750be6a0c3e3e42e3c2d697aeb40fa', 'fb37561499573109fc2cebb6a7b08f44917267dd', 'be0ef77fb0345c5851bb5d297f3ed84ae3c581ee', '1bbf746cca4bcafd274d197ac9fae82b245bf97b', '99542f614d7e4146cad17196e76c997e57a69e4d', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '8da55e685a7bef9c897788ab519a8710c695c419', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28', 'fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec', 'df24c3011fc42b72195e876ce052a0a072a1d923']}
{'paperID': '5ede529879d162d2779d410a5775d3f6cd6be3f4', 'abstract': 'Distributionally robust optimization (DRO) provides a framework for training machine learning models that are able to perform well on a collection of related data distributions (the"uncertainty set"). This is done by solving a min-max game: the model is trained to minimize its maximum expected loss among all distributions in the uncertainty set. While careful design of the uncertainty set is critical to the success of the DRO procedure, previous work has been limited to relatively simple alternatives that keep the min-max optimization problem exactly tractable, such as $f$-divergence balls. In this paper, we argue instead for the use of neural generative models to characterize the worst-case distribution, allowing for more flexible and problem-specific selection of the uncertainty set. However, while simple conceptually, this approach poses a number of implementation and optimization challenges. To circumvent these issues, we propose a relaxation of the KL-constrained inner maximization objective that makes the DRO problem more amenable to gradient-based optimization of large scale generative models, and develop model selection heuristics to guide hyper-parameter search. On both toy settings and realistic NLP tasks, we find that the proposed approach yields models that are more robust than comparable baselines.', 'bibtex': '@Article{Michel2021ModelingTS,\n author = {Paul Michel and Tatsunori B. Hashimoto and Graham Neubig},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Modeling the Second Player in Distributionally Robust Optimization},\n volume = {abs/2103.10282},\n year = {2021}\n}\n', 'references': ['6a3cc30d5d6342d912851deb4362b8c47fa5ede3', '2fbea3a1cad10c9bd5a01cd7eae6ca7f46f4e8a1', '03700ad9bea6cdb705fe51834f46cd037c78c13f', '034415a68c3f2d0710bf80d2b97bae1c583da4c8', 'e816f788767eec6a8ef0ea9eddd0e902435d4271', '43f2ad297941db230c089ba353efc3f281ab678c', '193092aef465bec868d1089ccfcac0279b914bda', '77568c594470f9aa029f92774e2c12ab0451d9bb', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', '8963317176fa81e185fd7a8f8cd001d7e11a4868', '1dcc2da3fde52cacdec926d5c4e2bb425959721b', '2673354bc246e65962a6dca32d5f41cc8f11a249', '44fc8d79fb8e0f8c6c6f680179b5803a789c6227', 'a1fb7236d104ae0343c1a09e3590ee2283483240', 'f9c56fb6e3001f3acbc994a894b4190d78270e1b', '52f7ae53e58c5098133d041794b4465d36c2fdb6', '290af67244094745eaa927bfa8a3727e93dba78b', 'c3b74808e482d13c46845980008fd1a578ec2865', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '6661802f5e1ee004c20a28dcce9b582d4b5fe6d7', '8dd6a2c9c88c9b3465484228c93f4dcc11cfeab9', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', 'efbd381493bb9636f489b965a2034d529cd56bcd', '1d9600229a4cf8ba5e8f5ad4d05b41af9c8f80a6', '7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c', '13497bd108d4412d02050e646235f456568cf822', 'e09aa121bca3a0b9a0fe5f7ca119f44791cba1d7', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', '687bac2d3320083eb4530bf18bb8f8f721477600', '583b55367f787eb0c4e295707b642e63547b9806', '831780b12cb41a9905c3d4f58831a2ea6d09223b', '6642e9c6cf7432e2d11b7edf7cd47f1285acd54e', '1187a77f857ad029168863ba0005ddf6d2b957c8', 'a7c183abb9b044bfbe1f09199ee970ea3a01104f', '2e9d221c206e9503ceb452302d68d10e293f2a10', '87cbed883368d4a9efd42fdd91f47038f8d8fbe6', '9405cc0d6169988371b2755e573cc28650d14dfe', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'e10642453c5c99442eb24743c4bab60a3a0b6273', 'f9a1b3850dfd837793743565a8af95973d395a4e', '26743ef8d4759d487dc3cc0993d3c6eed509e266', '8d56d4bc69a8c562434b9a129542bb79e9d6f1d6', '810b9ffea4c74db3923336a22dc9563679cfe564']}
{'paperID': 'e01eeb3fc6e681cd2edac0be9f6ef6493b485574', 'abstract': 'We consider a regression problem, where the correspondence between input and output data is not available. Such shuffled data is commonly observed in many real world problems. Taking flow cytometry as an example, the measuring instruments are unable to preserve the correspondence between the samples and the measurements. Due to the combinatorial nature, most of existing methods are only applicable when the sample size is small, and limited to linear regression models. To overcome such bottlenecks, we propose a new computational framework - ROBOT- for the shuffled regression problem, which is applicable to large data and complex models. Specifically, we propose to formulate the regression without correspondence as a continuous optimization problem. Then by exploiting the interaction between the regression model and the data correspondence, we propose to develop a hypergradient approach based on differentiable programming techniques. Such a hypergradient approach essentially views the data correspondence as an operator of the regression, and therefore allows us to find a better descent direction for the model parameter by differentiating through the data correspondence. ROBOT is quite general, and can be further extended to the inexact correspondence setting, where the input and output data are not necessarily exactly aligned. Thorough numerical experiments show that ROBOT achieves better performance than existing methods in both linear and nonlinear regression tasks, including real-world applications such as flow cytometry and multi-object tracking.', 'bibtex': '@Article{Xie2020AHA,\n author = {Yujia Xie and Yongyi Mao and Simiao Zuo and Hongteng Xu and X. Ye and T. Zhao and H. Zha},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {A Hypergradient Approach to Robust Regression without Correspondence},\n volume = {abs/2012.00123},\n year = {2020}\n}\n', 'references': ['d11c50e49998e2156da7c179a3caea86e9601abd', '6c8f84ba9a22450c3f14bb40e6dbcf9ac816d502', 'e1678aece552737a99e46b3e4ddb854347466e1e', '1c27871ebe0ec6c0d8611878541556a8f59747c7', '21b2e8702df2c2e3bd7ce760709bc221ea2cd52f', 'a1bbe29ebe5e42211b752c3a2b959cf6020556e6', '7ce80c7df1774e4483b32a813d54a8ff35dd0163', '61e26bd5333020a1aa4bf2f24605d265ee9f120a', 'e327593dc64ea38c6379b029a85d5de64a8fe9bd', '3826a4e2cc50ef2447a7908cd55f12fb9ad5ef86', 'f20049425c119f65cb7273e0abb1e48e848905b7', 'dc96f81fad85d3ccde836d8c0df09c09e59257b0', 'b848e7704e20338302f8ffc75acf9c10040584a1', '07ca883fe76d21a146808045cf48ca2304328381', 'e799c5c7e169f471950eb76dbb329c2d031347ae', '311433687edee9727f3c658140ad9e2c026093ef', '320d05db95ab42ade69294abe46cd1aca6aca602', 'a188e34579136f903f9f5939d71583c4fc0b2448', '287085ecd255081f8fe419a4b91db1a73b355208', 'ab6f383652f5ec1f78cb7c3c4d8ae5883407aad9', '3ecac5a84b7d8d8d15b25ecbd022ae353ad25c40', 'fcfefcee76261aba140b5d33bb557f8f89807ceb', 'd65ad6e4fe4229186d61798d6b796ca4ca8c3835', 'ea6dd69e4eed8fb5224fa96fbab47ae643eb1647', '4e31fe12a4af9d925531a6ce6fe3c5205efd31ab', '74da36b5da9af8c3b315367541f152a51866cc0f', '85aefde69e916523d9587b6abd01419420039474', 'a0fc60c26ffc730617355d208cab33e1f087e5b4', 'd21703674ae562bae4a849a75847cdd9ead417df', 'ac0d88ca5f75a4a80da90365c28fa26f1a26d4c4', 'a6e7513371a49cd7b8b30bb444e8fc448c5326cb', '9e53217d96aee1f2cc13c0157c6185806b60ffed', '5f13cef6f5a209c878a24192b3c38010ffa7cc5c', '7eed503702a760d49c744ada559481ce6cfd6502', 'f124d97459e402031e6f3731a871cff53b6a8c32', '424561d8585ff8ebce7d5d07de8dbf7aae5e7270', '7a7c849639e40f723c65ec98aacf746f9706fdad', 'd3831561301ef27a6ba0d3c68d30bdf0f27eef63', '0080118b0eb02af581ff32b85a1bb6aed7081f45', '400d517d1e51c4f292bac4f5f336745b6f8d4ec0', 'e79272fe3d65197100eae8be9fec6469107969ae', 'eebb60d3406659279fd5823d14e9fb81cdf91295', '2eebe44d0e8bb4326451b4c7880ab7b932da8ec7', 'ed7c7c079c8c54d3b82e016cc52a7a2c3a61f237', '0143dc12950df920e2540afc9afc3519de906632', 'a0554946249ac831077c76b345f95940e6c05496', 'ec27a1f0b5655736eaa7ef20cef274a75fa8ba14', '70e0d2487440b519b392503f4fa6b94891238c90', '7212b33956f1b36acc77d0da66fa2634ff5146c5', 'b63665e46a1c8e2286e7f88418fac29e3db8858a', 'd40afb32a77b52bf4775f8ab21ddadf94f239218', '7e98ece450ee547d4ffa42beaa4477a9b32ef077', '5edecf6197abdbb88c75e2938ebca50befab7360', '3f83d1db7b9a53087db3ea7c796285489d83f1a0', '8e56b9e532f6f850785d625635887cf61a8d63e5', '24a183b16c5c9ee803e8af3daa1ac5164c6e19e3', 'ee3d03e8a42fb063b53a644e56660f8125b840dd']}
{'paperID': 'bafb088c459188fd22fc20eb3af6b731d4856629', 'abstract': 'The memorization effects of deep networks show that they will ﬁrst memorize training data with clean labels and then those with noisy labels. The early stopping method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will inﬂuence the memorization of clean labels before early stopping. In this paper, motivated by the lottery ticket hypothesis which shows that only partial parameters are important for generalization, we ﬁnd that only partial parameters are important for ﬁtting clean labels and generalize well, which we term as critical parameters ; while the other parameters tend to ﬁt noisy labels and cannot generalize well, which we term as non-critical parameters . Based on this, we propose robust early-learning to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Speciﬁcally, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.', 'bibtex': '@Article{Xia2021RobustEH,\n author = {Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and ZongYuan Ge and Yi Chang},\n booktitle = {International Conference on Learning Representations},\n title = {Robust early-learning: Hindering the memorization of noisy labels},\n year = {2021}\n}\n', 'references': ['ccbcacc4fcde0eebcf27a774a874db7d1b391319', '5b0d1cf668c92f23fa9be2e5efe6d8c8ab9376b3', 'fde5afcab0731d751bb1ecca773b9bca7914db53', '39d087cd6a27a72174f365407cf82ac4450c552d', '99d86a078e6aacffa39489af93051c27f8432f10', '599ed9357448d8c55e2dc7f4f12224d5c6dd1fcc', '97ef364d56622a2d8431d9603aafec014f46ec0b', '233de8bacf9b57b208583965762ceb4b8e7f1d03', 'c9b08639a28ab70a06ba9a09eaae98b2fe3dc6c9', '07cca761749bfe21c2d096ff60f32b574d5c84c4', '194842d570ec3c18ef641f74cd56eb5a4c669656', '23858481090f7dbd8e8de859b0795a448cbcb478', '71c9b6f950f96bd7e01aaefdd1593aa7c5b9a090', '3d85d0d80c4052eee25bacb50b400cf61d402552', '560c7437538dbadbb1e3e27e309945f2befd521f', 'ce435482acc0e195be8d8f002b2655b4c7b08be6', '0fa23380cb18a4be40706459e088617b0232c79a', 'c385e811f98260b673d52abcfdb981b60e880695', '3ba8d3060731d64cd46d27e933cbdfb8b7853f4b', '4f62d8a036f7616510909d4a19db19b735264d9c', '4b5744dd44a0026c6f386d5cb21b795499d5efb7', '1b4c4f16a798dacbafb30ddc4511e03f98f59fd7', '509f652d67a793a614633d4a5c236592d80b6522', '77c474e38d2833cfa0edaf4a6098e413a76557c5', 'b235c83564f8cd4b27343ff30faf744929d7b961', '29090beb90c184a9aaf7aa610bfed5ee1631d2f2', '5d99d520410efec67a515e3bcc20ce8868e13b6d', '313b368457e54e6a7482b008d5eb4182eb1b4d1c', '611fe6e34df07ea1b2104899e49642b4531b53e9', '80c82b5182c8796e452946953d0879e02db68645', 'cf440ccce4a7a8681e238b4f26d5b95109add55d', '622727f595542afa3caf8802927d880818ddb17a', '67f7033265a9e6babdbe28472634270297d2eb46', '77a1dcdb52b5b76ae17fb4f181840797e58f7fed', '1e1855ca80e8ac3de0e169871f320416902e9ad1', 'e061d23b68e7d4aac5aece4794c044c80e638dca', 'c4c4bc0367ec099f1e00a7700332cd0bf393aa55', 'c5420ef59d7508d82e53671b0d623027eb58e6ed', '21937ecd9d66567184b83eca3d3e09eb4e6fbd60', 'a9022d8ffb5e417458fba9a280f90c1b08cb6c73', '780d18205252a4d3fc1ab4b4a2db55a370d0105e', '306a2e8ca31fdcc148618d37074785c290f96375', '32ed897dfeda3bfdb572d0f748048417bef2838d', 'd07284a6811f1b2745d91bdb06b040b57f226882', '7d34d84f0022fa7dfcecf7e9f5444c938d2bc3fe', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '5de5848dc3fc35e40420ffec70a407e4770e3a8d', '5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf', '96f4d4fc345698b9b44f034c0d63b704772c8386', '4f48c8653cd38cd18f08924c9304bc02ed7ea492', '54ddb00fa691728944fd8becea90a373d21597cf', 'c6850869aa5e78a107c378d2e8bfa39633158c0c', '91d331d2bdd5fc86400c40c497bcb4c741c652be', 'b5c26ab8767d046cb6e32d959fdf726aee89bb62', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '1ff9a37d766e3a4f39757f5e1b235a42dacf18ff', '5d6ae67e569f974360b107060c23cbb8a13b0687', '424561d8585ff8ebce7d5d07de8dbf7aae5e7270', '4568d7951f8f4d5604a4a8528ccac4ce40a43706', '8e3f12804882b60ad5f59aad92755c5edb34860e', '075f328ef87a076151feb4d5b1f97b66aa597a90', '4177ec52d1b80ed57f2e72b0f9a42365f1a8598d', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'e7b18110c70ccb71305dda7a973f89630ffd9879', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', 'b0816b4f1fdf1ebc324663f66485e05d58cbd1a7', '96261be30b1c3ed388efce149dd93772f27478f6', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2', '072d756c8b17a78018298e67ff29e6d3a4fe5770', '8c8f198d582898ef06aea1edc51cbc419d922a00']}
{'paperID': '05ad9fc4ba9fad5aade4b700e8505d37eefd0519', 'abstract': "We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. Drop-Bottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DMLab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance. As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction.", 'bibtex': '@Article{Kim2021DropBottleneckLD,\n author = {Jaekyeom Kim and Minjung Kim and Dongyeon Woo and Gunhee Kim},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration},\n volume = {abs/2103.12300},\n year = {2021}\n}\n', 'references': ['b68a38f6fb6061d641f85b0de5daf7372eb29da2', '1ec049a29d369294a5a5ffdfb67e872dce899dac', '27071f68e1bf7d564cdb0baef11f8a4f9320efcb', 'dcc2153afbbf2a21273d3ef02218805892a25e94', '2c3a1a088ef51548264197ed8882f42e0ad73a9b', 'c60d789bf93dee52fc1e076c005cfb8385c84719', 'bf7f1ada5feecc0992f71b39c1ebeccb19ae631b', 'fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71', '4cb3fd057949624aa4f0bbe7a6dcc8777ff04758', '4b59846404c085f5c9523c69cd790537613a3df5', 'af3825437b627db1a99f946f7aa773ba8b03befd', 'ca14dce53be20d3d23d4f0db844a8389ab619db3', 'b13130497dbeb5032e4d26d7c6bedbf0d0c112a4', '3116c094f9c72fc42b7f63809849f4a63dae6b71', '9198b96ad9e4d00b4d847af6becbd46e8c16eef2', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '225ab689f41cef1dc18237ef5dab059a49950abf', 'bd9febe8347eccaf5555865e549574d908d9d213', 'c06cc4f5741e9f0ae9b9f8fc76247e87ba5aac6e', '29e944711a354c396fad71936f536e83025b6ce0', '515a21e90117941150923e559729c59f5fdade1c', 'e7eef2ac4136ec93bd306d2c9c353a13729a4553', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'a473f545318325ba23b7a6b477485d29777ba873', '8b79a8dba98ad570df6b7cd85ceeccf32ab24339', '415229903f91a1f3fc7404f5e5997fde025c221d', 'bf654c3339f232f9d8f7cda7f00f4e7d5e97a7d8', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', '4d376d6978dad0374edfa6709c9556b42d3594d3', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', 'ab99cb06acad1011ae0cc791cb7b35b74577cdfc', '67107f78a84bdb2411053cb54e94fa226eea6d8e', 'a538b05ebb01a40323997629e171c91aa28b8e2f', '5729e9bc42a8c006fcdeaf6ca3434fabf49e7e7c', 'c76c62c5ab6c076a80f925d277ef04dd36f6bf9c', '57d697214af89d8b8f77eeaaf040a43d1a0c651b', 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c', '34f25a8704614163c4095b3ee2fc969b60de4698', '5d90f06bb70a0a3dced62413346235c02b1aa086', '810b9ffea4c74db3923336a22dc9563679cfe564', '2b114f4d05494fceb22473fcd29d940e9aa52bf4']}
{'paperID': '20e0abdfed000269d3eabe1313223701030c2ed9', 'abstract': 'Among multiple ways of interpreting a machine learning model, measuring the importance of a set of features tied to a prediction is probably one of the most intuitive ways to explain a model. In this paper, we establish the link between a set of features to a prediction with a new evaluation criterion, robustness analysis, which measures the minimum distortion distance of adversarial perturbation. By measuring the tolerance level for an adversarial attack, we can extract a set of features that provides the most robust support for a prediction, and also can extract a set of features that contrasts the current prediction to a target class by setting a targeted adversarial attack. By applying this methodology to various prediction tasks across multiple domains, we observe the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively.', 'bibtex': '@Article{Hsieh2019EvaluationsAM,\n author = {Cheng-Yu Hsieh and Chih-Kuan Yeh and Xuanqing Liu and Pradeep Ravikumar and Seungyeon Kim and Sanjiv Kumar and Cho-Jui Hsieh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Evaluations and Methods for Explanation through Robustness Analysis},\n volume = {abs/2006.00442},\n year = {2019}\n}\n', 'references': ['70dbe3e740a5e7927ccce00fd615365b08a6eaae', '320cb910e7b758bfc43f09a6ff34da3a71192a74', '5473175211aff4a8a099c44d1a57802d1b7ecf9e', 'eb84415703c70ac3ab1ac3d9a4a08f29016a5066', 'a34954d9e36ea6c57743f55124a6ae444b951c2c', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', 'f7f1b5437b9c97d67fae59142b242ce079d7cb08', '8dc8f3e0127adc6985d4695e9b69d04717b2fde8', '98cc371f4e3a39b5c69b4e8980a5990f9011f223', '5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1', 'f1173ca43481c1b33f4e7891ce77200e51eecba2', 'b862efa06baea0b032214675eb3c3645d5d69d46', '34783ad136c0105a78dbb3546100b8864d1b38ca', 'd00c7fc5201405d5411b5ad3da93c5575ce8f10e', 'd21fde0f55ee0285c66334d37b8920c867959784', '1d8f4f76ac6534627ef8a1c24b9937d8ab2a5c5f', '9db631435f7f79646a4e0a1841fbeb3340e44261', '748dc54a89a276639f9d571cdd8e7d1ae3f9a57a', '29176632807b17bf3da444713763b4b2b568306c', '1b225474e7a5794f98cdfbde8b12ccbc56799409', '8d8bc608da14bc0ce32c3a5d1fdfbe037993626d', 'de2447a25012c71ad316487b4b9e7378a4fcccc0', '4b23012689e0f17912fb38d4984775e567cff8d6', '135bafc83e9a73c88e759f98a28edfdb5c02f81d', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '442e10a3c6640ded9408622005e3c2a8906ce4c2', '7e29b68fcc39b7ae94e4d8b1edea93d058804a92', '7380e343dd4547e21d5118b16daf03d021d98c4e', '1a2118bed729579528deb51e745d58dd3629baf6', '08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1', 'f302e136c41db5de1d624412f68c9174cf7ae8be', '5c39e37022661f81f79e481240ed9b175dec6513', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', '4c41104e871bccbd56494350a71d77a7f1da5bb0', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'e96506ee4baab43fa81cf1870cf7befb4a71fec7', '6df11b0bb0244d4d36e8955436067cc5d19734fa', '51a55df1f023571a7e07e338ee45a3e3d66ef73e', '17a273bbd4448083b01b5a9389b3c37f5425aac0', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '1a2a770d23b4a171fa81de62a78a3deb0588f238', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'b04e995db245796dc207e25342ed1fc37a9d6207', 'bb92676f9ec13783ac664c268191f20944718f95', 'e00f2c23b9d441a9ec32f1cf58497bc4648a6423', '40723b6709b4ec64cf079255fc4c1f9f9e74a3ef']}
{'paperID': '02200d454717ffea6c1daf64d635ab945d4fa140', 'abstract': '3D pose estimation is a challenging but important task in computer vision. In this work, we show that standard deep learning approaches to 3D pose estimation are not robust when objects are partially occluded or viewed from a previously unseen pose. Inspired by the robustness of generative vision models to partial occlusion, we propose to integrate deep neural networks with 3D generative representations of objects into a unified neural architecture that we term NeMo. In particular, NeMo learns a generative model of neural feature activations at each vertex on a dense 3D mesh. Using differentiable rendering we estimate the 3D object pose by minimizing the reconstruction error between NeMo and the feature representation of the target image. To avoid local optima in the reconstruction loss, we train the feature extractor to maximize the distance between the individual feature representations on the mesh using contrastive learning. Our extensive experiments on PASCAL3D+, occluded-PASCAL3D+ and ObjectNet3D show that NeMo is much more robust to partial occlusion and unseen pose compared to standard deep networks, while retaining competitive performance on regular data. Interestingly, our experiments also show that NeMo performs reasonably well even when the mesh representation only crudely approximates the true object geometry with a cuboid, hence revealing that the detailed 3D geometry is not needed for accurate 3D pose estimation. The code is publicly available at https://github.com/Angtian/NeMo.', 'bibtex': '@Article{Wang2021NeMoNM,\n author = {Angtian Wang and Adam Kortylewski and A. Yuille},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose Estimation},\n volume = {abs/2101.12378},\n year = {2021}\n}\n', 'references': ['817de816932bf7a311fb52793f627e77a1573a9a', '20a60d836827b4dea659875bccab084da6336119', '9bd4a01fc784d4f222b2ded2e8960fa2cb5115b7', '9ce6780acbce5d7be7da499f62ac7358938908f3', '38ea78469ef3a89f420ccd9615751f44b9194a5d', '1195fbeed30ebfb13432b9a85c131af299c1dd42', 'e6c71df73d6a77f6fd88a0c70454fff46a32702d', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '0174d263d3a77bf03fce831a9a5ce2678e1959f0', '47962eb9deda5d280f135ac9eaaf1a35ec0dc900', 'efae64551ff0b38fb6ac938727a001a9892be67f', 'c8844833b24cc60a0fd5622b1eac7c234da58a75', '743eab7fa743dc00532ea7c2bc0f6f8d87c93405', '3fc26991557d28ff720edef2d284ef2663e1e530', 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', '155b7782dbd713982a4133df3aee7adfd0b6b304', 'fd577c0b62d4d2d2d088566ab7e87bfd2d5a43f1', '8b6afef69b14b97a272c667b6b9004e441085c89', '06da85e7bf9439f3b56efa9025b032f381f9bb5a', '7caa3a74313f9a7a2dd5b4c2cd7f825d895d3794', '2ddc9c1de2eb5eebd66e23e8fd7eb59622662f5f', '5dfdc1ed397deca1e8451d09b7699914020b20c1', '78a11b7d2d7e1b19d92d2afd51bd3624eca86c3c', '9e880a4283a3a4e67304f54e00daf30cc535eeef', '5ad704bff9bd8cc259ba9542755cc1ec5434b45f', '6f115fffa3a2af837cf869996e76b805e8f8cea4', '5aa26299435bdf7db874ef1640a6c3b5a4a2c394', '794d79e03c68cdf066b0d6a5f2c8e90c1d564010', '71b7178df5d2b112d07e45038cb5637208659ff7', '60c7a5d919806e18e31f139b7df9f1172b776f17', '4602bbec65b0c718d5887fdf2381fb7cee77a64d', '711a22f55111db6f5599076dcdd791a94a5e9368', '49a4c7a645638bf98f5cd252a8a44356e01c5de6', '46f30e94dd3d5902141c5fbe58d0bc9189545c76', '6d66c98009018ac1512047e6bdfb525c35683b16', '0f3c956766a250fb3c787cd7d62e45100575da35', '999be5bede5cc405feec806d07ad220408180039', 'df24c3011fc42b72195e876ce052a0a072a1d923', '71d67283157475c4e6460c52408c00e9f6b8d2fe']}
{'paperID': '4ea5678069a6c4213f53972872a211d780f9f42b', 'abstract': "Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant computational cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved `for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.", 'bibtex': '@Article{Havasi2020TrainingIS,\n author = {Marton Havasi and Rodolphe Jenatton and Stanislav Fort and Jeremiah Zhe Liu and Jasper Snoek and Balaji Lakshminarayanan and Andrew M. Dai and Dustin Tran},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Training independent subnetworks for robust prediction},\n volume = {abs/2010.06610},\n year = {2020}\n}\n', 'references': ['52bac2108400df6abbd729b1477b5e1d5aacf019', '175e3e14b7c8872f7c99638e90ca978d69b41297', 'b145ea2049648535d6081407ebd315b072248183', '22e8361f6bd05bb4d7fe17919acd91b54c82a7af', '260affdb5d8036f630f3cd1798e1c128a7d76390', '6aad42dfcc59a119269f9765936d9c55911971ab', '6ddf83bee5ca7a0542964e389a98adc1ed4a6838', 'f5d073c513208cdbfd1089b440f1dc69b205cd9b', '49b64383fe36268410c430352637ed23b16820c5', 'b2c8e834ac5f7be68b9ca3691d39925036dd74a3', '21937ecd9d66567184b83eca3d3e09eb4e6fbd60', 'f90720ed12e045ac84beb94c27271d6fb8ad48cf', '3b4d671a8c7018c0b42673ba581e5ff3ae762d6c', '3db8730c203f88d7f08a6a99e8c02a077dc9b011', '4b63e34276aa98d5345efa7fe09bb06d8a9d8f52', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '73801d5bab1dd5cc2aaaf8855e4365a1a5d0d109', '62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d', 'da6057368920585bcf2443295b98418840f1fc80', '88843f4372f7c0b8c820a164b64060983df12049', 'fa25610fb8586c2b50a3654edc5bb42fa7fc4729', 'ad574fa9347bdeb5e940312f238c07f825ac0ed2', '1c46943103bd7b7a2c7be86859995a4144d1938b', 'a22b36cf5dba3e85eb064220be7ef03be4efba48']}
{'paperID': 'e3f41f4b6b4e3ab740176f022bcad522ad4c38ec', 'abstract': 'A recent study', 'bibtex': '@Article{Chen2021RobustOM,\n author = {Tianlong Chen and Zhenyu (Allen) Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\n booktitle = {International Conference on Learning Representations},\n title = {Robust Overfitting may be mitigated by properly learned smoothening},\n year = {2021}\n}\n', 'references': ['dcfb420412d76600eb124625f62fb28499af1e8c', '49f4a967f66d740ee3efb704f70b8d5da197394f', '6218de58c9191acb49e106059fa9e3b1ecc502d9', '47b4744162537f40572cdd723f8f37fb489a3e75', '99a599d8fe56529f47e78243ed61250190f96196', '61f765deaeea4a195c6b77406ee65bf58513bb0a', '2ed374eece816e461efc0cd9b6207c05d38df212', '9851c5a26ce75133fcb5df4d7a949d943cffa9f7', '24cf86a418c9471e8001961c87697c825f0bba8f', '1e475a263f3e2f4a6552bfb3468b3748d940c39a', '7ea2a78a8d8a6327bd13aa4f2d9ace9231bd9662', '764eff31d9596033859895d9513b838d2c57a6fb', '232c55babd84ea40f40cccf2684dd46c02bb8a49', '962a8ffc7d72990a28d505f49a39108b4803c223', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', 'b27da51d2b33c67b1b366f6f3a1e61e84dbab230', '70a6f1820eec8152f4af826d9adf61f442a24743', 'd5b84236178d7805c2e7b503cc6cf4a24b7da626', '8de0b5b58f62f8fbc6c5b88692bcabcd93eadb30', '2fb43a4c5cab8215d510fc585ca81fb5ee8a3abb', '99716c3a0bcd2f587e3605f09888dcdcd3b4076e', '6d4a87759917132913319960389f17fa1fe8b630', '8fc7b672ee6dd2ee08cb2315d64be07ff9779e22', 'ea415809bf87ef4b99966c6c50de6cb996a02a97', '176875a416ff1b1a259e0efc7a03c5e2fa43126f', '604dc3c7ad3736e58d7fd8a5839f8d8ba63e63b6', '6c894670faadc7dc24228a86925f828edbe8085a', 'e1dea4c733ee7c98aaa42972452f545821b5d3b5', '0fd54f51e0c9dc4a3d1abccd0f5604625d7b3156', 'f8de25118af2abc4c48afb947d6ec298e05ef1e5', 'c68cd22de315a14587120e98bb02fdcf51edec46', '4f8c49cd3eccc2417c78e0b310698d9f603aa2e9', 'd33deae7f654b07ac8a5c437a4fa018c29e6af17', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', 'f79bfe86ced096597e6a8b4a88ca12f4b53be115', '4f53bb893ca92f0a1b9b3d1bd2ee5de3cdb7c0da', '36aa6c01c9683783499395953c6bc856d6101feb', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'f86f1748d1b6d22870f4347fd5d65314ba800583', 'a25b63a6a0071d7d88ff4671c1fd40f320a08533', '52a4555f85b18243a95b426d48aeb69e5b332322', '5ae786deaa875613e85ed2df0dbeec4301109f74', '5ca131d97019ff1e40a92fa1c9c4c5179632744a', '9eda74b1219572287e489f84134bb935f139c4e7', '49956bdcfb55d8b42c696a743843f2a1e232d739', '6effa092456e30e7e54954fd28b755e0a75b52b8', '98286df6d923d787f26e034bbaf3a5a64ac29cb1', '2444be7584d1f5a7e2aa9f65078de09154f14ea1', 'b8989afff14fb630ca58b6afa917fb42574228ee', 'f6195d8dc6aad8231e97b563246f2585842bc68b', '2f201c77e7ccdf1f37115e16accac3486a65c03d', '651adaa058f821a890f2c5d1053d69eb481a8352', 'ca9c1224636b0a7dd37340a4691c34a9914b5af8', '1cf361d02f5ad84567e48754f1a8f895653bc701', '9a089c56eec68df722b2a5a52727143aacdc2532', '8e37a3b227b68953f8067215828dc8b8714cb21b', 'd53fb3feeeab07a0d70bf466dd473ec6052ecc07', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '255d2c2af6d7abbbebfc03dab51cd8574ad3558e', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', '58123025178256279bb060ca5da971b62bc329ee', '54ddb00fa691728944fd8becea90a373d21597cf', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'c00f744f103a528f5b45bf0482f54b5e6a9f7740', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a', '1c4e9156ca07705531e45960b7a919dc473abb51', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '23ffaa0fe06eae05817f527a47ac3291077f9e58', '0c908739fbff75f03469d13d4a1a07de3414ee19', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'eb42cf88027de515750f230b23b1a057dc782108', 'be9a17321537d9289875fe475b71f4821457b435', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '1c5b068ce6dff86bf152312b99f3360456a00faf', 'e225dd59ef4954db21479cdcbee497624b2d6d0f', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': 'a8e5c059d2acc2030663a088cd21cd198641974f', 'abstract': None, 'bibtex': '@Article{Gowal2021SelfsupervisedAR,\n author = {Sven Gowal and Po-Sen Huang and Aäron van den Oord and Timothy Mann and Pushmeet Kohli},\n booktitle = {International Conference on Learning Representations},\n title = {Self-supervised Adversarial Robustness for the Low-label, High-data Regime},\n year = {2021}\n}\n', 'references': []}
{'paperID': '66d9cb93003f61e56f825d4bc62023ceceacace4', 'abstract': 'Neural network training can easily overﬁt noisy labels resulting in poor generalization performance. Existing methods address this problem by (1) ﬁltering out the noisy data and only using the clean data for training or (2) relabeling the noisy data by the model during training or by another model trained only on a clean dataset. However, the former does not leverage the features’ information of wrongly-labeled data, while the latter may produce wrong pseudo-labels for some data and introduce extra noises. In this paper, we propose a smooth transition and interplay between these two strategies as a curriculum that selects training samples dynamically. In particular, we start with learning from clean data and then gradually move to learn noisy-labeled data with pseudo labels produced by a time-ensemble of the model and data augmentations. Instead of using the instantaneous loss computed at the current step, our data selection is based on the dynamics of both the loss and output consistency for each sample across historical steps and different data augmentations, resulting in more precise detection of both clean labels and correct pseudo labels. On multiple benchmarks of noisy labels, we show that our curriculum learning strategy can signiﬁcantly improve the test accuracy without any auxiliary model or extra clean data.', 'bibtex': '@Article{Zhou2021RobustCL,\n author = {Tianyi Zhou and Shengjie Wang and J. Bilmes},\n booktitle = {International Conference on Learning Representations},\n title = {Robust Curriculum Learning: from clean label detection to noisy label self-correction},\n year = {2021}\n}\n', 'references': ['1f6de95137e96872274eedae1beb1bd55f03c57a', '07cca761749bfe21c2d096ff60f32b574d5c84c4', '82c77a88969ac0e3a4e55c9a7dc5ced4afee0225', '2b1effdbf6e9e34be4c98cbaf13821b8f642a73e', '41b058078b0fd949594282843a3df7df3e0957eb', '87f6a7c014ce206ac5b57299c07e10667d194b39', '3ba8d3060731d64cd46d27e933cbdfb8b7853f4b', 'fa3bcc1126adc5d4305e3db821949e46810c8f9b', 'b8317bc29531d78942a75fa493a852535f40b2d6', 'b235c83564f8cd4b27343ff30faf744929d7b961', 'c42816f497d663c681df20d48a6e66a5632600d8', 'eeecea3097cf5629eb72a06e5caaf24d774adce7', '58b4cf057bdd361be289601ef3dd69b4efbef83e', 'c307baff85987544b0b85821f4a5c02b889994e9', '625afaba9ec6bc5125c217be5cc176ce283e3794', '622727f595542afa3caf8802927d880818ddb17a', '1e1855ca80e8ac3de0e169871f320416902e9ad1', 'e061d23b68e7d4aac5aece4794c044c80e638dca', 'c4c4bc0367ec099f1e00a7700332cd0bf393aa55', 'c5420ef59d7508d82e53671b0d623027eb58e6ed', 'f81376925a626fc053883f440a1051b244d3e813', '306a2e8ca31fdcc148618d37074785c290f96375', 'b4bd88c2a349d29f7aeaf5c1fcb355021058b3fa', '4feef0fd284feb1233399b400eb897f59ec92755', '5de5848dc3fc35e40420ffec70a407e4770e3a8d', '5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf', '0c0017ae798f5850524941597616f87a77bb6226', '4f6610f23af1bf193ca55b4af119b9be733bce99', '4f48c8653cd38cd18f08924c9304bc02ed7ea492', '1342c1e1684620c019972e2679d5131f1e8a4a13', '33d6aa6c41ce3000161d9b5eea910a5b78e14330', '6ce1922802169f757bbafc6e087cc274a867c763', '9d986c130c4e903ce07bb63c7c5b411fc95b5f9c', '54ddb00fa691728944fd8becea90a373d21597cf', 'd2e4587744a89bad95fea69e08842cad6c8ff0dd', '91d331d2bdd5fc86400c40c497bcb4c741c652be', '4c20e7f95448ca3c1042a6d7fa5fa15ec27e9aeb', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '23ffaa0fe06eae05817f527a47ac3291077f9e58', '99487be08cab00554c5c8db73161b2615c694f71', '5d6ae67e569f974360b107060c23cbb8a13b0687', '21d255246cd7ddba24a651fd716950f893ea8eb2', '77e3c48aa10535276e7f570a3af594ba63de7d65', '44606e1209a47d1fcf88b90e306db9e4b84fa2c5', 'e6069f2448ade3d01d957a3248529a3658796343', '1c721511e4c0e21bd264ca71c0d909528511b7ad', 'f2f9acd903fc8a94fcb70671022f60630e69998b', 'c76f02c3ef58d269e692df862362c1ee3cf1aa37', 'f9a824511dc4d73a09a077414ea18018cea3e49d', 'a049555721f17ed79a97fd492c8fc9a3f8f8aa17', '282383a8c25e1987ea4a14146ed466d5b2d12727', 'b82a8645fbf29966848e4db9e3abff9f0840b9dd', '3a2102945e45a720167809d29df4e3321c707c92', 'ca029b1a3c2ff774f5f76b94367da4d6f7611572', 'f917b5bc88565501f4345b989de685367cd69574']}
{'paperID': '1a452afbaf41306dbc9cbeb5cdedb85b85eacb0f', 'abstract': 'Despite their success in massive engineering applications, deep neural networks are vulnerable to various perturbations due to their black-box nature. Recent study has shown that a deep neural network can misclassify the data even if the input data is perturbed by an imperceptible amount. In this paper, we address the robustness issue of neural networks by a novel close-loop control method from the perspective of dynamic systems. Instead of modifying the parameters in a fixed neural network architecture, a close-loop control process is added to generate control signals adaptively for the perturbed or corrupted data. We connect the robustness of neural networks with optimal control using the geometrical information of underlying data to design the control objective. The detailed analysis shows how the embedding manifolds of state trajectory affect error estimation of the proposed method. Our approach can simultaneously maintain the performance on clean data and improve the robustness against many types of data perturbations. It can also further improve the performance of robustly trained neural networks against different perturbations. To the best of our knowledge, this is the first work that improves the robustness of neural networks with close-loop control.', 'bibtex': '@Article{Chen2021TowardsRN,\n author = {Zhuotong Chen and Qianxiao Li and Zheng Zhang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Towards Robust Neural Networks via Close-loop Control},\n volume = {abs/2102.01862},\n year = {2021}\n}\n', 'references': ['6b8f965431aeb16aeb3f9cc387041bcb6edcec45', '925e6e1e9dec14569c3fe06d238f961443b3b48e', 'a595767fa35bcc84362f629fbc4d2d9b05d7342a', '085f82a273239242fd511037daf0a84e7e52810b', '3813b88a4ec3c63919df47e9694b577f4691f7e5', 'd33deae7f654b07ac8a5c437a4fa018c29e6af17', '0ebc300c16f01a4e94c8551997922fdb67ac1951', '00f9e7f7c83a70829da876ffffcedeaeba0f7a55', '84e1dd0722c752321a4c4a7778246ec285404614', 'f7bb1636ced9036b3d0edafc7d82ad43164d41a3', '2d015c62f258eaad5b578a3d9bef762e7943ec47', 'afa0d49c1399c752d6f4665d75ecec640c000468', '1cf361d02f5ad84567e48754f1a8f895653bc701', 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd', '5e3fd9e6e7bcfc37fa751385ea3c8c7c7ac80c43', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '37be889f4654312109dc9c53395fe117adb0f72b', '061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '41f1d50c85d3180476c4c7b3eea121278b0d8474', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '4543670c4b2d88a9b67525e0084044adef94ae76', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'bb1847fbde3d4809a6998c435fad81c782a38a39', '2a8cf34a315343311f3450671bb57c187ed6ef04', 'dc9047917d1ceb3805d954c73899ddd2d40dd5eb', '08b88af20db007702b57ddcbe0539681b6d3d558', '977706d1dc022280f47a2c67c646e85f38d88fe2']}
{'paperID': 'f659031ceb7bbdcb7b0690742f35e2924fd1ed75', 'abstract': 'Robustness against word substitutions has a well-defined and widely acceptable form, i.e., using semantically similar words as substitutions, and thus it is considered as a fundamental stepping-stone towards broader robustness in natural language processing. Previous defense methods capture word substitutions in vector space by using either $l_2$-ball or hyper-rectangle, which results in perturbation sets that are not inclusive enough or unnecessarily large, and thus impedes mimicry of worst cases for robust training. In this paper, we introduce a novel \\textit{Adversarial Sparse Convex Combination} (ASCC) method. We model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution, thus aligning our modeling better with the discrete textual space. Based on the ASCC method, we further propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experiments show that ASCC-defense outperforms the current state-of-the-arts in terms of robustness on two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural language inference, concerning several attacks across multiple model architectures. Besides, we also envision a new class of defense towards robustness in NLP, where our robustly trained word vectors can be plugged into a normally trained model and enforce its robustness without applying any other defense techniques.', 'bibtex': '@Article{Dong2021TowardsRA,\n author = {Xinshuai Dong and A. Luu and Rongrong Ji and Hong Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Towards Robustness Against Natural Language Word Substitutions},\n volume = {abs/2107.13541},\n year = {2021}\n}\n', 'references': ['a58c97f8421ad97da4a08c8d45b8e355ab7de2ad', '309b906fed883e5efe4acf676c655ead21f6c17b', '8dad8858ff6d1160f9c0764fa101f78a9fae6bc9', '4690190d6c110f7525f7250e1acf4a4eab42519f', '07398e448180ad75c44d30f23a65289d40ff6f52', 'a3347bbd82938788ec085772813c095de17a0b37', '1adfa30bf112de20cb959014e44626d760aa8e4e', 'afd975a296886e89722891ad13c8dba0d26b1ed2', '838baf0acddf90946aeed45945613050f137743e', '4f8c49cd3eccc2417c78e0b310698d9f603aa2e9', '162515d87256f13888d9d7ba95275ac4b6c35396', '4dda68faa3ea2c888711ce5ced009afcb612e05b', '7c0a73771778fa8362c3e3abe7734dc9b1de3c76', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '472644c5f4155635cf9e9e37540bfa53c20e7610', 'fd7789de401811fd8692466b8d49230e7184655f', '107a53a46f3acda939d93c47009ab960d6a33464', '23c884679d59c5557413ff0932f3580faae3f42d', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', '2b110fce160468eb179b6c43ea27e098757a56dd', '51de2f73ad68a0ff2289d4e02957a07ffc4236f4', 'f09db8a42d713774483f023b31e0ee96361823f4', 'fa12574c228542151ccd7d4e3f42cc4896cd274a', '43d1fe40167c5f2ed010c8e06c8e008c774fd22b', '514e7fb769950dbe96eb519c88ca17e04dc829f6', '765bdcf27ebc1eb03a14f1e47aefa4dda1e03073', '3502b5ef1afb16f76bcae33db17179195bbcdaae', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'ca062c5e48d230d0ac51da96d492f3cb4bf82b39', '4b1c6f6521da545892f3f5dc39461584d4a27ec0', 'b663e16f3b466278c0ee24f1780935755c6dd436', '16aa01ca0834a924c25faad5d8bfef3fd1acfcfe', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '2cd8e8f510c89c7c18268e8ad51c061e459ad321', '2cd55ded95d5d13430edfa223ba591b514ebe8a5', '1439e05971a053c2368e6dee6d484b43c833d43c', '819167ace2f0caae7745d2f25a803979be5fbfae', 'f04df4e20a18358ea2f689b4c129781628ef7fc1', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'f37e1b62a767a307c046404ca96bc140b3e68cb5', '075f328ef87a076151feb4d5b1f97b66aa597a90', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '649d03490ef72c5274e3bccd03d7a299d2f8da91', 'f66d61c73de8ee9fd75f4aaab3575db15321ccfa', '73fb1f854dd152af88835298a54b9bfbb9f711ce', 'd87ceda3042f781c341ac17109d1e94a717f5f60', '37ae0ead03605de4c72f6d77af19b9001c0937f9', 'faffddbd4996df1a4f49e9f5ba86e90be372d38f', '5b716dcc220d26af9e7d5b76978dc2d5b265386b', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'be0dd2e91bb104494feeb5da2761cf930564f650', 'f97e0c07b44be324ec6cb2b8b2604a88596af886', 'b83e9556c42015f36da632ca39ac565087f54e63', '4a8c4354756c69545c8c57390c26a8dd41039724', '810b9ffea4c74db3923336a22dc9563679cfe564']}
{'paperID': '118a605ad954c8f8e1ad65941429d0fd2c14c918', 'abstract': "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a meta-initialization} of model parameters (that we call meta-model) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how adversarial robustness can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study WHEN a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate HOW robust regularization can efficiently be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.", 'bibtex': '@Article{Wang2021OnFA,\n author = {Ren Wang and Kaidi Xu and Sijia Liu and Pin-Yu Chen and Tsui-Wei Weng and Chuang Gan and Meng Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning},\n volume = {abs/2102.10454},\n year = {2021}\n}\n', 'references': ['17293cd36ee5e7ec37dcec1d5ab85f9b77ad65d5', '393619a12436701c796d8b0bf9e72efb68c4913f', '24cf86a418c9471e8001961c87697c825f0bba8f', '9a56ab8b1aba50dc2fea3cf4b531d30891a88ba9', 'af985dea540bd489397e7d28affa10f32a4d7167', '962a8ffc7d72990a28d505f49a39108b4803c223', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '6d4a87759917132913319960389f17fa1fe8b630', '2d75cf1dc599d7274fa57a02af5c2da2747db36c', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '0c1513b21d703e7e0d5a1d99a2ad5f6614a2706d', '1563e56e22d40769bdda91bffe481cfa3b9dac6c', '837ca5b8e57262398d3540649bd9545e6d6291d9', '5d28bdfa02f0766febff32b7a6b287611d6f2995', 'abf5478c24664a1380b7e213a3ab1c4af54775d0', '6ad5f1d88534715051c6aba7436d60bdf65337e8', '341880efaef452f631a4a5cd61bef5dae47741d7', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', '0d94293388417458eae73632e33840a772375900', 'd33deae7f654b07ac8a5c437a4fa018c29e6af17', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5', 'aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '676e40050453ddeb1387f8314478c0ac3681a8c6', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1', '3a26e26cfa8e8dbd70145f76e058d55fd83997f7', '0b949c29b118d7e4321efb777c99d7f2e36984df', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', '208cd4b25768f0096fb2e80e7690473da0e2a563', '90dc22818bd2d97d8deaff168b0137b75a962767', '9c0912153128e31743948cd629d6eebe6d916b42', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '68409946aa855b9a14de341bd321c38762817122', 'c269858a7bb34e8350f2442ccf37797856ae9bca', 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518', '470d11b8ca4586c930adbbfc3f60bff08f2a0161', '29c887794eed2ca9462638ff853e6fe1ab91d5d8', '3904315e2eca50d0086e4b7273f7fd707c652230', 'be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6', '815c84ab906e43f3e6322f2ca3fd5e1360c64285', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'be9a17321537d9289875fe475b71f4821457b435', '7ce7f49ffcb3cf58a5bcad7e6d53e42579e53f5b', '53bb7789be36a58f865f2ec84f6d8f816ddaae6a', 'd3fde30a26bff87d24aba05400334a8dd5aba2c3', 'f216444d4f2959b4520c61d20003fa30a199670a', '810b9ffea4c74db3923336a22dc9563679cfe564']}
{'paperID': '6d60d0dd613fb1bc9b6a52f6b3e8b65599cade5a', 'abstract': 'Overparameterized Neural Networks (NN) display state-of-the-art performance. However, there is a growing need for smaller, energy-efficient, neural networks tobe able to use machine learning applications on devices with limited computational resources. A popular approach consists of using pruning techniques. While these techniques have traditionally focused on pruning pre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et al. (2018) has shown promising results when pruning at initialization. However, for Deep NNs, such procedures remain unsatisfactory as the resulting pruned networks can be difficult to train and, for instance, they do not prevent one layer from being fully pruned. In this paper, we provide a comprehensive theoretical analysis of Magnitude and Gradient based pruning at initialization and training of sparse architectures. This allows us to propose novel principled approaches which we validate experimentally on a variety of NN architectures.', 'bibtex': '@Article{Hayou2020RobustPA,\n author = {Soufiane Hayou and Jean-Francois Ton and A. Doucet and Y. Teh},\n booktitle = {International Conference on Learning Representations},\n title = {Robust Pruning at Initialization},\n year = {2020}\n}\n', 'references': ['758cf7cd62bb8b62a6a4bc550a34e0a574bbbcb2', '0932abfd0fb90e8a28f7bd195633c9891bfd7ecb', '3b0fb765716ef6861a84abffcbe40643857c613b', 'dc418c0b5ac24a67fef336323ef0417600ba3718', '542d5cfd36963e35e55e2596b46d63a552367801', '296adb9c495c3105924e3f9d9dc81ea2b224d3a6', 'c114ce10c4a315d92c3815f54bc9893e7e6ef182', '231c8bdecec9a7a8d64766e75d69fb9dd2b473b7', 'cfc52ae6ab8d0c79d32b22415c96ce71c63baa48', 'e7907e7e7d470a12bdab5e6381ad12c4f832ea49', '1029daa28aa772e441470e61bdd610c222e92932', 'bd3df472bc848083068a76e9ce2b2ab49543dc78', 'e5b7c1ce5a46e059fce96249c0c034afdd3c287a', 'e663797275a20c0ab960772fe74e86c855b33767', '9f9fc406c76255fec51a6196ce167c0ff1d1efc0', '9b15a6f2434b9274cd1228eed4288b98cd316394', 'd6f6d1504cfedde4efb23e7ec0f42f006062c6a0', 'cf440ccce4a7a8681e238b4f26d5b95109add55d', 'ad6309d1ea001098189425f54d069ef12abcb583', 'd719009bade1c245ac6e2fa9e4cd74eddd4f34b4', '06ee9741730e4a2c7c8cdf643f5f34bc497a0b7c', '21937ecd9d66567184b83eca3d3e09eb4e6fbd60', 'fb350d3b03e9308ccbd131d3d45dd44e383e6227', '751201109e644e1422d025fe8433f29570997b7d', '2ec7156913117949ab933f27f492d0149bc0031f', '45a154f8be8ec31821a0e409d4b69635670a2e1e', '075556dd42900a6bc4552a2f2531ba21b9b7b4c0', 'b57dc63e5149dcdb94f344d2d47c048edd00e2d4', '773d5ddc414424a8948446ddaa5275b944f50891', '17ebe1eb19655543a6b876f91d41917488e70f55', '4fdc7df2c737141a1bf5aec27a438b77d01f8af0', '54ddb00fa691728944fd8becea90a373d21597cf', 'c2a1cb1612ba21e067a5c3ba478a8d73b796b77a', '5694e46284460a648fe29117cbc55f6c9be3fa3c', '6e997fec1412abb4b630d0e6d4df95813a01e093', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '9d3582ae92b9e42db13dbb56d30f782e60068aa9', 'e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724', '0b8dedcc16c6203e1f39c4f35c03932ff40a4275', 'e7297db245c3feb1897720b173a59fe7e36babb7', 'a87953825b0bea2a5d52bfccf09d2518295c5053', '629c4f332b5c84ce024dec21245cfce940954882', '810b9ffea4c74db3923336a22dc9563679cfe564']}
{'paperID': '7915568a86a196c06bb70fb98b3bc076a5a18b8d', 'abstract': 'When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often result in simple controllers that perform poorly in the average (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. We propose a technique that combines the strengths of these two approaches: a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Specifically, we show that by integrating custom convex-optimization-based projection layers into a nonlinear policy, we can construct a provably robust neural network policy class that outperforms robust control methods in the average (non-adversarial) setting. We demonstrate the power of this approach on several domains, improving in performance over existing robust control methods and in stability over (non-robust) RL methods.', 'bibtex': '@Article{Donti2020EnforcingRC,\n author = {P. Donti and Melrose Roderick and Mahyar Fazlyab and J. Z. Kolter},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Enforcing robust control guarantees within neural network policies},\n volume = {abs/2011.08105},\n year = {2020}\n}\n', 'references': ['4350f9b0fb5c2fbe6791cab4990d42908358bed7', '9ec0bd60df62b997bb371e65d254434f8851ea9b', '4e97fd0688a04f3749252a3ae47b1726bacb6999', 'de93aec00784fba2b9138cad0f9bc6987eacc8f5', '88e83776313effc1564044d7bf19972981815e3c', '689532925c32fac5bb1aabed390c0c0a057b058f', '320b227027030fc291de2896fc3c6da49d7614be', '3e411d95097a12e63cc4812ab4c3fdaba50df85e', '513ea47003a522cebe03f2f1bfe96b3efc598234', 'd3850595d3ae7c73e9488054c9b437f75511b569', '4fee3220123ac6d9294b35cab0dce3fc313a33aa', 'f3b78a8b96eec9b30721022e10604a4b04a4f23b', 'de5b28e232178ad7a3ed1c048563b4184a2ae8ee', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '7a4193d0b042643a8bb9ec262ed7f9d509bdb12e', '88880d88073a99107bbc009c9f4a4197562e1e44', '35d86b54da61d95cd2cf8950b23874b5442954e7', '9c4082bfbd46b781e70657f14895306c57c842e3', '0076b232181e4e5be58dce8354a813ad2bbf663a', '30f2888e81d955ac9d41b66580cfe3c3b3bcfca2', 'a9beebe284b2c70895d4f51fe14fc50eda41fc60', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', '410c3573849dfb673ce5c6f5b3108db7a0745551', 'd0b0c3e5a1e768490bc9b759685930541957508b', '04f014a6c23eb56bece071b0a5d35e3545cd9685', '4bfadf34f26cc9ef5594b349d89cfc16b93ebbfa', '9ab97c847003bec63578fda689d3d29226acdeb4', 'c800730e10a269dfd61298d45fb3ebd17681c2e9', 'a2809d0a46df8fde2eb579f1662cb1bc3b755583', 'ce6406918b9e029a89eb093a50281888a996047b', '7ef670b391f1d2511ca89c3455a764cd7310c797', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', '1c0f7087367315e4e8cd1d8654ab33db12663c2b', 'b7f9d1bb891935f6c41738a61c27f1018607af1c', 'f61117966b1ef9cf9f016e62bca19509dae33a9b', 'fda6ba40c325b6fae89ddf2713832387eeaf2af8', '05697e41fff4bbf3aad38ed5626cdddbd6057ae3', '9ebf90f7e96961c34f2dad649d9f450acd997506', 'ac26c803bdb46b4ade1e2e4a5ec580665cebd112', '8fa6729de2ab91811568af264c25a73884b2f1b0', 'fde4234a537f35167acbab2c1e179fa896a92ecb', 'e16d3aeeabc610ba4a9012ee85b16521734b97e8', '2e50405acb7818717e39ea2821f665601d0a5662', 'adac5f6e4324a9d277242b2e9e86ed671bb7f618', 'e48ed1073dfacaa79d00c8cfdc948d6871992f1d', '427ce4ce5f64295f39c8714de6ff09fb3bb7d187', '839c64b86d978baf5180381c975c0947eacdb7ba', 'cceb877d4d909d02ce6bfe6402928524842adbf5', 'f9f56f478fd001e41380c59b264e0a47a420f645']}
{'paperID': '42a37d42369c08f15f55a57e522d0a177da20ab7', 'abstract': "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", 'bibtex': '@Article{Cetin2021DomainRobustVI,\n author = {Edoardo Cetin and Oya Celiktutan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Domain-Robust Visual Imitation Learning with Mutual Information Constraints},\n volume = {abs/2103.05079},\n year = {2021}\n}\n', 'references': ['709422784b059294bb8546fe370e1f879bbbcd27', 'a71b917c48e892a2ee410870bbb2da488519e9aa', '465c4fe8e4e4d43cfc89802a76b99bbcaaaa565d', 'bbd93f7a687cdece41b2d92399525f03cc00cede', '2542f383c4002f7e523e1bf44caaea6d68beaee6', 'f8c24f7bd963cf540e7646e3c18e6f39cc234bfa', 'cbcbdb44d9d4ad7bb6bf4e9104653aa7623a17c5', '12c0751b4f51ed833172a713b7e32390032ead93', '4b59846404c085f5c9523c69cd790537613a3df5', '876053977063ab843dd24c78425cbad1779a62ed', '84de7d27e2f6160f634a483e8548c499a2cda7fa', '6b73775f40467aed52784ff355b9bb7168e9078c', '811df72e210e20de99719539505da54762a11c6d', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '77fa0239b9b074e7b62ca3798b8abf6fa3823f80', '2adae2da173b9dd720c8bcac0250a90a7f1ec697', 'edf73ab12595c6709f646f542a0d2b33eb20a3f4', '7f710932a0aa99f2c2ddbec6e7765f10c48d3fc2', '2e1a1b9c2e8feeb31c6855292859bf94101e8382', '9a700c7a7e7468e436f00c34551fbe3e0f70e42f', 'a31d0e5668b311b1ec74c5607a9f96c35b395fa8', '4ab53de69372ec2cd2d90c126b6a100165dc8ed1', '183357d25d6bad84e16bdb8db8f57ad616f3a9ce', '04162cb8cfaa0f7e37586823ff4ad0bff09ed21d', '024006d4c2a89f7acacc6e4438d156525b60a98f', '9ba266a4a4644e877fc37a64be3beddce8904cf7', '449532187c94af3dd3aa55e16d2c50f7854d2199', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', 'f81a40aac09d2131bd5f25eafb962a4d26cffe50', 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1', '5c356cad0d75cc1139d5293443940027a82595d5', '79ab3c49903ec8cb339437ccf5cf998607fc313e', '92c78a1eeb3fb335f4427a46c8741b37b2f98f1e', '117a50fbdfd473e43e550c6103733e6cb4aecb4c', 'f65020fc3b1692d7989e099d6b6e698be5a50a93', '8d652a1980e743c7c85ff6066409ea1e3be4d685', '2eb46dd423a351c25dc643f1ce8c8b54d8b48527', 'b05b67aca720d0bc39bc9afad02a19f522c7a1bc', '7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3']}
{'paperID': '473a854a939eca4bf39420ff496f8e24e223d460', 'abstract': 'We present adversarial attacks and defenses for the perceptual adversarial threat model: the set of all perturbations to natural images which can mislead a classifier but are imperceptible to human eyes. The perceptual threat model is broad and encompasses $L_2$, $L_\\infty$, spatial, and many other existing adversarial threat models. However, it is difficult to determine if an arbitrary perturbation is imperceptible without humans in the loop. To solve this issue, we propose to use a {\\it neural perceptual distance}, an approximation of the true perceptual distance between images using internal activations of neural networks. In particular, we use the Learned Perceptual Image Patch Similarity (LPIPS) distance. We then propose the {\\it neural perceptual threat model} that includes adversarial examples with a bounded neural perceptual distance to natural images. Under the neural perceptual threat model, we develop two novel perceptual adversarial attacks to find any imperceptible perturbations to images which can fool a classifier. Through an extensive perceptual study, we show that the LPIPS distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Because the LPIPS threat model is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against 12 types of adversarial attacks and find that, for each attack, PAT achieves close to the accuracy of adversarial training against just that perturbation type. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial defense with this property.', 'bibtex': '@Article{Laidlaw2020PerceptualAR,\n author = {Cassidy Laidlaw and Sahil Singla and S. Feizi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Perceptual Adversarial Robustness: Defense Against Unseen Threat Models},\n volume = {abs/2006.12655},\n year = {2020}\n}\n', 'references': ['4d3e1c4b597a44e7f020e911636f7ccc739df3d3', '63d529b0a9aabfe12a8fc6c5eebda869fc471ed8', '18939eadc9c4460c8385e0591cde214a1ead067b', 'e6bd6b24b17e7b2df30a2787488276451bbb3344', '849e1797a24550d6a525f4475ace7c2f87c479ef', '604dc3c7ad3736e58d7fd8a5839f8d8ba63e63b6', 'a30e33d5323343fd02be10975946e14df4a962e8', '6c894670faadc7dc24228a86925f828edbe8085a', 'fb38fc75f58cf8e171d59b868b1afbddbb9a28eb', 'daf8cd0f2c159d022477914bfacee9ff6da70c8b', 'd66c7ec5cdbf4df77789748d9173e2c4775933f0', '1e01fd005f5f1c2cf1b27c1ac8b014dfd3983da3', '49b64383fe36268410c430352637ed23b16820c5', 'c59841c623a6cde0413349d5c0f70fd0d93a3eba', '7c0a73771778fa8362c3e3abe7734dc9b1de3c76', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'e749e8c947550485eddf864f8efeb870b894e4ce', 'ceb2ebef0b41e31c1a21b28c2734123900c005e2', '052252e5d5523e9b6488f0f6022bf3621f44e0be', '1b9c6022598085dd892f360122c0fa4c630b3f18', '5023544ad6fa49b35526a62f22207e43c4db870d', '60104351ac65115503c9e92e856bcab6a13b0ce8', '48d88124dd89d988386ae452516dbba6c4a96e85', '578c891983e2fb02693b748879112d7f8a9add46', 'c468bbde6a22d961829e1970e6ad5795e05418d1', 'd3c071dbbb4520ed5875f7e064a9da87240534db', 'e3b17a245dce9a2189a8a4f7538631b69c93812e', '0314e777333a63aca5735ea136c74e113aa8801d', 'afa0d49c1399c752d6f4665d75ecec640c000468', '704cffb06e002faf5d8822e5d9f9a2046deafa3a', 'b36a5bb1707bb9c70025294b3a310138aae8327a', 'd295a620fc10a7a656dc693e1b1bf668d1508a8e', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'e7867244de690a1ae11a7a6d5a021e868fa75a3c', '1f07761d481d03d2dff9ea64ddf5ff5ffb3da445', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'eb42cf88027de515750f230b23b1a057dc782108', 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'b69d13c6a1b848c7f4816a73fbdb1d396a03928c', '13eb5c34d9c4c2374b982897a3d762c7d58fa3aa', '4834f221a260e02a22aec77a97472dc437a2ad80', 'eae2e0fa72e898c289365c0af16daf57a7a6cf40', '554cb0e8a604701ca78f2d782f2a26119eadaa81', '1c5b068ce6dff86bf152312b99f3360456a00faf', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'c2f5a9007603481292718ee1b878bf10f0e7b933', '810b9ffea4c74db3923336a22dc9563679cfe564', 'f6a883e5ce485ab9300d56cb440e8634d9aa1105']}
{'paperID': 'df872e72e87a85f9b5cd28da06ace46386462fde', 'abstract': 'The study of adversarial examples and their activation has attracted significant attention for secure and robust learning with deep neural networks (DNNs). Different from existing works, in this paper, we highlight two new characteristics of adversarial examples from the channel-wise activation perspective: 1) the activation magnitudes of adversarial examples are higher than that of natural examples; and 2) the channels are activated more uniformly by adversarial examples than natural examples. We find that the state-of-the-art defense adversarial training has addressed the first issue of high activation magnitudes via training on adversarial examples, while the second issue of uniform activation remains. This motivates us to suppress redundant activation from being activated by adversarial perturbations via a Channel-wise Activation Suppressing (CAS) strategy. We show that CAS can train a model that inherently suppresses adversarial activation, and can be easily applied to existing defense methods to further improve their robustness. Our work provides a simple but generic training strategy for robustifying the intermediate layer activation of DNNs.', 'bibtex': '@Article{Bai2021ImprovingAR,\n author = {Yang Bai and Yuyuan Zeng and Yong Jiang and Shutao Xia and Xingjun Ma and Yisen Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Improving Adversarial Robustness via Channel-wise Activation Suppressing},\n volume = {abs/2103.08307},\n year = {2021}\n}\n', 'references': ['dcfb420412d76600eb124625f62fb28499af1e8c', '6218de58c9191acb49e106059fa9e3b1ecc502d9', '47b4744162537f40572cdd723f8f37fb489a3e75', '1e6f6ede6707c4687c755851bc6924c1b55ef9c5', '764eff31d9596033859895d9513b838d2c57a6fb', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '26cb37bf23299d18af4a76b11907e821910f8e88', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', '58c143069444c7dff4be53531a47efefc40be497', '43f2ad297941db230c089ba353efc3f281ab678c', '2fb43a4c5cab8215d510fc585ca81fb5ee8a3abb', '1067c814c5d517cd50af176f3c919493fa799c0f', '8d5ed715390944c3bf07f826377ade48de4fba1a', '2172552b917ef3757b0af47d17fce18586d56cba', '91a05cb84f1c7dbb0354da2ff11ae92549152435', '449b8ee671510b7ba749074e053815a3a655b0be', '3402a06dbce96fc06e7c0089add732499d78e3bc', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '717425092f2e7a2a00c98e8e1778036e9b5a8b4e', '8d7cff029570cf4dc15fc49693067154823a562e', '0ede3139fc2c7ae3d39d7498298ed1bd9c1255aa', '144a8fb6e9d573f7ffb0768846f94d7859811d44', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '41071dbbbcbb27af3fec70de045f19c28535f5b7', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '986139329ca5863b4f65d9cbca0ce49de910d487', 'ef0a6e167c95927dac6d61d96e2d08da413868ac', '99ce59c65f1bdf1ae2592e5d4f939b42c084e193', 'f0c5991dbb130fa6b5de011cf7a04f6ed815ef68', '2f201c77e7ccdf1f37115e16accac3486a65c03d', '651adaa058f821a890f2c5d1053d69eb481a8352', 'a18ada04d93981178234d9c8907fb99ea92fddcb', 'ca9c1224636b0a7dd37340a4691c34a9914b5af8', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'da1231a3a7536010ddb6ef5e163a785d03974af1', '99e5a8c10cf92749d4a7c2949691c3a6046e499a', '4cfd770ccecae1c0b4248bc800d7fd35c817bbbd', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '4f9f7434f06cbe31e54a0bb118975340b9e0a4c9', 'eb42cf88027de515750f230b23b1a057dc782108', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'e5693e6b6d7551a82b64cd4ca0e2071c3dfef4c9', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', '1c46943103bd7b7a2c7be86859995a4144d1938b']}
{'paperID': '0abff557bf1c38a1fbbbeaa01dea66cd1ac5e988', 'abstract': "We show when maximizing a properly defined $f$-divergence measure with respect to a classifier's predictions and the supervised labels is robust with label noise. Leveraging its variational form, we derive a nice decoupling property for this particular $f$-divergence when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. The above derivation helps us analyze the robustness of this measure for different $f$-divergence functions. With established robustness, this family of $f$-divergence functions arises as useful metrics for the problem of learning with noisy labels, which do not require the specification of the labels' noise rate. When they are possibly not robust, we propose fixes to make them so. In addition to the analytical results, we present thorough experimental studies.", 'bibtex': '@Article{Wei2020WhenOF,\n author = {Jiaheng Wei and Yang Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {When Optimizing f-divergence is Robust with Label Noise},\n volume = {abs/2011.03687},\n year = {2020}\n}\n', 'references': ['646c28e0bee560f6a37b37289284774e53962ce6', '599ed9357448d8c55e2dc7f4f12224d5c6dd1fcc', '194842d570ec3c18ef641f74cd56eb5a4c669656', '82c77a88969ac0e3a4e55c9a7dc5ced4afee0225', '906c299c9386f70198f44e60ee9a073c42271638', '98115d0f8aa50e9fc5237e5b81cfe1c5cbb8fce8', 'ab7539b938ca8a99b5fb34695e1b86ef0c6f3632', 'fa3bcc1126adc5d4305e3db821949e46810c8f9b', 'eeecea3097cf5629eb72a06e5caaf24d774adce7', '58b4cf057bdd361be289601ef3dd69b4efbef83e', 'de476b346ab2aa3b2b759bc9f273f1e8550409f8', '35216558a821ff1ae77ad2f4571e4d83327da244', '570f3c52e4e9608d65afd00076e784800c286524', '1e1855ca80e8ac3de0e169871f320416902e9ad1', 'e061d23b68e7d4aac5aece4794c044c80e638dca', 'c4c4bc0367ec099f1e00a7700332cd0bf393aa55', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '33d6aa6c41ce3000161d9b5eea910a5b78e14330', '91d331d2bdd5fc86400c40c497bcb4c741c652be', 'ffdcad14d2f6a12f607b59f88da4a939f4821691', '77f0a39b8e02686fd85b01971f8feb7f60971f80', 'd274f7ccf34a9e21bdd6934b2c5dc229d5ec2911', '6d8984600e3cd9ae9d6b803f43f2410fa5c0ad0b', '5d6ae67e569f974360b107060c23cbb8a13b0687', 'dd05ce86cacf50e3aee70d633040241923deb120', '001677dffb9b4ccb3ba49076f7f5cbe3f7a405eb', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '4568d7951f8f4d5604a4a8528ccac4ce40a43706', '9f59d0a003558066d2ff4fc1c77f461b4d233663', 'ad84f49b2cd1b85a6d7df2304144a093f5b610a8', '38861d0d3a0292c1f54153b303b0d791cbba1d50', '1ab5c006caf3bf8c128fdfad80e58277cb8b1455', 'ce9de6c26a4772f2349b29d76c22b5a436f6cbca', 'a035210a6dc001a0c3404f94a5c9ec0e73507cfd', 'b78f419ace9938fad9402361e1aa0dbd288ec004', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '92ed2e34501903d922d74f28a012d6e337418fa4', 'abstract': 'Mixup is a popular data augmentation technique based on taking convex combinations of pairs of examples and their labels. This simple technique has been shown to substantially improve both the robustness and the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.', 'bibtex': '@Article{Zhang2020HowDM,\n author = {Linjun Zhang and Zhun Deng and Kenji Kawaguchi and Amirata Ghorbani and James Y. Zou},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {How Does Mixup Help With Robustness and Generalization?},\n volume = {abs/2010.04819},\n year = {2020}\n}\n', 'references': ['a65cff2a792cd2133d2db91d88327bdac6cbf108', 'be21780b2e2fa8abb2243e91d5af5c7bd49d4079', '264972e27882e53a4671f95b2a4c789e15d12cd7', '404c8ec7d40d58b8ea6bc634262101486cb74300', '0d37c762336ce69801c7fda5eb140d716ece0859', '43f2ad297941db230c089ba353efc3f281ab678c', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', '6d15683422ffa9c044c2a90f45ea0ff845de83d9', '7b2e26fd3e862c5f58c522a13ae86a768aaaccad', 'ed17929e66da7f8fbc3666bf5eb613d302ddde0c', 'c42816f497d663c681df20d48a6e66a5632600d8', 'eeecea3097cf5629eb72a06e5caaf24d774adce7', '743ce0d2c9d86caa5a570fec36e8d2378d031b3f', 'f1aa40ba7e3166744955ceae2e6d8d60515e7021', '43054544c4ff2e25513de8b1a655593b8ff89338', 'd3eef9744324abc397c82b112b026aad3ec56708', '714e3e81ce270518e20d56c56967475eaffedee3', '1b59eea8ec4684381a885b59acd09c9151a49487', '1b9c6022598085dd892f360122c0fa4c630b3f18', '06ee9741730e4a2c7c8cdf643f5f34bc497a0b7c', 'a9022d8ffb5e417458fba9a280f90c1b08cb6c73', '4feef0fd284feb1233399b400eb897f59ec92755', '430de87a0a8996bc93b1998f9a6261f7558a5679', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '9753967a3af8e1db1e2da52a9bb3255bd1ce5c51', '54ddb00fa691728944fd8becea90a373d21597cf', '1c4e9156ca07705531e45960b7a919dc473abb51', '77f0a39b8e02686fd85b01971f8feb7f60971f80', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '0f7c85357c366b314b5b55c400869a62fd23372c', 'bb8043fc95475ae862c13fba290d5b737b36bd3b', '02480b5d060eb4cb2228ac7e824fda22b29c3e9e', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '178b2407fb2165a0253862b52f287e333f79dbc1', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'd124a098cdc6f99b9a152fcf8afa9327dac583be', 'ec92efde21707ddf4b81f301cd58e2051c1a2443', '4a18360a14facea50dc819145b1daf4c53d5d59e', '009f35c0e453f2435efd8d8ef8086b76b294967a', 'e4fd07d80407bd974c1c550962a7af20d42d49a0', '53f2bf254c530c4412a8892896422511bc2cee45', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '364fb0677a5d7083e56c0e38629a78cb94836f53', '02227c94dd41fe0b439e050d377b0beb5d427cda', 'cb067015b18da10c48668e08989c2fe2cf6e2d11', '5d90f06bb70a0a3dced62413346235c02b1aa086', '8213dbed4db44e113af3ed17d6dad57471a0c048']}
{'paperID': 'fc1ad6fbf705bb2e45f0e832fb10325a868d87f9', 'abstract': 'In tasks like node classification, image segmentation, and named-entity recognition we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respectively. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first collective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural Networks and leverage their locality property - perturbations only affect the predictions in a close neighborhood - to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from $7$ to $351$.', 'bibtex': '@Article{Schuchardt2023CollectiveRC,\n author = {Jan Schuchardt and Aleksandar Bojchevski and Johannes Gasteiger and Stephan Günnemann},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks},\n volume = {abs/2302.02829},\n year = {2023}\n}\n', 'references': ['511aba67a50408ad52f1bc502985cf3e64345869', 'a47424106165acd212b3233af8eb5a26cc567b4b', '49f780951539665dea602c5ae4528fc67e656404', '9bf050287e0cd8df167cc345878335fc5a8d045e', '60cb22635e8d05a986fa6de2fc7090a9451e2de3', '544f62fbe3d43d31edc7ac710ed83dee5c279b82', '07c67c090ee27676ef91d288d897f4b2352b42e3', 'ad331b2035602c58221e9092920de0fd6ed2f629', 'a71e40e0895f9a3d1e85e0bf9c1b032855446ede', 'e8c8e6621a5646546f18c48303afc86cbcf8c0c1', '409c309573b0e5f0cf7bad0ed779acf95613fe5a', '3aab8bea2ba6bd7f076e6f92a504a1e322ca64b8', '458757a205dc73e17683458d1c432e9bbff42e5c', '6ad5f1d88534715051c6aba7436d60bdf65337e8', 'f5252075bb34666863cd01cc82c2d941d4ffe6c6', '1bfad6fd818bd64db381791efd9252e0313dc100', '50c5763d2d35f2c4eaa5cebea310faf2cf0a10dc', '0349593412dbafbcec736da8c2547e94fa702607', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'ac225094aab9e7b629bc5b3343e026dea0200c70', '6c44f8e62d824bcda4f291c679a5518bbd4225f6', 'b514949ad8344071c0f342f182390d2d88bcc26d', '33998aff64ce51df8dee45989cdca4b6b1329ec4', '2b76b6e766547b3c6dbc2785a084ec3b72cb760d', '99cb08c76c120599abd1d1637e32aaf577f38d39', 'e24cdf73b3e7e590c2fe5ecac9ae8aa983801367', '36eff562f65125511b5dfab68ce7f7a943c27478', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '43d2ed5c3c55c1100450cd74dc1031afa24d37b2', '04f4085c0126ba29453a582cd1e62e05c8e15c82', '8446830f3c05b97c4d12a0751c022d1ae6a5115b', 'efac04450c531b3769451a886ed9a42fce4754d9', 'df24c3011fc42b72195e876ce052a0a072a1d923', 'b6b26564df790262abbe48fa18079d9610189b29']}
{'paperID': '10daddaa1e1ed27e88b08b1c124d800de865c5e3', 'abstract': 'Despite the fast development of differentiable architecture search (DARTS), it suffers from a standing instability issue regarding searching performance, which extremely limits its application. Existing robustifying methods draw clues from the outcome instead of finding out the causing factor. Various indicators such as Hessian eigenvalues are proposed as a signal of performance collapse, and the searching should be stopped once an indicator reaches a preset threshold. However, these methods tend to easily reject good architectures if thresholds are inappropriately set, let alone the searching is intrinsically noisy. In this paper, we undertake a more subtle and direct approach to resolve the collapse. We first demonstrate that skip connections with a learnable architectural coefficient can easily recover from a disadvantageous state and become dominant. We conjecture that skip connections profit too much from this privilege, hence causing the collapse for the derived model. Therefore, we propose to factor out this benefit with an auxiliary skip connection, ensuring a fairer competition for all operations. Extensive experiments on various datasets verify that our approach can substantially improve the robustness of DARTS.', 'bibtex': '@Article{Chu2020DARTSRS,\n author = {Xiangxiang Chu and Xiaoxing Wang and Bo Zhang and Shun Lu and Xiaolin Wei and Junchi Yan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DARTS-: Robustly Stepping out of Performance Collapse Without Indicators},\n volume = {abs/2009.01027},\n year = {2020}\n}\n', 'references': ['5a0859bd66ec5b70f3230bdf2181fde1e641bfde', '38b64492ac1b5d6c8166c7952073e760bfb8f46a', '69599593f93023e2f91ef6673ee9860f85777d98', '645a24296f96f325f4a6fd324cef85661a8987da', 'f5f35340893d550bd5d1a2711f04308525c6dcd2', '40d076d07c36c94ee43bbe0c2e66f4e4cc92d039', '52fa3eb17723571bb7127db42fed9e78cfa4c00f', 'a4cc0701170331a1fd0e58bad962bd7f39f5efc9', '7ffac30cd47fe173bec897d2b8b81c93e2771b85', '7f4b0db1111d95d7135ea8135f3530a1d9357ae2', '3242bf8767179c13c7322ccfdbe18c66c1e25a99', '237a230fa4bc28085d470e7e990938ad32be2c37', 'a2401eea772949f79aba080a6df7bbd91d93346e', 'e333036efb9f3a1cd402d81d4491931563945c66', 'fb564bacfa790d44ab02a72256d55aa8b2209914', '4ebce2425e231031f89a4a68dc52a151cd735d03', 'a7ce95c6f674b7b5b19a532491d160d142f8b2d6', 'c2c083df88e88223e1a411e61040b94c233b1b63', 'fe8907302f9d14233cd03cc2948a1c4e2a50bdb6', '4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9', '5e19eba1e6644f7c83f607383d256deea71f87ae', '727c7abef2dc19d220e9e9417cd41852a05d19fb', 'b5375995ab8d679a581ffcc2f2e8d3777d60324b', '18ca023bbb1a24873140b5440479e74c7f90b684', '6eb3a62cd365e4f9792eedca43c90595e1a862ba', '16c844fd4d97f3c6eb38b0d6527c87d184efedc3', '191b88c374c8fc4367238c3f4df2af250c32a4ef', 'f4838839719cf96951ade45a221700341f57c4d7', '45532bffbfbb5553da0b2d0844e95a1b37e59147', '3f0a2de309f21a957b4741dd68007eb08d9b12e3', 'f323407464c4cd492d3fc1afd7170eab08f44d9b', '7a71941e60894ae7e1f5af8e79c37cec6cd6c6ad', '693c97ecedb0a84539b7162c95e89fa3cd84ca73', 'c1f457e31b611da727f9aef76c283a18157dfa83', 'f723eb3e7159f07b97464c8d947d15e78612abe4', 'fe9b8aac9fa3bfd9724db5a881a578e471e612d7', '50bdda28de3dcf82a0e10f9ec13eea248b19edb5', 'dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4', '6baca6351dc55baac44f0416e74a7e0ba2bfd03e', '5f79398057bf0bbda9ff50067bc1f2950c2a2266', 'fb37561499573109fc2cebb6a7b08f44917267dd', '79cfb51a51fc093f66aac8e858afe2e14d4a1f20', 'd0611891b9e8a7c5731146097b6f201578f47b2f', 'a55970013b984f344dfbbbba677d89dce0ba5f81', 'b236322720184c558bfa2a43416aed7701526ec0', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '424561d8585ff8ebce7d5d07de8dbf7aae5e7270']}
{'paperID': '1a627d2a169d71563109546da590a7cceb0b349a', 'abstract': 'We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.', 'bibtex': '@Article{Zhang2021RobustRL,\n author = {Huan Zhang and Hongge Chen and D. Boning and Cho-Jui Hsieh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Reinforcement Learning on State Observations with Learned Optimal Adversary},\n volume = {abs/2101.08452},\n year = {2021}\n}\n', 'references': ['634ecab404ce2b78564ae992e6dbfa4929b810fd', '87f2e2031b479fbaf0c5ec853118a82df47a0241', 'd415b724fbc35afcc8dd91738123edfa6a5db634', '1764924b9c892ad85c677b95677c344d7ce99143', '3a7d95aed866d68e189db6f4eab29f46f68c5ffc', 'c54b90aae50cf06cea8ffe912d2424a4e8b82e1a', '20a53578f84be351bd90385fcd674821e1ace17d', '9e5e1944d4b227a55127264754f17b8437d2fa7f', 'b224d0f575237feb681717b7e74157dc0bd500df', '1a20a92125904772261fdf646881121059932ce6', '14a74243ce18d36c4ee7460f94b7e1d2b45b8b34', '6946ae0c23257586c12d01675e05167d74cb89fa', 'ff22e140a0423f1cf0595d213f36402668084014', '30cb55a404cd1b470aca6c533a786af606abe593', '6ff50528f3d7c72772f8c0e3f8398f9dd8e06575', '5473175211aff4a8a099c44d1a57802d1b7ecf9e', 'bcdb21ca1703fc6f62df420626e36d138480a6a1', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', '9476b41d04c1a861523eebb6fda5499d03e0aabb', 'fbcd128be3d539f52640ffa58f05a6b61f0749aa', '12e21b59a13cc4111255d926830a4cf98be57e1e', '75339d34bdac0d21a41461228ec6088eecdf857a', 'e37d2cafe99b198640ba4c1ea42fc718892ee268', '4debb99c0c63bfaa97dd433bc2828e4dac81c48b', '4e43d0365e4e922123de54c5e9a430bbced4a817', '4b47531e2cf3ad58b14da00bb665e359e3bc2600', '3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3', '4b23012689e0f17912fb38d4984775e567cff8d6', 'dcd4bf5124c4813388dca713df2a6e8c568c0a9d', '2a5734ea4cb938c437e5a439f3d439877029735d', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '8db9df2eadea654f128c1887722c677c708e8a47', '9c4082bfbd46b781e70657f14895306c57c842e3', '236b40f3144b95cd84779484c8269092122920aa', 'cf8ed2793bc6aec88da5306fe2de560dc0be9b15', 'c8c16a56d2a9520197da9a1546f517db5f19b204', '1d65848c563b2c3a7f0153551c1b39e0e5c2d776', '846aedd869a00c09b40f1f1f35673cb22bc87490', '024006d4c2a89f7acacc6e4438d156525b60a98f', 'f5f323e62acb75f785e00b4c90ace16f1690076f', 'e994a15a9476ca92c4bf9c12ad89625401939d57', 'd450b0f12ae0437048e4047a630c31d902002d0c', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', '449532187c94af3dd3aa55e16d2c50f7854d2199', 'b6cc21b30912bdaecd9f178d700a4c545b1d0838', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '27ebe193a60186f888525a23376344254363e39b', '3761b43d38e826bd9c841c40ba15256fd3627215', '6cfc68137a1233f9bcb16a319fcbd02b5f3cf4fd', '6c485f3d910d46c88f00cdaa9883cc7c43805fb5', '92d009217b100882376ae5c90217da2e92471ad7', 'ab4a1c4dfe23b3a1e3d077df467452cc68f64de8', '9ea46f06b614edb9c60771e0829ef1558048a21f', 'd6f630cf3eac618bd075ec7ce37f60f60af237da', '7fbf55baccbc5fdc7ded1ba18330605909aef5e5', '8090121ad488b4af27bc59bf91b62e9c6a6f49c6', 'f5d4374d4a375afbee3979c0bd3eec60606cadbd', 'd8febb0df3d6caa480ac45ba9fd3d1d64606fd89', '59d59e19d3715128f609f65023517a5c61d06fde', '810b9ffea4c74db3923336a22dc9563679cfe564']}
{'paperID': '7ab3551d85efbc3c844dcf5714f2a5d3e8bafcf0', 'abstract': 'Variational autoencoders (VAEs) have recently been shown to be vulnerable to adversarial attacks, wherein they are fooled into reconstructing a chosen target image. However, how to defend against such attacks remains an open problem. We make significant advances in addressing this issue by introducing methods for producing adversarially robust VAEs. Namely, we first demonstrate that methods used to obtain disentangled latent representations produce VAEs that are more robust to these attacks. However, this robustness comes at the cost of reducing the quality of the reconstructions. We, therefore, introduce a new hierarchical VAE, the $\\textit{Seatbelt-VAE}$, which can produce high-fidelity autoencoders that are also adversarially robust. We confirm the capabilities of the Seatbelt-VAE on several different datasets and with current state-of-the-art VAE adversarial attacks.', 'bibtex': "@Article{Willetts2019ImprovingVR,\n author = {M. Willetts and A. Camuto and Tom Rainforth and Stephen J. Roberts and Chris C. Holmes},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Machine Learning},\n title = {Improving VAEs' Robustness to Adversarial Attack},\n year = {2019}\n}\n", 'references': ['fbc68afa5ca151a0933961ec86dbf6f330903b86', '2604797ff947ba554980344a81fa91d5323abdf7', 'd4463fe262306eaac336fa5cae38e98811bffa80', 'c2ed34facd63d72e5d03ba13a6a3956ed6b2ac6c', '9e4c467d5bf3cc752f21be0b67e47f75dfb5a4ec', 'd90c771bb565db9dc027970d50e1d47096174253', '9c5c794094fbf5da8c48df5c3242615dc0b1d245', '18063ed998c99bfef92fad8418610b97f863d878', '4563cbfbdba1779fc598081071ae40be021cb81d', 'c5285323f3b4dd5f099dbabd73d86b9e0c13f04f', 'fd7789de401811fd8692466b8d49230e7184655f', '287547fc81364e64d196abb8d891ade3f6599a5a', 'a71f1480abe044ae90494a23f994dfb5b40e6f8c', 'ff332c21562c87cab5891d495b7d0956f2d9228b', '04541599accc47d8174f63345ce9c987ef21685b', 'f53936c03fb089cc159c551081124aae8a32ec1a', 'b514949ad8344071c0f342f182390d2d88bcc26d', '8976e91ccae57a20c29f3c9d88bf45b19973c952', 'eef70764d07cb4a24986a9609e968bc1aad84df5', 'a2141a5ec0c65ea0a9861ae562f4c9fb8020d197', '6b2db002cbc5312e4796de4d4b14573df2c01648', '097889e0b93591d75de08f9da661ff882a1532f6', '222928303a72d1389b0add8032a31abccbba41b3', '977560251c2bd4c28a6c7c707c29f4091c5e6247', '657fca895e9217a0306739f8f58332c224b8a82e', '8e5d0c73eb29e3da8d6d3a0c8560b23680122bb2', '90f8962553280470ed0d12ebb543e89c84eb137a', '4db188284236d36a77cc0e1f69e5973eddac864a', 'a90226c41b79f8b06007609f39f82757073641e2', '6a97d2668187965743d1b825b306defccbabbb4c', '64d698ecd01eab99e81e586400e86d3d70b9cba7', 'c8c04ed972d38e2326a53d322a6f2d7e0f8218c1', '3e47c4c2dd98c49b7771c7228812d5fd9eee56a3', '687e80eb70c7bbad6001006d9269b202650a3354', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '6def29d024457f8897c3a634fef8a03dcaedc9a0', '484ad17c926292fbe0d5211540832a8c8a8e958b', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', '184ac0766262312ba76bbdece4e7ffad0aa8180b', '639937b3a1b8bded3f7e9a40e85bd3770016cf3c', '1d7d0e8c4791700defd4b0df82a26b50055346e0', '7e459946cb320935ee97eb9ffa23136524866257', 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c', 'bc4fca38a3b29e5d3d3d08c4836a7e637e2ce7e8']}
{'paperID': '857b1c3f171afb3cdf9df23d23e5d0cdfaa83efb', 'abstract': 'Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL.', 'bibtex': '@Article{Wong2020LearningPS,\n author = {Eric Wong and J. Z. Kolter},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning perturbation sets for robust machine learning},\n volume = {abs/2007.08450},\n year = {2020}\n}\n', 'references': ['2dd907736e272888e16b7646435e6444f0d0ff73', 'd036d7e51b975c6023a5eee9aa48008b8fb62d20', 'b1ef79a7bbe51a7f1f28e403c8bde35a3b54d985', '2eda2921a8da4b325f9d05f556594a5884c398a7', '75170439ccfe2271367e4ed7298f360b0443fde2', '86974ebbf9f7d5797e8ea919a347c3dbbe9a1f17', '78dabf7a32f9b76da8212a101482096197b437cd', 'e39ed8f737b439b827d39f9be3f463859194394b', '02b1607af35b48f0bd716367caf6a7428b969369', '193092aef465bec868d1089ccfcac0279b914bda', 'be8d616b2fea58aa818ba4f52d894e14937af945', '604dc3c7ad3736e58d7fd8a5839f8d8ba63e63b6', 'f4a3d6d26347df2c5c89ce0848ef72f1d4f80e3e', '6c894670faadc7dc24228a86925f828edbe8085a', '4690190d6c110f7525f7250e1acf4a4eab42519f', 'f3b76f7a1042972009c37302ea00daa30238934b', '71e3347dde362ba369b8103c8850dd07e6c23424', 'ff22e140a0423f1cf0595d213f36402668084014', '21de3a36cb51adc205fad8a1d3d69118891dc3dd', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', 'fb38fc75f58cf8e171d59b868b1afbddbb9a28eb', 'df26b25512f5e8ff8f2242f9b0aa40e7572f2f43', 'e808cac4b64a8c73ada719f76ad885454c71a74c', 'daf8cd0f2c159d022477914bfacee9ff6da70c8b', 'd66c7ec5cdbf4df77789748d9173e2c4775933f0', '49b64383fe36268410c430352637ed23b16820c5', '17a5cc88f3c18bd68e78755abb6285d1b59a8109', 'ded6d20b751a7b0a8b94175c12b64a40904d80d0', '7c0a73771778fa8362c3e3abe7734dc9b1de3c76', 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '43a4a354b67ab6d5531355a368094815d2d2593d', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', 'f0c5991dbb130fa6b5de011cf7a04f6ed815ef68', '20f85256555ad612148e52f9363e52f9d661728b', '5023544ad6fa49b35526a62f22207e43c4db870d', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', '966e3c7a65ec75a6359b55c0cecaf3896d318432', 'd3c071dbbb4520ed5875f7e064a9da87240534db', 'd5577abcc1fbf57d66017e3b5b2211a82022842c', '7b874e6fc3e3db83b350824f372761a415f5725f', '0314e777333a63aca5735ea136c74e113aa8801d', 'afa0d49c1399c752d6f4665d75ecec640c000468', '4b23012689e0f17912fb38d4984775e567cff8d6', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '4feef0fd284feb1233399b400eb897f59ec92755', 'eb35fdc11a325f21a8ce0ca65058f7480a2fc91f', '8dce99e33c6fceb3e79023f5894fdbe733c91e92', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'e2a85a6766b982ff7c8980e57ca6342d22493827', '2cd55ded95d5d13430edfa223ba591b514ebe8a5', '1c4e9156ca07705531e45960b7a919dc473abb51', '77f0a39b8e02686fd85b01971f8feb7f60971f80', '3f25e17eb717e5894e0404ea634451332f85d287', 'dbb6ded623159c867fbeca0772db7b2eb9489523', '37b5dfe87d82ba8f310155165d5bf841dc92dea2', '6364fdaa0a0eccd823a779fcdd489173f938e91a', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', 'b9ad22650772ff791e687cf67b73b00b9a22203d', 'fac8d2bd19f3a82f92a344d758007cff499b7725']}
{'paperID': '8246f0d3799290be5ed47254f6b88b601fa98230', 'abstract': 'The label shift problem refers to the supervised learning setting where the train and test label distributions do not match. Existing work addressing label shift usually assumes access to an \\emph{unlabelled} test sample. This sample may be used to estimate the test label distribution, and to then train a suitably re-weighted classifier. While approaches using this idea have proven effective, their scope is limited as it is not always feasible to access the target domain; further, they require repeated retraining if the model is to be deployed in \\emph{multiple} test environments. Can one instead learn a \\emph{single} classifier that is robust to arbitrary label shifts from a broad family? In this paper, we answer this question by proposing a model that minimises an objective based on distributionally robust optimisation (DRO). We then design and analyse a gradient descent-proximal mirror ascent algorithm tailored for large-scale problems to optimise the proposed objective. %, and establish its convergence. Finally, through experiments on CIFAR-100 and ImageNet, we show that our technique can significantly improve performance over a number of baselines in settings where label shift is present.', 'bibtex': '@Article{Zhang2020CopingWL,\n author = {J. Zhang and A. Menon and Andreas Veit and Srinadh Bhojanapalli and Sanjiv Kumar and S. Sra},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Coping with Label Shift via Distributionally Robust Optimisation},\n volume = {abs/2010.12230},\n year = {2020}\n}\n', 'references': ['66488a38c3bae5d928bb22aa615fd0e64ccac62b', '8352a4ec9a0cf24b7556945743c418f84f9ed9fb', '994a4f9a75ca10da4f9a0f4a3a03fd622aeeea41', '8747344b26da07f5ab398b16ed85375271c7019f', '31c53acd2a43dcec4342d9c42d0ffbfbef36e855', 'b37a3f6e3c6e0bdfde6c5531bdf90d16df3f8d5c', '7c63f4cf082c162625c2db4234c215f07ea58dfd', '193092aef465bec868d1089ccfcac0279b914bda', '0db252455fe10261c3c439e850dabccb72097c20', 'dcc4c760c3f1cb17f953c487190b735030c33b78', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', '753b7a701adc1b6072378bd048cfa8567885d9c7', '5a6a33dddee357ffda1d9a3a948f4019b6a68dfd', 'bcfba69c2fadf2efea83be12fda2601f8d4681af', '1dcc2da3fde52cacdec926d5c4e2bb425959721b', 'afd9786a6fb9c79fc6d41bacbe73607e73044950', '6d64363e52cd7ecad99d7ce6ae849f245dfbbf92', '159395b0f7a2b9ea04f9a758d18887bcb970ee78', 'd72d4e3da3788c7882d367737902ab0fd962bb89', '54036f43acc6c9b49b334270c7237217685f52fb', 'c98bcc8689c34d72fd0b696f2a49e7f86d151782', 'b661520bf0061b7d96ccf12016e351dd3a6ee780', 'a1fb7236d104ae0343c1a09e3590ee2283483240', '1d3de4dad3c3762e9ecd9fc60281a5896b8b9616', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', '4e280ac44bba6310648638eb76312a81182c70f2', '80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332', '0bdb738ea0f49b045a3232c61d3cc5734bcf1e93', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '2cf5ac39c8299a3304c6e9808593153c894ff1b5', '07b5093aace8e485e7d23b83edb6351618138127', '5fb808edfaae574ee7f4553ad7090ca1fed9b585', '6e77765dd3250fc671c413b44554087bad43ad92', '940b95df43f76005297e7be2590b531ff35b8f26', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', '66c3d69f94c90a884d3f6b5367813d51708f6ded', '79c286bf03ed97fb94d33511f3355770dcee0aec', '89024c395ce0d7e72f5b6d0e09fb4a6dc5adf209', 'fd802f44c645a81d1c960fb7e1d126edeb832abd', '8e9f5c99f8c006e78eb9e515ec9c618cc34f2794', '57c0b0fa5744a4f82514d76f111e3599f939eef4', 'c88b6caeca4c62525c032f2c138a860a5a17833a', '96c6bc559b79d8fd518f431c707e8b44ce3bc4de', '4e0c56f084a53f5c8fd7e2fb615a355f0431ee77', '7fed3e00be2bb09510f5f7cad7ac106e6c94a359', 'c991ae03942ac48f1da16aa1f034aa9fb91a2aef', '6a7364f6ed2846ea2b705336a4c49dd287102a50', 'a295f76c2afb7f79a970ccf086f16168d976bb93', 'e4350e816a350662ddb5f9ef92437aa8f3fd44f6', '99f65a048860d3f2bc500a2886c461d92188d2ca', '810b9ffea4c74db3923336a22dc9563679cfe564']}
{'paperID': '9835d0d85faa36d2c27bea806487b988935e92a2', 'abstract': None, 'bibtex': '@Article{Zhang2021LearningRS,\n author = {Amy Zhang and Shagun Sodhani and Khimya Khetarpal and Joelle Pineau},\n booktitle = {International Conference on Learning Representations},\n title = {Learning Robust State Abstractions for Hidden-Parameter Block MDPs},\n year = {2021}\n}\n', 'references': []}
{'paperID': 'b27ad18e20d27efe8a9fbc54b1c2dcef8b2da19f', 'abstract': 'While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. Hence, our goal is to train models in such a way that improves their robustness to these perturbations. We are motivated by the approximately shape-preserving property of randomized convolutions, which is due to distance preservation under random linear transforms. Intuitively, randomized convolutions create an infinite number of new domains with similar object shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. Especially for the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation.', 'bibtex': '@Article{Xu2020RobustAG,\n author = {Zhenlin Xu and Deyi Liu and Junlin Yang and M. Niethammer},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust and Generalizable Visual Representation Learning via Random Convolutions},\n volume = {abs/2007.13003},\n year = {2020}\n}\n', 'references': ['17293cd36ee5e7ec37dcec1d5ab85f9b77ad65d5', '022622e024890d6e044ac50e2da6b44c59bdf418', '89b95fb53727ee45be440045efef656718517c4c', 'd5c5bdbb1d0e137ba5316455627c7844ff685646', '1aab4548a7d55cf8a3562f8710b50ed20ad32546', '6cd205132c786e3bc03964fb63380cd3cf51b9c7', '23d7d2aae6308a840a597e823ae8214278304c5a', '6e4e75c88a0801c87f47a171aa69a9914f9129bf', 'edb293b7e75a9fae5dae80dc76af90e46da27a65', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', 'c42816f497d663c681df20d48a6e66a5632600d8', '4ae0c4a511697e960c477ea3e37b3e11bf3e0e02', 'f8d1cae82cdd4c6f4dcfefe5b2d1c0f0e68742fc', '7ce7941ffe5220383d4e614ebcf2397e30cb27a7', '7a8f8109e65ed9a6048859681a825eb5655e5dd2', '3217278e346fefbd34f0727321059c7ea5792612', '3dd8bf5cca76b1690a2642b73b509fb3a27e4f36', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', '96b32b204a62777bef66eea595de2c47b4e9d6e9', '4cb3fd057949624aa4f0bbe7a6dcc8777ff04758', '89ff54a10869113aa1d5c6754ac4928e64b54292', 'a60540a8407fd117fd8e6857d4728e661f53dcc8', 'f802802b3af5a22b79ac65d033ba3cbee33da91b', 'ba6ba7f488c1ece0803f4b9e1c83a3196d061610', '8a8cfa45b4c0d071fbffa091c02670b19c94b693', 'cc01e553052a502c9f7697e90296eb9bbf7f32df', 'be4a4f7f65d397a4e07dc83b95da6b414e0634e2', '4feef0fd284feb1233399b400eb897f59ec92755', 'b39b45a59c27a0cb3214d5a84547f54722d40c69', 'b1e7f07965a53491690bd31fdab626bfac606eae', '32ceb28e45a445df4d89df281bb0e3ab5aab1a2a', '1ea3cb713365e8cde0678875b8bdd787f83b7c42', 'bb3c4327f0cbac19d9f87b4c989aee8833cdcd40', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '8a29808865a0daf55e80a626a555c21f31c479f4', '1d5972b32a9b5a455a6eef389de5b7fca25771ad', 'd6f2f611da110b5b5061731be3fc4c7f45d8ee23', '2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1', 'dac7344737cb824634f757aede2dd46a6eed204b', '265069b3670930fd884b02062d7e7b79ff2a49d5', '5562a56da3a96dae82add7de705e2bd841eb00fc', '34f25a8704614163c4095b3ee2fc969b60de4698', '236db916e2c73eccfe8821110274affcc9b54360', '02227c94dd41fe0b439e050d377b0beb5d427cda', '162d958ff885f1462aeda91cd72582323fd6a1f4', '30a7fcdaa836837d87a8e4702ed015cd66e6ad03', '1d0635cda34b8af995313848a0c42bac6efe79ec']}
{'paperID': '2dc7741c3cd3c7fc0d0ae7b60cf7358f612e175b', 'abstract': 'Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust fine-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks.', 'bibtex': '@Article{Wang2020InfoBERTIR,\n author = {Boxin Wang and Shuohang Wang and Yu Cheng and Zhe Gan and R. Jia and Bo Li and Jingjing Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective},\n volume = {abs/2010.02329},\n year = {2020}\n}\n', 'references': ['19803adec3b97fb2e3c8097f17bf33fabf311795', '9b9a6d6a698cce777929ecc65c9fc5d09b2232ac', '41382835ae60fb3280ea9a5b3004a236af1eb01b', '2f5f81bc516a6d085d39479378af1fc27104f91e', '0a9c0e729dd95f5559e05f8bb4b7408f9409388e', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', '7f768fa192a76ab097ccfda0a68523bc36425423', '2ffcf8352223c95ae8cef4daaec995525ecc926b', '1fcbd3ff32ed9b909300802d77f890f84ff9b3d8', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', 'ab70853cd5912c470f6ff95e95481980f0a2a41b', '207da6d2c07289bf72a2b5974bb3f011ebb5dd0d', '309b906fed883e5efe4acf676c655ead21f6c17b', 'b85d339e49399966d629973c889e8edfca56517c', 'd01fa0311e8e15b8b874b376123530c815f52852', '07398e448180ad75c44d30f23a65289d40ff6f52', '4690190d6c110f7525f7250e1acf4a4eab42519f', 'ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', '1adfa30bf112de20cb959014e44626d760aa8e4e', 'fc09d6486be1c9bbfbef4165ce3c1ab664e5d084', '403227333329b36183004f04db72362b604adef3', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'f91175950edf3804ff1573f570b03db9b108dece', '67b72e427187b1113c787f9265926322e3d123e8', 'af3825437b627db1a99f946f7aa773ba8b03befd', 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', 'f0c5991dbb130fa6b5de011cf7a04f6ed815ef68', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', '2b110fce160468eb179b6c43ea27e098757a56dd', '6b73775f40467aed52784ff355b9bb7168e9078c', '514e7fb769950dbe96eb519c88ca17e04dc829f6', 'd295a620fc10a7a656dc693e1b1bf668d1508a8e', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '5ded2b8c64491b4a67f6d39ce473d4b9347a672e', '05dd7254b632376973f3a1b4d39485da17814df5', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', 'f04df4e20a18358ea2f689b4c129781628ef7fc1', '415229903f91a1f3fc7404f5e5997fde025c221d', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'aae4efb3d412d585ea0dec03f933397c93caf989', '16d70e8af45ca0ae2c1bb73f3be6628518d40b8f', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992']}
{'paperID': 'bbf64d3561c3dd89c254483bf10facdfc322907f', 'abstract': 'Principal component analysis is a simple yet useful dimensionality reduction technique in modern machine learning pipelines. In consequential domains such as college admission, healthcare and credit approval, it is imperative to take into account emerging criteria such as the fairness and the robustness of the learned projection. In this paper, we propose a distributionally robust optimization problem for principal component analysis which internalizes a fairness criterion in the objective function. The learned projection thus balances the trade-off between the total reconstruction error and the reconstruction error gap between subgroups, taken in the min-max sense over all distributions in a moment-based ambiguity set. The resulting optimization problem over the Stiefel manifold can be efficiently solved by a Riemannian subgradient descent algorithm with a sub-linear convergence rate. Our experimental results on real-world datasets show the merits of our proposed method over state-of-the-art baselines.', 'bibtex': '@Article{Vu2022DistributionallyRF,\n author = {Hieu Vu and Toan Tran and Man-Chung Yue and Viet Anh Nguyen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Distributionally Robust Fair Principal Components via Geodesic Descents},\n volume = {abs/2202.03071},\n year = {2022}\n}\n', 'references': ['3eb495010ec95fbd6e1e15b19400b113d5955bb8', '6a7f6b17e3bc15ddecd10581950a8ff8430ca401', 'a5590fb4f90ebd1d83b6f2d0df501cf66c0c6259', '09b3b1e8d0d920644c2466adf3b3fd5d0333fa7e', '18c72c585e3ba48f99f46e9485d75c822aebb189', '62a3b6364ad24b8d88b260e57052a55847dffbb8', '6be7c23245e41f11ff59115ea021ea0656f51591', 'b97fe7bbe8a5be93123be406f5d0701c08e6441b', '179bd624cda94ae8ac99dc9081d2c31e8a095767', 'f57d41efd502d3d9e8ee8384e7b815ac8c54104f', '0090023afc66cd2741568599057f4e82b566137c', '4a0d35989d91b3b7d2318802b1de6d10e4e6e830', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', '0a08cdd184ee52b579aa82a70aee0168fa67ea88', '1f868b5839b3126209612f6e2f8c40aa431b46fd', '916025e6d95cb5c6d09765ad4c2894d16a5b914d', '422513942a2a564e3fe3b6c3b7df20d6190f0d20', '869fdb53a40290a3941fd6ab808835e9b5184d62', 'a78f9467070992fc8742641ec97f9972597d869a', 'c657b3fc93a24349117bf87296ea2b9b780706cc', '755475cf0a1101adcca81d2d0424916210565e48', '0bdb738ea0f49b045a3232c61d3cc5734bcf1e93', 'c7330852a07170cd0e6990f5fbde5fca12b6ccd6', 'b514949ad8344071c0f342f182390d2d88bcc26d', '932404745d960291925b3f27b71734dff5b23633', '4eef0519f75911a2e132fac12427fa13bdb32a71', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '9eacd7d43c95be4c4771bf1a324e200918e6c0cd', '043f084e379a44608c470059c2aa174a323e9774', '37f5d47019f467c74acff22a38ffd4b98bdcb5d4', '6e77765dd3250fc671c413b44554087bad43ad92', 'd42b11ce90c9c69a20ed015b73dc33e0e4100a7b', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'f701b58e41d928cdcd8d733b638fd65a73623b72', '948dfbb55a93aa9585056a4b4dd3cd6553b236a9', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '0fee3b6c72f7676b4934651e517d0a328048c600', '37c3303d173c055592ef923235837e1cbc6bd986', '3ac3c11bf6cd8ccc657eb629148d6e346e52c8e0', '33cb597dde84766b1cdd6f35c964d83c9a718b39', 'adaa0523a5c9d5f92aa2009a51226391d8e62380', '28cfed594544215673db802dce79b8c12d3ab5ab', 'cac33f91e59f0a137b46176d74cee55c7010c3f8', '23a6bf80d090a3924bae8f0c80a22967a5987589', '1f9af2cb595bb283bbd5076e96128a612809e233', '02d7fe63bbd2444ea001ff6f10671b2cf8c5cdd4', 'f7b4c27abb76dff4c017849049541f3fc91e77be', '0e0c4a16e11b843f2b5d841ddfb60e97a376d5af', '9ebb5c0d6d54707a4d6181a693b6f755ec8a45a9', '8d56d4bc69a8c562434b9a129542bb79e9d6f1d6']}
{'paperID': '70864c48e643e852355f4a79e23baf3614740df6', 'abstract': 'End-to-end (geometric) deep learning has seen first successes in approximating the solution of combinatorial optimization problems. However, generating data in the realm of NP-hard/-complete tasks brings practical and theoretical challenges, resulting in evaluation protocols that are too optimistic. Specifically, most datasets only capture a simpler subproblem and likely suffer from spurious features. We investigate these effects by studying adversarial robustness - a local generalization property - to reveal hard, model-specific instances and spurious features. For this purpose, we derive perturbation models for SAT and TSP. Unlike in other applications, where perturbation models are designed around subjective notions of imperceptibility, our perturbation models are efficient and sound, allowing us to determine the true label of perturbed samples without a solver. Surprisingly, with such perturbations, a sufficiently expressive neural solver does not suffer from the limitations of the accuracy-robustness trade-off common in supervised learning. Although such robust solvers exist, we show empirically that the assessed neural solvers do not generalize well w.r.t. small perturbations of the problem instance.', 'bibtex': '@Article{Geisler2021GeneralizationON,\n author = {Simon Geisler and Johanna Sommer and Jan Schuchardt and Aleksandar Bojchevski and Stephan Gunnemann},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness},\n volume = {abs/2110.10942},\n year = {2021}\n}\n', 'references': ['3328a42bdc552fbfba5dbd5b6c16b8aff26fea18', 'a14aeb408d1d928c5da08bde8ba7f74915835a45', 'de7634ec3412712d216f01c98c75372839631825', '5673d574d742168f154d214d231e3adee7bc4715', 'c2929349db20144b2a0332477699e5a2f26dc91b', '3fccfd63590564038a4f7235b7ce5c4eddb9b33c', '9d7846b1c38f280370c7b841859a1d416ed07e6a', '89139a394ccdc8784ddbdf7d76d56e3d242cf557', '66eaabb2f14d04dea5773d29fa204a61f6323022', '0bcc81dab14626bcecfd47e3476e35f4df4c43ec', '04230489b09b9bfe31904fa043b901ad3591a4c4', '8045f05628d588a7b6c59898cdc4070978975cad', '6ad5f1d88534715051c6aba7436d60bdf65337e8', '341880efaef452f631a4a5cd61bef5dae47741d7', 'f598a8afec169c435e48ad19356c3b768f8ce7a7', '769d7c07eb58fb4957543a486c9db9db8c700500', '8bcd98bd5a451c2bbde4a22a4d1affe3c6407af0', '3f13a5148f7caa51ea946193d261d4f8ed32d81a', 'd77c0e84972c256a8922b952b04330e369f65f09', '2b64300879d6fcaabe932e87ecb412066359b286', '960d80dfff5cddf7ad16edcc95027ac9ddec2166', 'ea11436ed5979546c5a401d1aff4f7930d9a2ccc', '4db4f1af1b94fbd5defa0fa0010fdc449dd1e96c', '27a1d8192fc8449a85e0b9eb3da4813df4a24b52', '7f77058976e2fe75e98280371962c43d98c98321', '1b9c6022598085dd892f360122c0fa4c630b3f18', '6c44f8e62d824bcda4f291c679a5518bbd4225f6', 'e7a839428d06e9ea3719cf6fe5314fd861368ee7', 'fe257027193ea4a74fdab99d7509ce4002ad7de6', '1e819f533ef2bf5ca50a6b2008d96eaea2a2706e', 'e24cdf73b3e7e590c2fe5ecac9ae8aa983801367', 'd7878c2044fb699e0ce0cad83e411824b1499dc8', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '36de9353a9c319dfedba6412dfc084074846f091', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '2d9581655c87f56bbcc50c78df3b5693835175cf', 'de20cd242ec14e592b6e6d737f1d129c3e8b7467', '25732e870509248f24b8e46a9ac6b97c48691fdd', 'ed07e8f78eb97c7bd939557a0eacf27c0bb3cf14', '721d0265fce28b9b8b738892c28e65206c9224d6', 'f98d52cafd6de45c6c54f6067688b68d3ecea7cf', '94a25711f91bc18d666ad343024c1c4ed6e62a8e', '5a826febdca350f698c6e269e62479eee9ef9f38', '6c705e4ed89ad9b2f0310fe2ef243a6aa7e9df2b', 'ef40ec8296f519aef76f543197704e3c0e3fdade']}
{'paperID': '90fe9785e1ff58c1d7aa22319009e0d0e3077d29', 'abstract': 'There has been emerging interest in using transductive learning for adversarial robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020; Wang et al., ArXiv 2021). Compared to traditional defenses, these defense mechanisms"dynamically learn"the model based on test-time input; and theoretically, attacking these defenses reduces to solving a bilevel optimization problem, which poses difficulty in crafting adaptive attacks. In this paper, we examine these defense mechanisms from a principled threat analysis perspective. We formulate and analyze threat models for transductive-learning based defenses, and point out important subtleties. We propose the principle of attacking model space for solving bilevel attack objectives, and present Greedy Model Space Attack (GMSA), an attack framework that can serve as a new baseline for evaluating transductive-learning based defenses. Through systematic evaluation, we show that GMSA, even with weak instantiations, can break previous transductive-learning based defenses, which were resilient to previous attacks, such as AutoAttack. On the positive side, we report a somewhat surprising empirical result of"transductive adversarial training": Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks we consider.', 'bibtex': '@Article{Chen2021TowardsET,\n author = {Jiefeng Chen and Xi Wu and Yang Guo and Yingyu Liang and S. Jha},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Towards Evaluating the Robustness of Neural Networks Learned by Transduction},\n volume = {abs/2110.14735},\n year = {2021}\n}\n', 'references': ['b301da25b3d432e8293ca6c4ae27fed4f2689c03', '4dac4feac17a56290f79443be5c011f88b48850d', '096382c2490e205334ef7941474fae8584f53e27', 'b283f7688ebd8cfc7a46272e0beda3943ae0828c', '764eff31d9596033859895d9513b838d2c57a6fb', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '18939eadc9c4460c8385e0591cde214a1ead067b', '3805147a98dab8f0c7667fed25490adbd2300fbd', '58c143069444c7dff4be53531a47efefc40be497', '6d4a87759917132913319960389f17fa1fe8b630', 'c30f39f0bd346248b64ea3de89b4ea7db1145cc7', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '49b64383fe36268410c430352637ed23b16820c5', '979f4f67fb97b57c65867ffc92f9fffb9d30e137', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '93314b89c218c02cc1a32cad7071215693599907', '8195787260dfc6bc9abea3b1dac1ce15f747caa2', '7a84a692327534fd227fa1e07fcb3816b633c591', '804fb9542f4f56e264dd2df57c255a9a2011c00f', '85f75c7931b8176d38115c7679ae05e5a361b155', '651adaa058f821a890f2c5d1053d69eb481a8352', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '1deb6bd9bc6c0112cd06348fd738d7f50ff4b907', '99e5a8c10cf92749d4a7c2949691c3a6046e499a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '1d5972b32a9b5a455a6eef389de5b7fca25771ad', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'c3b38c2fd30adb316d0bdb32e983804be5595c30', 'c56e758ba18066a8cdc333f15dfdb7ea6af4d283', '6609c558425c9e1848944049c6e302c69eeb2842', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2', '385197d4c02593e2823c71e4f90a0993b703620e']}
{'paperID': 'a3c052386f0fae0c84c6743271ddb7a938fd755c', 'abstract': 'Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the l2 norm is useful for provable adversarial robustness, interpretable gradients and stable training. While 1-Lipschitz CNNs can be designed by enforcing a 1-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to certify the robustness of 1-Lipschitz CNNs by relaxing the orthogonalization of the last linear layer of the network that significantly advances the state of the art for both standard and provable robust accuracies on CIFAR-100 (gains of 4.80% and 4.71%, respectively). We further boost their robustness by introducing (i) a novel Gradient Norm preserving activation function called the Householder activation function (that includes every GroupSort activation) and (ii) a certificate regularization. On CIFAR-10, we achieve significant improvements over prior works in provable robust accuracy (5.81%) with only a minor drop in standard accuracy (−0.29%). Code for reproducing all experiments in the paper is available at https://github.com/singlasahil14/SOC.', 'bibtex': '@Article{Singla2021ImprovedDL,\n author = {Sahil Singla and Surbhi Singla and S. Feizi},\n booktitle = {International Conference on Learning Representations},\n title = {Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100},\n year = {2021}\n}\n', 'references': ['c3c2de457112ca1f54b08697affd68e80dabf99a', 'bef771d2430af7be7525201bd677e82cec38510f', '5ae274cc9ca0fc8a3c089d7320d103f0876bde4f', '9f0e0a59a4b3d689df8470b1218d2574244c26d6', '3fa6da03eb6c4d1ecb5560ffba299e7cc8826477', '1bfc6f9c9db5c6646f0f3e0213d407ae14d8f7bf', '06aaece45f8284de309d4d9d8772305fb848a66d', '6d0036bae18aa441a19b63fc4b2daf91f63e8029', '3f0ed6866620f76cffcb4b3653d9161a2d4aac5a', 'a3123379e326e585919f360429d77f1026ec929c', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '157605ff0f90b8193bbbcaf2e9d469d9b73be0c2', 'a1ea2a1dea5b126c24be2886c52b720b17e78b2d', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '7ad8c18994108a630c4564400f6137bf4d8b7818', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', 'f7f1b5437b9c97d67fae59142b242ce079d7cb08', 'ecba2826cd7a51d4d8b9820591ff0fa6b41d66a6', 'f0ded4902d7f9c111e50047f8c9494effb7282d1', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', 'ad6309d1ea001098189425f54d069ef12abcb583', '20f85256555ad612148e52f9363e52f9d661728b', '1b9c6022598085dd892f360122c0fa4c630b3f18', 'fb2edf25484c9e9e5f94b719c55dc1faf7591bfa', '9db631435f7f79646a4e0a1841fbeb3340e44261', '797e841a06e2f57163b86c24942b1e043fd3ca3e', 'ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf', '84de7d27e2f6160f634a483e8548c499a2cda7fa', '0bd8c29a206c46dccca63c010a95734018c98d2e', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '1cf361d02f5ad84567e48754f1a8f895653bc701', '4b23012689e0f17912fb38d4984775e567cff8d6', '69092affc3461a38eb05cf7982f104eb30b0492c', '9753967a3af8e1db1e2da52a9bb3255bd1ce5c51', '013efe3ff541e518c51f08d1b62a62e0c57c0b14', 'edf73ab12595c6709f646f542a0d2b33eb20a3f4', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '7f05b312f2de7e59c7869558507fa4a9fa0d0971', 'bb92676f9ec13783ac664c268191f20944718f95']}
{'paperID': '58947177663d73b4d7809e74482b54aadaee6444', 'abstract': 'Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts ("experts") and select one to query the LLM. They require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. P-Adapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of only using natural language queries. Finally, we investigate what makes P-Adapters successful and conclude that a significant factor is access to the LLM\'s embeddings of the original natural language prompt, particularly the subject of the entity pair being queried.', 'bibtex': '@Article{Newman2021PAdaptersRE,\n author = {Benjamin Newman and Prafulla Kumar Choubey and Nazneen Rajani},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts},\n volume = {abs/2110.07280},\n year = {2021}\n}\n', 'references': ['28692beece311a90f5fa1ca2ec9d0c2ce293d069', '339b2b711fb5b228d097b03ebc3e62a521779235', 'e337ed6543c2e6e7e51c312c7d998798fc79fdde', '91252e0e6fa12b7204719b05d85dab0923e0fe84', 'ffdbd7f0b03b85747b001b4734d5ee31b5229aa4', '209f9bde2dee7cf1677801586562ffe56d435d38', '92287e1b979e9a2cf0548bd503a0504ad2f6d54d', 'a847237e36b954c60e1959152468ebed0118f286', '128917425601a541c93c600a2f67d654512928bb', 'a49e9a8d29b5838ba392d5d33fb9694f4667c59e', '73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19', '32bc789f96acb37361ac55f36940bb52b759c229', '43f2ad297941db230c089ba353efc3f281ab678c', '81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85', '2bd5b4aed18400bf1a1cc866d9b8d931aa047290', '68c1bf884f0fc0e86641466a1f1fa67e79f16a17', 'af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2', 'd0086b86103a620a86bc918746df0aa642e2a8a3', 'f98e135986414cccf29aec593d547c0656e4d82c', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', '29ddc1f43f28af7c846515e32cc167bc66886d0c', 'd7b6753a2d4a2b286c396854063bde3a91b75535', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', '11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', '2b110fce160468eb179b6c43ea27e098757a56dd', 'd07284a6811f1b2745d91bdb06b040b57f226882', '765bdcf27ebc1eb03a14f1e47aefa4dda1e03073', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '53d8b356551a2361020a948f64454a6d599af69f', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'df24c3011fc42b72195e876ce052a0a072a1d923']}
{'paperID': '9700b4d0a9e6a64452192ae1c98e1aba34bc8c28', 'abstract': 'As reinforcement learning (RL) has achieved near human-level performance in a variety of tasks, its robustness has raised great attention. While a vast body of research has explored test-time (evasion) attacks in RL and corresponding defenses, its robustness against training-time (poisoning) attacks remains largely unanswered. In this work, we focus on certifying the robustness of offline RL in the presence of poisoning attacks, where a subset of training trajectories could be arbitrarily manipulated. We propose the first certification framework, COPA, to certify the number of poisoning trajectories that can be tolerated regarding different certification criteria. Given the complex structure of RL, we propose two certification criteria: per-state action stability and cumulative reward bound. To further improve the certification, we propose new partition and aggregation protocols to train robust policies. We further prove that some of the proposed certification methods are theoretically tight and some are NP-Complete problems. We leverage COPA to certify three RL environments trained with different algorithms and conclude: (1) The proposed robust aggregation protocols such as temporal aggregation can significantly improve the certifications; (2) Our certifications for both per-state action stability and cumulative reward bound are efficient and tight; (3) The certification for different training algorithms and environments are different, implying their intrinsic robustness properties. All experimental results are available at https://copa-leaderboard.github.io.', 'bibtex': '@Article{Wu2022COPACR,\n author = {Fan Wu and Linyi Li and Chejian Xu and Huan Zhang and B. Kailkhura and K. Kenthapadi and Ding Zhao and Bo Li},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks},\n volume = {abs/2203.08398},\n year = {2022}\n}\n', 'references': ['3b3d7adb9047d01af6dfa2975ad8addd69715e96', '5374af7bb076f9eaea7aea3045edd9b6a76d0a3b', 'dc55b6b89d536c8fee6a19a80505f447148a0b46', '018fb0e200d9fe8cb334cec5445895c0461b585f', '0a26f7a7579415e854324e4caa0c436f63804e83', '2d28fe7c7983e355a502be99c3265509d5158e99', '75454ae23df4010076b45d6e4e98723f32e68282', '2c9fc230cc4b9ff40f1b61b6dd1bac797d7f5b92', '099dc0aa605a0c5220eb8f8b39b3d8c7649df008', '6cf1116dc8b431b2471bb5407d9a8be0eb067c2d', '6fc7becf66166b9ad34409390082ff5b1b8a376e', '5e7bc93622416f14e6948a500278bfbe58cd3890', '28b924d5c9d9ded9d28d8d24cce7c9f044330875', '1764924b9c892ad85c677b95677c344d7ce99143', '5a84c26c7307df75e881a490f239877015980ae3', '4b8cc4d83437d67d803055472ed634fe8dcd1036', '9bb3d04c94a09e92375ae5377ab5187e1af3f6aa', '378d1f030d7790939681f36a7b1bf4938d662213', '3eb594bdc7057858a7bcd6243947c1944e89e2e3', 'c54b90aae50cf06cea8ffe912d2424a4e8b82e1a', '8713070502ef7ac1186d97b20f46bb26ecd84daa', '53f8181a5a414f77e3d887bb20878178f3e8f859', '320b227027030fc291de2896fc3c6da49d7614be', '20a53578f84be351bd90385fcd674821e1ace17d', '4eb7c627650395abc205010ac14f1cdb50cf788c', '45557cc70cd6989ab6b03e5aeb787e34299099f7', '4012d4ab621f3f5f04b0f91849a60c6eaabe64b4', '14a74243ce18d36c4ee7460f94b7e1d2b45b8b34', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '43a4a354b67ab6d5531355a368094815d2d2593d', '75339d34bdac0d21a41461228ec6088eecdf857a', '343a4443121f27b8f1e994501a685b91824c5789', '790ec1befba47991e8fd50a24d13be6094253f93', '9db631435f7f79646a4e0a1841fbeb3340e44261', '4b47531e2cf3ad58b14da00bb665e359e3bc2600', '3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3', 'fe3e91e40a950c6b6601b8f0a641884774d949ae', 'c1f4ef741242d629d1f56e442a09a7ba29595a0e', '4cd76f8353f0c4852cc432fc0e7a5f2b91ae6ce5', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'd951a9a7b0d24cec84ffd1022900733186504d05', '059a1297a61afc812de48edede631229608dc513', '8db9df2eadea654f128c1887722c677c708e8a47', '32ceb28e45a445df4d89df281bb0e3ab5aab1a2a', 'cf8ed2793bc6aec88da5306fe2de560dc0be9b15', 'c8c16a56d2a9520197da9a1546f517db5f19b204', '75a760c6bd5ae15e0fc489a074bc42bc1fc4e697', 'd5c26e47890f90d7b9ff64f9b395ff4210349b1b', '2319a491378867c7049b3da055c5df60e1671158', '65438e0ba226c1f97bd8a36333ebc3297b1a32fd', 'b6bfae6efa1110a57a4d8362721d152d78aae358', 'adbb4820c55771b54b9a0a275d1f36836dfb337a', 'a84f4fe31fcfb4ad92c995dba0fc09ed8fe6a4f4', '834eaef4cb1e47cd262722ee9471901db67128bd', '162d958ff885f1462aeda91cd72582323fd6a1f4', '9fb53a3bdfb47230eeaf7d956b1a238db5cba690']}
{'paperID': '3b35d09d5153edb814bfd82d63666dfb178902d0', 'abstract': 'To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective. However, the existing approaches to learning a distributionally robust model either require solving complex optimization problems such as semidefinite programming or a first-order method whose convergence scales linearly with the number of data samples -- which hinders their scalability to large datasets. In this paper, we show how different variants of DRO are simply instances of a finite-sum composite optimization for which we provide scalable methods. We also provide empirical results that demonstrate the effectiveness of our proposed algorithm with respect to the prior art in order to learn robust models from very large datasets.', 'bibtex': '@Article{Haddadpour2022LearningDR,\n author = {Farzin Haddadpour and Mohammad Mahdi Kamani and M. Mahdavi and Amin Karbasi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Distributionally Robust Models at Scale via Composite Optimization},\n volume = {abs/2203.09607},\n year = {2022}\n}\n', 'references': ['556d4ec7f098523cfbdf93a3ca5aad7908273f70', '1696660f0aa90803f72ee806750597162c373529', 'a3934b02a247d52a7e5409aa2548a2d8125f3035', '8246f0d3799290be5ed47254f6b88b601fa98230', '66488a38c3bae5d928bb22aa615fd0e64ccac62b', '1c401f906d78efb466df59d6a4e523222968c63d', 'f915e4b47291adbfa26668f84f99dbd1f85b4897', '2ffc4df07df933a732d442e23ce256c52de6d7c6', '5b3531d7f455b251d3a8a7150bb55cbaebcadf78', '10565078981924f27d38cdbb37353706ac9eba7d', '126485ec3a7ab47432a1c6e8d9d82faa6528923f', '193092aef465bec868d1089ccfcac0279b914bda', '52715c7ec1e333443e37f449e8ce481afd621ca1', 'cb340ba0b1b3c56e5003bfeb51ab6e4b60148364', 'e80f6b91767ff23c05fe42c2aa6c072531ab5380', '6ea0a369c0384987baae1a302536cf7932954fa2', '4a0d35989d91b3b7d2318802b1de6d10e4e6e830', '3077990b7fb8c730252ce77882e7bfff1a0b52ea', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', 'bce1870fbd2ea280f187da313d9b621017ac99ab', '312e6042c90338339d0366a89ac4e8b8ddf974c9', 'f7904d8876e3032d7a60f48773d93951611fec0a', 'e5f310e27f2e497dc98b76a10fdf7fb51d566d4e', 'ef6a4d065c215680c4abf0eba1fce14b21d4187f', '159395b0f7a2b9ea04f9a758d18887bcb970ee78', 'a1fb7236d104ae0343c1a09e3590ee2283483240', '0572951adde832f6f84e26b5fb87da8e1d1ebca5', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', 'afe6b57605af91525dad183171e3a850e599841f', '0bdb738ea0f49b045a3232c61d3cc5734bcf1e93', '52e557d33bdc25e0b1b834f82f2cf04176ecbceb', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '30237f91f079dcdb88d48b2a9ebc7bc5322a5386', 'b532b74b9112e5c37a72ad6de9cc618c48b0467a', '7b5c74af1ba906b1b848c024b55a2d329a0e2b52', 'db4d0e45560ceda35b6212036513bd4ab59ce99d', '352056cf4cf7e44fb8ebf863c4b632b759a39344', '322c913adfd417fa82671fb06edd66edae513ac3', '6e77765dd3250fc671c413b44554087bad43ad92', 'd42b11ce90c9c69a20ed015b73dc33e0e4100a7b', '07f5bae91cd45eafe82f3548a43268eb5c84df7a', 'd53d179dfba4804799278fcefa414c2017f391dc', '7753fa97349818282732dc795d4a439e32a2808a', 'd1dbf643447405984eeef098b1b320dee0b3b8a7', '70115dd8641dc6e5a99cc8319a6dfe9c6ba96fb7', '51dcc0c6c8ea27f0a5a3071fb8c4b32004cd55d8', '948dfbb55a93aa9585056a4b4dd3cd6553b236a9', '1a48e11f9b588ed9f4ff447b18426a4bbf875229', '0e16fb12330e653a251b7cb825d677d1af294b7c', '4daec165c1f4aa1206b0d91c0b26f0287d1ef52d', '09bf24d0340d0df49042b7f16dc5a58384b9a204', '50d7ecd6901d6759a6bd9da7f2fc8f346e073577', '961eabeaebd7035cd7668c9917fa9c39462e1113', '3bd42cfb7e633320bbeec7f6d361e92abec60b07', '7a4fa3f4974af547e42bc7d0c1ce61ff2a2c8b20', '583b55367f787eb0c4e295707b642e63547b9806', '96167ed3ebc9a2c3270f6ae96043e6f086eed4de', 'e50c7cba0a612e8045458dd2aa130d9b2a1ff560', '37ae0ead03605de4c72f6d77af19b9001c0937f9', '075e10678f83b2e9a33520f5d10d4c68e74ba5fe', '8b313c4c045bf3909e418e338b4e076e68955c85', 'e4350e816a350662ddb5f9ef92437aa8f3fd44f6', '15b2e43274de048e7aca3f3d2e7c0d5671a58163']}
{'paperID': 'acc8f7cc17ce5009bd2504572e8b9b76148e63a7', 'abstract': 'Conformal prediction is a model-agnostic tool for constructing prediction sets that are valid under the common i.i.d. assumption, which has been applied to quantify the prediction uncertainty of deep net classifiers. In this paper, we generalize this framework to the case where adversaries exist during inference time, under which the i.i.d. assumption is grossly violated. By combining conformal prediction with randomized smoothing, our proposed method forms a prediction set with finite-sample coverage guarantee that holds for any data distribution with `2norm bounded adversarial noise, generated by any adversarial attack algorithm. The core idea is to bound the Lipschitz constant of the non-conformity score by smoothing it with Gaussian noise and leverage this knowledge to account for the effect of the unknown adversarial perturbation. We demonstrate the necessity of our method in the adversarial setting and the validity of our theoretical guarantee on three widely used benchmark data sets: CIFAR10, CIFAR100, and ImageNet.', 'bibtex': '@Article{Gendler2022AdversariallyRC,\n author = {Asaf Gendler and Tsui-Wei Weng and L. Daniel and Yaniv Romano},\n booktitle = {International Conference on Learning Representations},\n title = {Adversarially Robust Conformal Prediction},\n year = {2022}\n}\n', 'references': ['445596c40dc421efe2354a340085b43181bea2be', '7e8a8c2f97ef62ab72aa57f175953df4b807d547', '40848b41ed8c9c255ecd8a920006877691b52d03', 'bcc5a2d443253d5fbbfbba883685f6cd273f28d2', '7ebd336d737609492dbc665a19c09cf42f2a4751', '00215f32433e4e69ddb5a678b3f02568334d67ca', 'a33daa0f2ed0e5bdc610be01d3ba014a2a8458d1', '55ce88684aca5dc53dbdf64e2c76d64b51ccd04a', '242a11c84c0f7198b3bef85bc89e288031eba6ca', '311ecce2e2653ca01430f65c86f0b43fc1dc7366', '108bb1678a10483aaeae07967b5e6d6a8a35afb9', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '50c5763d2d35f2c4eaa5cebea310faf2cf0a10dc', '02e2fd718da55b973965c83981e2c7fb306d2d79', '6f9dc6f8519e927d948a13aa7ae0df336f443eb9', 'f08e13d65cb17856427b429d79f01922584a6f01', 'ebc3193ae46286f82f741143f5d67891c1625209', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', 'f2b508ee78b240c1c6d7932f736a3eaeb4604602', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '405b6ff2ea2ec9a7c7d6b18ac951dc778892ffcf', '061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'eb42cf88027de515750f230b23b1a057dc782108', '5fc662287842e5cb2d23b5fa917354e957c573bf', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'acd3f1ee1c67bd56a8b6454f413b89a28d34288e', '5e7c16a648c5de60172b2a8ab9597d00ae1481b1', '33a0803fc10233bd05756ed83db92e8d827d3396', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'e0293e4a293ff1a0137c6fcfd3be3274c16b9959', '12ba5ebf8949170e200d9ebd7ee090ec54775093', '7f96ca1a46b8e1fabcad6d459e7820dd348675f7', '5d90f06bb70a0a3dced62413346235c02b1aa086', '4d8a2fcf9b43efd2636d5f90e0d35b98194c0025']}
{'paperID': '2cecb623ba2e2ee4872bb07b794552fd73a87976', 'abstract': None, 'bibtex': '@Article{Ding2022AudioLS,\n author = {Shaojin Ding and Tianlong Chen and Zhangyang Wang},\n booktitle = {International Conference on Learning Representations},\n title = {Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable},\n year = {2022}\n}\n', 'references': []}
{'paperID': '5b540745f4b51f95bf90fb3420e51edb037fc51a', 'abstract': 'Experiments with pre-trained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact tested in the experiment (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure which includes the architecture, training data, initialization scheme, and loss function. Recent work has shown that repeating the pre-training process can lead to substantially different performance, suggesting that an alternate strategy is needed to make principled statements about procedures. To enable researchers to draw more robust conclusions, we introduce the MultiBERTs, a set of 25 BERT-Base checkpoints, trained with similar hyper-parameters as the original BERT model but differing in random weight initialization and shuffling of training data. We also define the Multi-Bootstrap, a non-parametric bootstrap method for statistical inference designed for settings where there are multiple pre-trained models and limited test data. To illustrate our approach, we present a case study of gender bias in coreference resolution, in which the Multi-Bootstrap lets us measure effects that may not be detected with a single checkpoint. We release our models and statistical library along with an additional set of 140 intermediate checkpoints captured during pre-training to facilitate research on learning dynamics.', 'bibtex': "@Article{Sellam2021TheMB,\n author = {Thibault Sellam and Steve Yadlowsky and Jason Wei and Naomi Saphra and A. D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Turc and Jacob Eisenstein and Dipanjan Das and Ian Tenney and Ellie Pavlick},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {The MultiBERTs: BERT Reproductions for Robustness Analysis},\n volume = {abs/2106.16163},\n year = {2021}\n}\n", 'references': ['df37ba004c3bec70c9ed6f614944338d6ec6ec68', '6a8cb4fb5a20c7e5733a9bd50cd5feaad6c11360', '7c799b7bd8c069c6feb7235345c97aa1f5330b84', '79b8ef3905a42b771248719495a2117271906445', '0672f88d5dc762002b515ca4a0a9f101017fea35', 'dfb4e80deb187bcb85708f751c5d466c399f76f3', '71a85e735a3686bef8cce3725ae5ba82e2cabb1b', '186d26390779f7c54930e05812cfe85e6973961f', '3d864a8bc5a55ccab9993aa66203d8e70b88148c', 'b1d309073623d46548e55269fb73485a3b7f11a8', '056935031bc5cf0aeeaa0946320de26e14a1817e', '8b9d77d5e52a70af37451d3db3d32781b83ea054', 'babeda48b10a4d638252118f2238d05a06f4ec55', '82a44fbe798d514c81439c90c655975a32c2af10', 'bd20069f5cac3e63083ecf6479abc1799db33ce0', 'baf60d13c98916b77b09bc525ede1cd610ed1db5', '48689c4bb52a45c0bc97d1421d72d11bab6c346b', 'b3ea2d9c8e5ea3b87ace121f0bece71565abc187', 'af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2', '222b9a7b8038120671a1610e857d3edbc7ac5550', '7402b604f14b8b91c53ed6eed04af92c59636c97', '5744f56d3253bd7c4341d36de40a93fceaa266b3', 'd0086b86103a620a86bc918746df0aa642e2a8a3', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', '6a7769116c6733dffa347444b2835e50129e0143', 'd6a083dad7114f3a39adc65c09bfbb6cf3fee9ea', '455a8838cde44f288d456d01c76ede95b56dc675', '07a64686ce8e43ac475a8d820a8a9f1d87989583', '97906df07855b029b7aae7c2a1c6c5e8df1d531c', '42ed4a9994e6121a9f325f5b901c5b3d7ce104f5', 'b47381e04739ea3f392ba6c8faaf64105493c196', 'd10df96b3fb0ab5c6b1d0cc22c7400d0acccc3cc', '4d1c856275744c0284312a3a50efb6ca9dc4cd4c', '175b58fe7e49bb5c0c771b73f8834bcff21b59c7', 'cb0f3ee1e98faf92429d601cdcd76c69c1e484eb', '9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7', '451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c', '8cb87626ceab76f8f90e0aeacde868d562a146a1', 'a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096', '5ded2b8c64491b4a67f6d39ce473d4b9347a672e', '05dd7254b632376973f3a1b4d39485da17814df5', '0e6824e137847be0599bb0032e37042ed2ef5045', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '0db4e6f24f1e56f9c2cc5d7cfb6adae858f6fb19', '687bac2d3320083eb4530bf18bb8f8f721477600', '645c9d018d6379a4c2eed5ab2f62aeb5629544ce', '20268778b9cfef7211121ea01675c7c14ee45415', 'e54d8b07ef659f9ee2671441c4355e414e408836', 'cb826a3899752b796f14df1c50378c64954a6b0a', 'd7da009f457917aa381619facfa5ffae9329a6e9', 'b8894e5b2d3afde2142b7ae49d4ba08b77984b29', '45ee7447b9dd406496c4a5d9d8fb6556366a01c6', 'e3649e2b89a3aabd4c3b13289f49353bffcbe66d', 'e61cd2d6b8cd9e964d2fb01146d20aca606d9e87', '0e72382cdd4b8e4064e2cc14febfda29540a4da6', '4499bc6fa534c9b22241eb6f5a39a09dd2fa1905', '888c3a3788c52d6637d45dc4238691083884589d', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '8ff46c88964a36985f2b45933a3d47b81bd87bd0', 'db8885a0037fe47d973ade79d696586453710233', '351ec42df2b60c6042addf96e6b98673bbaf4dfd', '75f8a4d7ed6a0f32fa098cac967de247938d9ce5', '475354f10798f110d34792b6d88f31d6d5cb099e', '35a1f341ac3453cc4982b8c13d9169119cf9a893', '859093f5cae3dd580ae34e39fbef19a8c89fd85f']}
{'paperID': '1e57462f93d78279549a8508e691dc4920151b35', 'abstract': 'We consider the problem of training a classification model with group annotated training data. Recent work has established that, if there is distribution shift across different groups, models trained using the standard empirical risk minimization (ERM) objective suffer from poor performance on minority groups and that group distributionally robust optimization (Group-DRO) objective is a better alternative. The starting point of this paper is the observation that though Group-DRO performs better than ERM on minority groups for some benchmark datasets, there are several other datasets where it performs much worse than ERM. Inspired by ideas from the closely related problem of domain generalization, this paper proposes a new and simple algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind our proposed algorithm is that while Group-DRO focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group-DRO. Empirically, we show that our proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM and Group-DRO on standard benchmarks on both minority groups and across all groups. Theoretically, we show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions.', 'bibtex': '@Article{Piratla2021FocusOT,\n author = {Vihari Piratla and Praneeth Netrapalli and Sunita Sarawagi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Focus on the Common Good: Group Distributional Robustness Follows},\n volume = {abs/2110.02619},\n year = {2021}\n}\n', 'references': ['216d093cb2ad81bf55c21dbce2217f2b9032e67b', '714fc6626c527e05f2a31626d067d46520c6740e', '984f00eb0fa62a91d9b4168190218d85eaaa9ae1', '791ad7876370704b69184927403dc8eefb7c3f33', '2263df3f2a10f43876aa280442f14090a9098369', '40848b41ed8c9c255ecd8a920006877691b52d03', '00325cb5408da77827951abd3fa93ec3bd019608', '553028f7f7c850371379c621e40d7d00e75303a6', '9207480a5cd071a3e85f408082b09283413cbfa5', '7bfaca28948164006002a3a71a38165d36af51c5', '26e858cf3c82b66bbd539bb79356b0e885bdc694', '5d0e2635a1ebe2c9347529975bc876d4286c9ab7', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'b611a8095630557229dc5fb6b07c272f1cd614da', '1b27b9cfe0ce17950b6ea72f9ef8cf5a7459bccd', '57eedf785fd9e3ea28b4cd30539cb0fa374f9e74', 'a1fb7236d104ae0343c1a09e3590ee2283483240', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', '2997b26ffb8c291ce478bd8a6e47979d5a55c466', 'cabc42832388b1995d1f815f9fc4253f3f593993', 'a588d38ec81c0337b445931eadf6f443aea13380', '5f7b1cf0323893735220332ef6aecbff90f3d44b', '5ded2b8c64491b4a67f6d39ce473d4b9347a672e', '07b5093aace8e485e7d23b83edb6351618138127', '12d0cf8ae5ffe1b89345e1dcead22be592d844b2', '359c56a068e4a84a9f3b78f43b53fe3b333c0ba0', '0aae10ade8fc9e58e177e034b794fce45c32fde8']}
{'paperID': '1d05745fddf59153931976d07468e710b6ef3939', 'abstract': 'A fundamental question in adversarial machine learning is whether a robust classifier exists for a given task. A line of research has made some progress towards this goal by studying the concentration of measure, but we argue standard concentration fails to fully characterize the intrinsic robustness of a classification problem since it ignores data labels which are essential to any classification task. Building on a novel definition of label uncertainty, we empirically demonstrate that error regions induced by state-of-the-art models tend to have much higher label uncertainty than randomly-selected subsets. This observation motivates us to adapt a concentration estimation algorithm to account for label uncertainty, resulting in more accurate intrinsic robustness measures for benchmark image classification problems.', 'bibtex': '@Article{Zhang2021UnderstandingIR,\n author = {Xiao Zhang and David Evans},\n booktitle = {International Conference on Learning Representations},\n title = {Understanding Intrinsic Robustness Using Label Uncertainty},\n year = {2021}\n}\n', 'references': ['a4f9e7e695bba1ffb90b30752a40d5ee907dcb36', 'ba8d84deb3633076a135e6b885da609eb7b1c7e0', '2aab97e35c43d961d645e650808d5b052ec180ab', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '18939eadc9c4460c8385e0591cde214a1ead067b', '2c624c74f64ea60402c155deeca5877a85fb9587', '58c143069444c7dff4be53531a47efefc40be497', 'cbaaa1154c491f9da2f050d3c22970e15bb7b52b', '41b058078b0fd949594282843a3df7df3e0957eb', 'c9d239db4ab86522a6fdecb86116d1083a48823c', '2f255dcc2f28c11ea3b2888f537dbf9916abcc02', 'd2a2be6ce932a0f1939f31cfff4d64ea3d76723d', 'ff22e140a0423f1cf0595d213f36402668084014', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', '30fabb3369cda7c1d515c87bf453a3ebd61e149e', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'ceb5e51c800e1d243f406da5d004bb39e7f4b6be', '15c1e7166708737d87545eae8f37f302599f61e1', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', '88311ee3fbb9d8d307386c0fb53aaa0283c5eb74', 'fd02c5b49bab02fb814c6999ebf161f3be377c75', '20f85256555ad612148e52f9363e52f9d661728b', 'ba4883eafede39be6494a80c8b999abc46fa6b5e', '7005fe514458b538b7516b41af5f5e1971154070', '80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '9a089c56eec68df722b2a5a52727143aacdc2532', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'e9b5098f6083585c15f9a6315f457db98086bf82', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'a4b4baa5d861347c21b435d938d84e5ac0ded9f2', '1ab5c006caf3bf8c128fdfad80e58277cb8b1455', '219f2f413cf2fa2e17ba763a417b25d2e4a5247d', '1179f0698939f0ed10f144fb87dbbb70f124525d', '5f82a74801c38b2b8cf5cb836e1de233ead164e6', 'e225dd59ef4954db21479cdcbee497624b2d6d0f', '5d90f06bb70a0a3dced62413346235c02b1aa086', '43fcdee6c6d885ac2bd32e122dbf282f93720c22', 'a38b2bd0ea02ae33237fbeb3d03db0f088e725de', '135f43022b71acf6dce63514b1740aee9f358937']}
{'paperID': '7cb6a6369c6b01de8f88539687cb4acc121edb94', 'abstract': "We introduce the concept of provably robust adversarial examples for deep neural networks - connected input regions constructed from standard adversarial examples which are guaranteed to be robust to a set of real-world perturbations (such as changes in pixel intensity and geometric transformations). We present a novel method called PARADE for generating these regions in a scalable manner which works by iteratively refining the region initially obtained via sampling until a refined region is certified to be adversarial with existing state-of-the-art verifiers. At each step, a novel optimization procedure is applied to maximize the region's volume under the constraint that the convex relaxation of the network behavior with respect to the region implies a chosen bound on the certification objective. Our experimental evaluation shows the effectiveness of PARADE: it successfully finds large provably robust regions including ones containing $\\approx 10^{573}$ adversarial examples for pixel intensity and $\\approx 10^{599}$ for geometric perturbations. The provability enables our robust examples to be significantly more effective against state-of-the-art defenses based on randomized smoothing than the individual attacks used to construct the regions.", 'bibtex': '@Article{Dimitrov2020ProvablyRA,\n author = {Dimitar I. Dimitrov and Gagandeep Singh and Timon Gehr and Martin T. Vechev},\n booktitle = {International Conference on Learning Representations},\n title = {Provably Robust Adversarial Examples},\n year = {2020}\n}\n', 'references': ['71ea8f105803703893b5c2d01f0c9508643b6554', '2a88cc8cc9562b4addec03ba16b35cb4d3baaa43', '042f60694b8955f176057efd87b713815ede260c', '4c9ee8358d82afa960708391e2b8e83c4a737ae9', '8ab4cf57fa4464a594638b9d684071def6db6d0b', '163ead56c23d6b0f5df5f24f7dc74ef82cb41eb3', '58c143069444c7dff4be53531a47efefc40be497', '8733fe2371b615609b04e2e910b1ecfa8e77cbc2', 'e9382c9150cd53289ea7af0b1dafd1a0bb9dbd12', '91a05cb84f1c7dbb0354da2ff11ae92549152435', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '45a0d88560840ea2a352ae6dd3919d40f3ab2778', 'c68cd22de315a14587120e98bb02fdcf51edec46', '649de559f530aab8f22f6022d40cdfd3bb4e1039', '1da3aab62fc3d4540b36c851df7993c53441d2a7', '361c7858fa8f55928dc6358bc25d18fe3316d735', '23da4126cedfb52858a2e320ab28b1df774d7836', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'f20de741e2d4ece239261de010d6d9beca3b26b9', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', 'f749b2576c062a4fd16bb668d76d1e1084ad704e', '75339d34bdac0d21a41461228ec6088eecdf857a', 'd21fde0f55ee0285c66334d37b8920c867959784', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '0314e777333a63aca5735ea136c74e113aa8801d', '8dce99e33c6fceb3e79023f5894fdbe733c91e92', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '333416708c80d0c163ca275d1b190b1f2576fa5f', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'dbb6ded623159c867fbeca0772db7b2eb9489523', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '3ce2d233cee585ecff73729836918ba87195c18f', '37ae0ead03605de4c72f6d77af19b9001c0937f9', '0ae5e6f0a656aa5a133485b498a2386942821786', '641864f095a113c8a0e4f8e58647c4b22f5ef478', 'bb92676f9ec13783ac664c268191f20944718f95', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '01594f00b0deed32cba4fc4ea8c74b60be31db4a', 'abstract': 'Recent studies demonstrate that deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., injecting appropriate forms of sparsity during adversarial training. We introduce two alternatives for sparse adversarial training: (i) static sparsity, by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) dynamic sparsity, by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training. We find both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overfitting, meanwhile significantly saving training and inference FLOPs. Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet. For example, our methods reduce robust generalization gap and overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with ResNet-18. Besides, our approaches can be organically combined with existing regularizers, establishing new state-of-the-art results in AT. Codes are available in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.', 'bibtex': '@Article{Chen2022SparsityWT,\n author = {Tianlong Chen and Zhenyu (Allen) Zhang and Pengju Wang and Santosh Balachandra and Haoyu Ma and Zehao Wang and Zhangyang Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Sparsity Winning Twice: Better Robust Generalization from More Efficient Training},\n volume = {abs/2202.09844},\n year = {2022}\n}\n', 'references': ['e468f74ebffa8bbdd99bf8d0233822a1d2a9b430', '756eac52299df7c1e525e72bb53a6e0ececdcf01', 'ffbcbced0ec14a9267f185be87d9386407640a11', '5c493a976f724ea8f238509f6fe087a7bde8c93d', '7a9ac680f286f56cd9206111416733bf0845618b', '4ecb57ba76714ed4f14d11d3b30548225b2f15bc', 'd4788b3996a5ec681ec72373111035f6d84da4f6', '9c4dd36ad206ca8be96ae4000568e899f4acfa91', '6bc4681828143f5ecc49b7ecd388a86c70c7237a', 'abbe3a82bb11a9f28eba39ff6dc17982a724c2fd', '23fa2f604f73785b638eb49df4c1bbf293e16cd5', '22299b440277b4bc887168a669408d5547c1461a', '0563077a0dbd532c19e35e60e7188ba661fb886e', '009fae4facad32774d1acc61c3a779d646bc1c55', '33ca8d34d226e47e0830b6eb73c06e0b85ae7ab7', '5dbc41ce67d979a1e3b7099577a4827c99119d3b', 'cb0691748506ab827f6f03fafbabb4141d22ca79', '01217fd88d07b05affa75213672d3d31dbcb6617', '229a4d27d04bd3901ef0ca41942eb0cdd4f28eed', '4df2175c0daadf630623a505f623fe41a386853d', '5f6fccc32953f57fe29b2316eb8351e84b0179dc', '49f4a967f66d740ee3efb704f70b8d5da197394f', '99a599d8fe56529f47e78243ed61250190f96196', '67f74fe9d46f88661573003f8f1f12967ae49fa3', '9bc25860c60974331283216ef16425095477f84a', '9851c5a26ce75133fcb5df4d7a949d943cffa9f7', '389036b1366b64579725457993c1f63a4f3370ba', '3b0fb765716ef6861a84abffcbe40643857c613b', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '232c55babd84ea40f40cccf2684dd46c02bb8a49', '962a8ffc7d72990a28d505f49a39108b4803c223', '63c9a825d002a376c1cfeb5cde0e231fb4669814', '3a1e9670446a39bb26e8265e78caef0ed11b9a54', '850464c9006261bd632c4203f3e630db09a32faf', '365fb36b15f13c0c69596a9fc98ddcaed3fe739c', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', 'b27da51d2b33c67b1b366f6f3a1e61e84dbab230', '3805147a98dab8f0c7667fed25490adbd2300fbd', '70a6f1820eec8152f4af826d9adf61f442a24743', 'c114ce10c4a315d92c3815f54bc9893e7e6ef182', '6d4a87759917132913319960389f17fa1fe8b630', '52184d7a541eff0b9537e75da7327dd41daba207', '3f06d02513a2763e472d2b5d5db08e9061081b9e', '2e3002f131e1815bda7a10303eff97f79dea01ec', 'a29c3bb07d478a354fd5bc5635f98560ede8f8bb', '336868be817536e7c7fc88c391a2860cd869ea2b', '30b2c7dfec16457aeb7e6f6e6e12af0300f2fef4', '7c83832f00579685ce454d0f2d61758db2b635d6', '60ed82ca3ec8fbfef4d52e98e49ab687ce501a0c', '7139d823ad17ba2c958ec0f821c4bbcd69c92a69', 'c96bcdd27aaa365a770d4a6c739e1a374a441cb1', 'a6f4917d043494d2ebaebe6b65cb35e6a07fda41', 'c3d846a3c51dc6423381257b95a4b821e778dce0', 'dd7bae431e0e4d94f24d54b0ac3a422703d38ed3', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', '1eff01027877843f1b492c4abecdbbc112497d29', 'ebde1d64f5a77f36258ac6c23f6285050968e3f9', '8e2c65ff58b28a076883c99b96840e19b5e0b916', '170dd6ea32861684e9fa4cc45816e5f6c2f44ea2', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '7bc43b9f701823c04c7ee3ed780a34b041ee4b07', '52a4555f85b18243a95b426d48aeb69e5b332322', '5ae786deaa875613e85ed2df0dbeec4301109f74', '77030ffbd1119bf98614459bc087a8a59ce5697d', '9eda74b1219572287e489f84134bb935f139c4e7', '2aa2fe58f441fbecc183965bf79f4ef7fb2dd4a7', '1802a7870a642d414f435273dd9e9190a0dc4fcb', 'cf440ccce4a7a8681e238b4f26d5b95109add55d', '64db2e2c76aa3f028b6866f91795a7c005a3f13b', '6effa092456e30e7e54954fd28b755e0a75b52b8', '804fb9542f4f56e264dd2df57c255a9a2011c00f', 'b8989afff14fb630ca58b6afa917fb42574228ee', '21937ecd9d66567184b83eca3d3e09eb4e6fbd60', '2f201c77e7ccdf1f37115e16accac3486a65c03d', '651adaa058f821a890f2c5d1053d69eb481a8352', 'ca9c1224636b0a7dd37340a4691c34a9914b5af8', '1cf361d02f5ad84567e48754f1a8f895653bc701', '9a089c56eec68df722b2a5a52727143aacdc2532', '8e37a3b227b68953f8067215828dc8b8714cb21b', '90a16f34d109b63d95ab4da2d491cbe3a1c8b656', 'ee53c9480132fc0d09b1192226cb2c460462fd6d', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'f538dca4def5167a32fbc12107b69a05f0c9d832', '255d2c2af6d7abbbebfc03dab51cd8574ad3558e', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', 'ee48b932a60085d7fd5540637540509144b07030', '34cc3ceae5c3f7c8acbb89f2bff63f9d452b00d5', '3ed94217fbf29b86d5f1baec90dc33adacb40b58', 'e7eef2ac4136ec93bd306d2c9c353a13729a4553', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'c00f744f103a528f5b45bf0482f54b5e6a9f7740', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '0a33c01af2e563dd8e2a6f131fc8ba7943702469', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '642d0f49b7826adcf986616f4af77e736229990f', '1ff9a37d766e3a4f39757f5e1b235a42dacf18ff', '0c908739fbff75f03469d13d4a1a07de3414ee19', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'eb42cf88027de515750f230b23b1a057dc782108', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '2cecb623ba2e2ee4872bb07b794552fd73a87976', '05e2ca9357bcf542a33b3f97310d9f477cd0776f', '8d0cd2e89afc2aae9a1637b0b53b656cc7e7dd76', 'd5906006e6efc5dbc02878d76407326eb56c363a', 'e3f41f4b6b4e3ab740176f022bcad522ad4c38ec', '3c9c6e623391d9265058b6d452a39ad78f0a251c', '6ee98d9b218fcace923fe5fef742cee54ebd32f3', 'e225dd59ef4954db21479cdcbee497624b2d6d0f', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'e7297db245c3feb1897720b173a59fe7e36babb7']}
{'paperID': 'f943391013a0436b084d6e11b8527e1465cfff53', 'abstract': None, 'bibtex': '@Article{Fu2022RobustUE,\n author = {Shaopeng Fu and Fengxiang He and Yang Liu and Li Shen and Dacheng Tao},\n booktitle = {International Conference on Learning Representations},\n title = {Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning},\n year = {2022}\n}\n', 'references': []}
{'paperID': '4bcd4f8ef3f269562dce183ed0329f93b24fd4e6', 'abstract': 'Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness enhancement. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research.', 'bibtex': '@Article{Yang2022OnRP,\n author = {Zonghan Yang and Yang Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On Robust Prefix-Tuning for Text Classification},\n volume = {abs/2203.10378},\n year = {2022}\n}\n', 'references': ['31e46ed4722a4895a19eda37dbc02da55572783a', '6f0aba8102d63938ce0b48ec23ff5ddd8110f2e8', 'f659031ceb7bbdcb7b0690742f35e2924fd1ed75', '28692beece311a90f5fa1ca2ec9d0c2ce293d069', 'e399e78f2c236802aa50aef95554a3768079edb1', '01b5412f3d17e90e09226d7c40ad4d4468a1414d', '353c88c231ce156d604e074af276422422fc73f7', 'ffdbd7f0b03b85747b001b4734d5ee31b5229aa4', '209f9bde2dee7cf1677801586562ffe56d435d38', 'a847237e36b954c60e1959152468ebed0118f286', 'e837660da43eb9637fa33aee3b58599e438d1f5f', '16a8e329c06b4c6f61762da7fa77a84bf3e12dca', '3ab9145d5134e4e89bcceb1c8a95f9f98c98c5ff', 'a6ca91afe845ef5294c40c2029e0c1cba19ba40b', '1a452afbaf41306dbc9cbeb5cdedb85b85eacb0f', '85e7d63f75c0916bd350a229e040c5fbb1472e7a', '5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e', 'c5662edb2182b5e27eb73d1187c37db28c98fba6', 'c94529aff09763b607b7594197f1bbf01c006759', '74276a37bfa50f90dfae37f767b2b67784bd402a', '2dc7741c3cd3c7fc0d0ae7b60cf7358f612e175b', '95968b89040146cb015827aee8ff6f77d67bbaf1', '024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb', '0a9c0e729dd95f5559e05f8bb4b7408f9409388e', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', '32bc789f96acb37361ac55f36940bb52b759c229', '76a9f336481b39515d6cea2920696f11fb686451', '2ffcf8352223c95ae8cef4daaec995525ecc926b', '06a427e1688f92053a38c73cb4e0da25177c89e7', '88338c58701f34503c7af77e34f19d9a5cd66313', '18a9bb863e3110e2e981b53618b214585a32f877', '6189bf5f4c851ad0217a782509f8818aca4c7ff4', 'bbf105d2286c5a6b09998f514f685310562973b3', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', '81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85', 'ab70853cd5912c470f6ff95e95481980f0a2a41b', '395de0bd3837fdf4b4b5e5f04835bcc69c279481', '3cfb319689f06bf04c2e28399361f414ca32c4b3', 'af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2', 'd01fa0311e8e15b8b874b376123530c815f52852', 'b60a5fcbb085f136c71b6215c4c5f4c287e99f9b', '112fd54ee193237b24f2ce7fce79e399609a29c5', '07398e448180ad75c44d30f23a65289d40ff6f52', '4690190d6c110f7525f7250e1acf4a4eab42519f', '9f0a45103aa474ee4d0946de8b690087dd065b60', '18a1c21f35153c45d0ef30c564bffb7d70a13ccc', 'ce177672b00ddf46e4906157a7e997ca9338b8b9', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', '1adfa30bf112de20cb959014e44626d760aa8e4e', 'e0c6abdbdecf04ffac65c440da77fb9d66bb474c', 'd6a083dad7114f3a39adc65c09bfbb6cf3fee9ea', '135112c7ba1762d65f39b1a61777f26ae4dfd8ad', '162515d87256f13888d9d7ba95275ac4b6c35396', 'e235ad7dcf6e97cd372f09724dc947c5b1efac79', '1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f', '4dda68faa3ea2c888711ce5ced009afcb612e05b', '29ddc1f43f28af7c846515e32cc167bc66886d0c', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'f91175950edf3804ff1573f570b03db9b108dece', '00f9e7f7c83a70829da876ffffcedeaeba0f7a55', '472644c5f4155635cf9e9e37540bfa53c20e7610', '9784fbf77295860b2e412137b86356d70b25e3c0', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', '2b110fce160468eb179b6c43ea27e098757a56dd', '3febb2bed8865945e7fddc99efd791887bb7e14f', '514e7fb769950dbe96eb519c88ca17e04dc829f6', 'd07284a6811f1b2745d91bdb06b040b57f226882', '5e3fd9e6e7bcfc37fa751385ea3c8c7c7ac80c43', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '4b1c6f6521da545892f3f5dc39461584d4a27ec0', '455afd748e8834ef521e4b67c7c056d3c33429e2', '2cd55ded95d5d13430edfa223ba591b514ebe8a5', '51a55df1f023571a7e07e338ee45a3e3d66ef73e', 'f04df4e20a18358ea2f689b4c129781628ef7fc1', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '687bac2d3320083eb4530bf18bb8f8f721477600', 'cb826a3899752b796f14df1c50378c64954a6b0a', 'cac33f91e59f0a137b46176d74cee55c7010c3f8', '53d8b356551a2361020a948f64454a6d599af69f', '0cca27a289b595763d33b0a66ac1b3fc5b3ddc73', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '9405cc0d6169988371b2755e573cc28650d14dfe', 'cef931cb5547d615f253b759a2a0fe084112cb38', 'df24c3011fc42b72195e876ce052a0a072a1d923']}
{'paperID': '5374af7bb076f9eaea7aea3045edd9b6a76d0a3b', 'abstract': 'As reinforcement learning (RL) has achieved great success and been even adopted in safety-critical domains such as autonomous vehicles, a range of empirical studies have been conducted to improve its robustness against adversarial attacks. However, how to certify its robustness with theoretical guarantees still remains challenging. In this paper, we present the first unified framework CROP (Certifying Robust Policies for RL) to provide robustness certification on both action and reward levels. In particular, we propose two robustness certification criteria: robustness of per-state actions and lower bound of cumulative rewards. We then develop a local smoothing algorithm for policies derived from Q-functions to guarantee the robustness of actions taken along the trajectory; we also develop a global smoothing algorithm for certifying the lower bound of a finite-horizon cumulative reward, as well as a novel local smoothing algorithm to perform adaptive search in order to obtain tighter reward certification. Empirically, we apply CROP to evaluate several existing empirically robust RL algorithms, including adversarial training and different robust regularization, in four environments (two representative Atari games, Highway, and CartPole). Furthermore, by evaluating these algorithms against adversarial attacks, we demonstrate that our certifications are often tight. All experiment results are available at website https://crop-leaderboard.github.io.', 'bibtex': '@Article{Wu2021CROPCR,\n author = {Fan Wu and Linyi Li and Zijian Huang and Yevgeniy Vorobeychik and Ding Zhao and Bo Li},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing},\n volume = {abs/2106.09292},\n year = {2021}\n}\n', 'references': ['7f2f8042750df1be7562023760148a391d247904', '4754ad07af3dce5262382ae47e496f694b61f589', 'b284afe9a7363b898661c9b3cfb7f015b158cc63', '1a627d2a169d71563109546da590a7cceb0b349a', 'f4f28269ad1aa5231e72415945afb4a8a9a0c70a', '7915568a86a196c06bb70fb98b3bc076a5a18b8d', '06aaece45f8284de309d4d9d8772305fb848a66d', '2c9fc230cc4b9ff40f1b61b6dd1bac797d7f5b92', '07c67c090ee27676ef91d288d897f4b2352b42e3', '62cf842a62bf9c78ec40faae72b60398dc87a576', '28b924d5c9d9ded9d28d8d24cce7c9f044330875', '63fde44331c14fdf1f5dc0da19921a13f39b1fe0', '9bb3d04c94a09e92375ae5377ab5187e1af3f6aa', '3a7d95aed866d68e189db6f4eab29f46f68c5ffc', '75170439ccfe2271367e4ed7298f360b0443fde2', '61bee52afa721d13982289497f3408e54444f85b', 'c54b90aae50cf06cea8ffe912d2424a4e8b82e1a', '320b227027030fc291de2896fc3c6da49d7614be', 'fe463f18d2515a7052a575beb89e14c93c99e66a', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '50c5763d2d35f2c4eaa5cebea310faf2cf0a10dc', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6252044bed5824ea8ca519c71fed56a90f5a0ee2', '6ff50528f3d7c72772f8c0e3f8398f9dd8e06575', 'adcd4bfd88213da0c33d3cb7057411dd15b72c7a', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'f20de741e2d4ece239261de010d6d9beca3b26b9', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', '75339d34bdac0d21a41461228ec6088eecdf857a', '343a4443121f27b8f1e994501a685b91824c5789', 'f0c5991dbb130fa6b5de011cf7a04f6ed815ef68', 'd21fde0f55ee0285c66334d37b8920c867959784', '9db631435f7f79646a4e0a1841fbeb3340e44261', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '4b47531e2cf3ad58b14da00bb665e359e3bc2600', '3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3', '65a2534e3bd229eb368bbaad32f92a68bd0e936a', '9de69a46e6c619255eeffbfbb6c7b7163690eb48', '4b23012689e0f17912fb38d4984775e567cff8d6', '2a5734ea4cb938c437e5a439f3d439877029735d', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', '4cd76f8353f0c4852cc432fc0e7a5f2b91ae6ce5', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '255d2c2af6d7abbbebfc03dab51cd8574ad3558e', '171bfa2abddd30ad177cd620c86b7f8fa64964d1', '333416708c80d0c163ca275d1b190b1f2576fa5f', '059a1297a61afc812de48edede631229608dc513', '9f92a0ccc8b039a83bd5ba5482facb5829c712aa', '8db9df2eadea654f128c1887722c677c708e8a47', '32ceb28e45a445df4d89df281bb0e3ab5aab1a2a', '236b40f3144b95cd84779484c8269092122920aa', 'f96478d0694f18384934fc19a2655170f32e2d8c', 'cf8ed2793bc6aec88da5306fe2de560dc0be9b15', 'c8c16a56d2a9520197da9a1546f517db5f19b204', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', '1d65848c563b2c3a7f0153551c1b39e0e5c2d776', 'e2a85a6766b982ff7c8980e57ca6342d22493827', '3ed67ded2b4d3614b38798b3f17a8e69803d0980', '75a760c6bd5ae15e0fc489a074bc42bc1fc4e697', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'c6170fa90d3b2efede5a2e1660cb23e1c824f2ca', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '3b9732bb07dc99bde5e1f9f75251c6ea5039373e', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '2319a491378867c7049b3da055c5df60e1671158', '65438e0ba226c1f97bd8a36333ebc3297b1a32fd', 'b6bfae6efa1110a57a4d8362721d152d78aae358', 'f82e4ff4f003581330338aaae71f60316e58dd26', 'a51f6c9930b5604918a649ebbebdd2eaf8d1701e', '6db16608fccddef51202af84112b34cfebfbe20a', 'ab4a1c4dfe23b3a1e3d077df467452cc68f64de8', 'd4180fc8e2a9d9f69f815e33b9f937197f649f96', '03b7e51c52084ac1db5118342a00b5fbcfc587aa', '0bb3cdf6222e0f6d2d949edf3a83e62834652c0a', '385742fffcf113656f0d3cf6c06ef95cb8439dc6', '22faafeba7d7443da14c1e23e549b94e40d7d6ee', '1c74180188a592d20a63cedb45d53089201fe127', 'e073a7c5a6418d96fc16d8337a6056a457e75c1e', 'c7bf61f72cce609ce7b754e570fe1ec05ca3827b']}
{'paperID': 'bef771d2430af7be7525201bd677e82cec38510f', 'abstract': 'Recently, Zhang et al. (2021) developed a new neural network architecture based on $\\ell_\\infty$-distance functions, which naturally possesses certified $\\ell_\\infty$ robustness by its construction. Despite the novel design and theoretical foundation, so far the model only achieved comparable performance to conventional networks. In this paper, we make the following two contributions: $\\mathrm{(i)}$ We demonstrate that $\\ell_\\infty$-distance nets enjoy a fundamental advantage in certified robustness over conventional networks (under typical certification approaches); $\\mathrm{(ii)}$ With an improved training process we are able to significantly boost the certified accuracy of $\\ell_\\infty$-distance nets. Our training approach largely alleviates the optimization problem that arose in the previous training scheme, in particular, the unexpected large Lipschitz constant due to the use of a crucial trick called $\\ell_p$-relaxation. The core of our training approach is a novel objective function that combines scaled cross-entropy loss and clipped hinge loss with a decaying mixing coefficient. Experiments show that using the proposed training strategy, the certified accuracy of $\\ell_\\infty$-distance net can be dramatically improved from 33.30% to 40.06% on CIFAR-10 ($\\epsilon=8/255$), meanwhile outperforming other approaches in this area by a large margin. Our results clearly demonstrate the effectiveness and potential of $\\ell_\\infty$-distance net for certified robustness. Codes are available at https://github.com/zbh2047/L_inf-dist-net-v2.', 'bibtex': '@Article{Zhang2021BoostingTC,\n author = {Bohang Zhang and Du Jiang and Di He and Liwei Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Boosting the Certified Robustness of L-infinity Distance Nets},\n volume = {abs/2110.06850},\n year = {2021}\n}\n', 'references': ['4de70afa21e54e8d8f7925471384c28d5c685276', 'a3c052386f0fae0c84c6743271ddb7a938fd755c', 'd4aa4fd1d0ea6da1905640adb17c67db435f9f12', '37613cdd48d6e32d995bbd2dc2e8e3902892dd76', '5ae274cc9ca0fc8a3c089d7320d103f0876bde4f', '9f0e0a59a4b3d689df8470b1218d2574244c26d6', 'baee6bd21ac34e4e96479b928917e0c4f78c9c14', '275588741254b9c2e7f1048d66c138f8abec02b9', '1bfc6f9c9db5c6646f0f3e0213d407ae14d8f7bf', '04eb7cc8b9a84999ba45d56bf87b170c9ad8082e', '525d1f68539c436072a3cb6f3b8a88e3b124260d', '8f3ee84811064fba1ab9b86d4f4bd39036263cef', '62cf842a62bf9c78ec40faae72b60398dc87a576', '62a88def88f4993803596dade79884fd3d811bed', '764eff31d9596033859895d9513b838d2c57a6fb', '71ea8f105803703893b5c2d01f0c9508643b6554', '1b434b5cf7b2d139f9576e29c2c8daec01fca27e', '18939eadc9c4460c8385e0591cde214a1ead067b', '18a9bb863e3110e2e981b53618b214585a32f877', '7ea9ff6dbf22ed2327eb44e04412dddb443e41c5', '75170439ccfe2271367e4ed7298f360b0443fde2', '58c143069444c7dff4be53531a47efefc40be497', '7d8472fe362b829dc105cf63a905339ee72e630d', '3f0ed6866620f76cffcb4b3653d9161a2d4aac5a', '4136cbc5f7f1fa34b91bf7bd335b173afaaf68d6', 'a3123379e326e585919f360429d77f1026ec929c', 'ff22e140a0423f1cf0595d213f36402668084014', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '5473175211aff4a8a099c44d1a57802d1b7ecf9e', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '96c82727dd5a80fef93007f888bb8569feb6bd85', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '93314b89c218c02cc1a32cad7071215693599907', '7ad8c18994108a630c4564400f6137bf4d8b7818', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', 'ecba2826cd7a51d4d8b9820591ff0fa6b41d66a6', '3a606480406886742572a956e221e986c65d94c1', 'f0ded4902d7f9c111e50047f8c9494effb7282d1', '98cc371f4e3a39b5c69b4e8980a5990f9011f223', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', 'de49430578bb3f8de3e610423255662c45f17610', '5548307d3ad1c2ec777e5084ecd478964da3947b', '75339d34bdac0d21a41461228ec6088eecdf857a', '20f85256555ad612148e52f9363e52f9d661728b', '1b9c6022598085dd892f360122c0fa4c630b3f18', 'd21fde0f55ee0285c66334d37b8920c867959784', '9db631435f7f79646a4e0a1841fbeb3340e44261', '797e841a06e2f57163b86c24942b1e043fd3ca3e', '8d35663a80199b173d8cbd12dbf2300a9f86a021', 'ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '0bd8c29a206c46dccca63c010a95734018c98d2e', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'f07c036a26bfca2b043f7c85f0326b177cd5561f', '99cb08c76c120599abd1d1637e32aaf577f38d39', '013efe3ff541e518c51f08d1b62a62e0c57c0b14', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', 'cac705d5bf752525f0a517ec7f34cac359cfc7ee', 'f6aee5a366a798ca08c12013cffcbb78740bfd01', '42371cfea5154e77a5057cafde7dc00a6446ea16', '7edda0f7cbbe47c66b8a231ecf50342cef3a8504', 'bb92676f9ec13783ac664c268191f20944718f95']}
{'paperID': 'a35bd2ddff96d876a0462acfbab0c3714dd906d0', 'abstract': 'Due to numerous breakthroughs in real-world applications brought by machine intelligence, deep neural networks (DNNs) are widely employed in critical applications. However, predictions of DNNs are easily manipulated with imperceptible adversarial perturbations, which impedes the further deployment of DNNs and may result in profound security and privacy implications. By incorporating adversarial samples into the training data pool, adversarial training is the strongest principled strategy against various adversarial attacks among all sorts of defense methods. Recent works mainly focus on developing new loss functions or regularizers, attempting to find the unique optimal point in the weight space. But none of them taps the potentials of classifiers obtained from standard adversarial training, especially states on the searching trajectory of training. In this work, we are dedicated to the weight states of models through the training process and devise a simple but powerful Self-Ensemble Adversarial Training (SEAT) method for yielding a robust classifier by averaging weights of history models. This considerably improves the robustness of the target model against several well known adversarial attacks, even merely utilizing the naive cross-entropy loss to supervise. We also discuss the relationship between the ensemble of predictions from different adversarially trained models and the prediction of weight-ensembled models, as well as provide theoretical and empirical evidence that the proposed self-ensemble method provides a smoother loss landscape and better robustness than both individual models and the ensemble of predictions from different classifiers. We further analyze a subtle but fatal issue in the general settings for the self-ensemble model, which causes the deterioration of the weight-ensembled method in the late phases*.', 'bibtex': '@Article{Wang2022SelfEnsembleAT,\n author = {Hongjun Wang and Yisen Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Self-Ensemble Adversarial Training for Improved Robustness},\n volume = {abs/2203.09678},\n year = {2022}\n}\n', 'references': ['dcfb420412d76600eb124625f62fb28499af1e8c', '83d68fb9906d17c3d623ddbd70bc36e7baee96f9', '2d36eaa618da03d28a48a03e562a9fbc314609c4', 'dc32a984b651256a8ec282be52310e6bd33d9815', '34416bf36736776715cc75859a02c1a077555bf5', 'df872e72e87a85f9b5cd28da06ace46386462fde', '762752eb9a9a92b028026b17c46d50474ddf3f06', '3e577c9bdc82cb7fed337a74f90bbc4505fdfb69', '8e7bc6ea46f0ccb3c86c7795af95a33799f71883', '1bcbf1efb3f81f0e777b4b754cf5b9789841d12f', '99a599d8fe56529f47e78243ed61250190f96196', '289db3be7bf77e06e75541ba93269de3d604ac72', '764eff31d9596033859895d9513b838d2c57a6fb', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', 'b852634098dd8c1fcdfc3c96c86d599d47f7c302', '18939eadc9c4460c8385e0591cde214a1ead067b', 'b27da51d2b33c67b1b366f6f3a1e61e84dbab230', '2eda2921a8da4b325f9d05f556594a5884c398a7', '8733fe2371b615609b04e2e910b1ecfa8e77cbc2', '1067c814c5d517cd50af176f3c919493fa799c0f', '2172552b917ef3757b0af47d17fce18586d56cba', 'f368de6a7f90daec66e1eef7922773390b75fb9d', '676e40050453ddeb1387f8314478c0ac3681a8c6', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '93314b89c218c02cc1a32cad7071215693599907', '22aab110058ebbd198edb1f1e7b4f69fb13c0613', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', 'f6195d8dc6aad8231e97b563246f2585842bc68b', '6baca6351dc55baac44f0416e74a7e0ba2bfd03e', 'ca9c1224636b0a7dd37340a4691c34a9914b5af8', 'ac8e45a0451ac578f17f631fc2663ee4b98b83a9', '8e37a3b227b68953f8067215828dc8b8714cb21b', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', '4b1c6f6521da545892f3f5dc39461584d4a27ec0', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', 'abf38db4775bec89c950013030d8eda56a89d32a', 'da1231a3a7536010ddb6ef5e163a785d03974af1', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '4361e64f2d12d63476fdc88faf72a0f70d9a2ffb', '1c4e9156ca07705531e45960b7a919dc473abb51', 'f701b58e41d928cdcd8d733b638fd65a73623b72', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', '9fca2af9a0e3f2c5c3ed47abb3ebd21b7265ac2b', '4d4d09ae8f6a11547441f7fee36405758102a801', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '6fa5d3508788f1ec9973d44f65b207092f91298f', '009f35c0e453f2435efd8d8ef8086b76b294967a', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992']}
{'paperID': '68b7532be018dbaf4fe7f500b19b46fd31b82ab9', 'abstract': 'The adversarial vulnerability of deep neural networks has attracted signiﬁcant attention in machine learning. From a causal viewpoint, adversarial attacks can be considered as a speciﬁc type of distribution change on natural data. As causal reasoning has an instinct for modeling distribution change, we propose to incorporate causality into mitigating adversarial vulnerability. However, causal formulations of the intuition of adversarial attack and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and deﬁne the adversarial distribution to formalize the intuition of adversarial attacks. From a causal perspective, we ﬁnd that the label is spuriously correlated with the style (content-independent) information when an instance is given. The spurious correlation implies that the adversarial distribution is constructed via making the statistical conditional association between style information and labels drastically diﬀerent from that in natural distribution. Thus, DNNs that ﬁt the spurious correlation are vulnerable to the adversarial distribution. Inspired by the observation, we propose the adversarial distribution alignment method to eliminate the diﬀerence between the natural distribution and the adversarial distribution. Extensive experiments demonstrate the eﬃcacy of the proposed method. Our method can be seen as the ﬁrst attempt to leverage causality for mitigating adversarial vulnerability.', 'bibtex': '@Article{Zhang2022AdversarialRT,\n author = {Yonggang Zhang and Mingming Gong and Tongliang Liu and Gang Niu and Xinmei Tian and Bo Han and B. Scholkopf and Kun Zhang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adversarial Robustness through the Lens of Causality},\n volume = {abs/2106.06196},\n year = {2022}\n}\n', 'references': ['8f566001453bc6be0a935bf69ffd90d9db3af32b', '57835c5ad5424f94ee75901c3113730f3900e656', '99a599d8fe56529f47e78243ed61250190f96196', '24a07bc3826ae3518ccbd0e004c65319896e0c5d', '6d0036bae18aa441a19b63fc4b2daf91f63e8029', '1a6adb28532afd81b32bf5b9e0af23bb3a4f0dc8', '764eff31d9596033859895d9513b838d2c57a6fb', 'b311dec6aa6963e51c21fc9a89988a7c369145bc', '7378f30cd38496acf315bb18fd64e468f0f8001e', '2eda2921a8da4b325f9d05f556594a5884c398a7', 'd5b84236178d7805c2e7b503cc6cf4a24b7da626', '58c143069444c7dff4be53531a47efefc40be497', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '38d28841d70a9d322f02268aaf218b5f33592de1', '6d4a87759917132913319960389f17fa1fe8b630', 'add2f205338d70e10ce5e686df4a690e2851bdfc', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '4f8c49cd3eccc2417c78e0b310698d9f603aa2e9', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '3f7bc67330b3eff749459568e7995f0017dfe645', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', 'ac644a74a0ebc8cfbe1b0af8120004909828d283', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '41071dbbbcbb27af3fec70de045f19c28535f5b7', '3af6d1113cd45ccbf4ecf710d6bb491436dd277b', '715a73290f260cf2196307e59fe0b6776841f170', '60bc5831f9aea45bc63608cb1af74cabadc39eb7', 'b043ab547742422c88b8f6518d0eba3f2f5c0a76', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', 'e02ae07b45a2241f7ee1180b446ed7ba208c51e8', '4b23012689e0f17912fb38d4984775e567cff8d6', '8e37a3b227b68953f8067215828dc8b8714cb21b', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '1d6bc45c31c17f5091eec3def813cc2cd26d811e', 'ff7bcaa4556cb13fc7bf03e477172493546172cd', 'e2a85a6766b982ff7c8980e57ca6342d22493827', '16aa01ca0834a924c25faad5d8bfef3fd1acfcfe', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '1c4e9156ca07705531e45960b7a919dc473abb51', '53b047e503f4c24602f376a774d653f7ed56c024', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', 'f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6', '0c908739fbff75f03469d13d4a1a07de3414ee19', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'e15cf50aa89fee8535703b9f9512fca5bfc43327', 'eb42cf88027de515750f230b23b1a057dc782108', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '66c3d69f94c90a884d3f6b5367813d51708f6ded', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '3d985d72a4ff6232503b5a95fa6f3eefbab01cef', '5d90f06bb70a0a3dced62413346235c02b1aa086', '792695c436fd0148a71e7f2830ea5bac7938b014']}
{'paperID': '51836dfa1542277ed982612caa90ecf31ead4ba8', 'abstract': 'As machine learning models are deployed ever more broadly, it becomes increasingly important that they are not only able to perform well on their training distribution, but also yield accurate predictions when confronted with distribution shift. The Distributionally Robust Optimization (DRO) framework proposes to address this issue by training models to minimize their expected risk under a collection of distributions, to imitate test-time shifts. This is most commonly achieved by instance-level re-weighting of the training objective to emulate the likelihood ratio with possible test distributions, which allows for estimating their empirical risk via importance sampling (assuming that they are subpopulations of the training distribution). However, re-weighting schemes in the literature are usually limited due to the difficulty of keeping the optimization problem tractable and the complexity of enforcing normalization constraints. In this paper, we show that three simple ideas -- mini-batch level normalization, a KL penalty and simultaneous gradient updates -- allow us to train models with DRO using a broader class of parametric likelihood ratios. In a series of experiments on both image and text classification benchmarks, we find that models trained with the resulting parametric adversaries are consistently more robust to subpopulation shifts when compared to other DRO approaches, and that the method performs reliably well with little hyper-parameter tuning. Code to reproduce our experiments can be found at https://github.com/pmichel31415/P-DRO.', 'bibtex': '@Article{Michel2022DistributionallyRM,\n author = {Paul Michel and Tatsunori Hashimoto and Graham Neubig},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Distributionally Robust Models with Parametric Likelihood Ratios},\n volume = {abs/2204.06340},\n year = {2022}\n}\n', 'references': ['9f47fe66a23dbf48d0b2fa5fb66e378a9c51951e', '714fc6626c527e05f2a31626d067d46520c6740e', '5ede529879d162d2779d410a5775d3f6cd6be3f4', '40848b41ed8c9c255ecd8a920006877691b52d03', '66488a38c3bae5d928bb22aa615fd0e64ccac62b', '8352a4ec9a0cf24b7556945743c418f84f9ed9fb', '2fbea3a1cad10c9bd5a01cd7eae6ca7f46f4e8a1', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '03700ad9bea6cdb705fe51834f46cd037c78c13f', 'e816f788767eec6a8ef0ea9eddd0e902435d4271', '193092aef465bec868d1089ccfcac0279b914bda', '0db252455fe10261c3c439e850dabccb72097c20', 'a54b56af24bb4873ed0163b77df63b92bd018ddc', '77568c594470f9aa029f92774e2c12ab0451d9bb', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', '8963317176fa81e185fd7a8f8cd001d7e11a4868', '1dcc2da3fde52cacdec926d5c4e2bb425959721b', '5c01d5f6b113713caf9d85fb127eaa376cc01673', '42ed4a9994e6121a9f325f5b901c5b3d7ce104f5', 'a1fb7236d104ae0343c1a09e3590ee2283483240', 'ce89ee7aaeeea2c9d474707690f3ea9d948776a3', '2d15a7546c16d5821ffa8f769eb7ec18e435e64d', 'fbda91cfacd2b792794fb726e9417aef58480c72', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', 'd3707cf521e3596313af1f53acba6413d0d528a6', '52f7ae53e58c5098133d041794b4465d36c2fdb6', '290af67244094745eaa927bfa8a3727e93dba78b', '18858cc936947fc96b5c06bbe3c6c2faa5614540', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '0d57ba12a6d958e178d83be4c84513f7e42b24e5', '2cf5ac39c8299a3304c6e9808593153c894ff1b5', '07b5093aace8e485e7d23b83edb6351618138127', '7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c', '41f1d50c85d3180476c4c7b3eea121278b0d8474', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '13497bd108d4412d02050e646235f456568cf822', 'e09aa121bca3a0b9a0fe5f7ca119f44791cba1d7', 'c824cd5510fecac3a03330f79c14b23856c229c0', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '583b55367f787eb0c4e295707b642e63547b9806', '393a553e3f601e1ec1a205eeb2981a8fc596012a', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'a7c183abb9b044bfbe1f09199ee970ea3a01104f', '9642a175637a400b425f0ac0cb6a2b067cc8fe6b', '006a9930546a9e1f25704a07ee8454a805fd2875', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'e10642453c5c99442eb24743c4bab60a3a0b6273', '8d56d4bc69a8c562434b9a129542bb79e9d6f1d6']}
{'paperID': '601ab36b6f077ff57472f4a0cf2e061dd05b9b85', 'abstract': "Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs trained on ImageNet are overly reliant on local textures and fail to make adequate use of shape information. ViTs thus have difficulties generalizing to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.", 'bibtex': '@Article{Mao2021DiscreteRS,\n author = {Chengzhi Mao and Lu Jiang and Mostafa Dehghani and Carl Vondrick and R. Sukthankar and Irfan Essa},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Discrete Representations Strengthen Vision Transformer Robustness},\n volume = {abs/2111.10493},\n year = {2021}\n}\n', 'references': ['cf5e6e3c50a798d87033e0e108e88b3647738bbe', '722ad6ac92286507437b31486f47987d6ece05c9', '42a7015e48a1e00b70ebb442a82afb4b10017c0b', '03db529f0bfae6d0b64b0feef565196327fe8d50', 'b8cee43a51c44f8f4448e78e41ecf081987707cf', '5e4f03f68c6867d850f457dc5cc36738e5dff6c1', '2def61f556f9a5576ace08911496b7c7e4f970a4', '6709d5583f658f589ae6a2184805933aceb18849', '739ceacfafb1c4eaa17509351b647c773270b3ae', 'd2a3bb6356d439146cd8d8e72dc728a1e3d93e7f', '610b302950a19acef1c45456111dcd495f638c18', '2cd605106b88c85d7d8b865b1ef0f8c8293debf1', '3e398bad2d8636491a1034cc938a5e024c7aa881', 'ad7ddcc14984caae308c397f1a589aae75d4ab71', '945aa2eb4b7ceecebf0562dfc12fcadb8fd38970', '47f7ec3d0a5e6e83b6768ece35206a94dc81919c', '18e2f6b0f8e5644205cecc0df7d6fe1d7105cfda', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', 'b27ad18e20d27efe8a9fbc54b1c2dcef8b2da19f', '09472ff0d3c3f975ef1fdc02cfb1605d3d4275fa', '022622e024890d6e044ac50e2da6b44c59bdf418', 'f5c8464032a936451b222be1984cabf42d6adfa8', 'a0265f14b07811c502f9dd730ec1f10daf2ff345', '76a9f336481b39515d6cea2920696f11fb686451', '1e6de530a183cded2373c8f0ffc54f4a9b7bd02e', 'cbaaa1154c491f9da2f050d3c22970e15bb7b52b', '87f6a7c014ce206ac5b57299c07e10667d194b39', '45557cc70cd6989ab6b03e5aeb787e34299099f7', 'db787640c9b42416ff8d7015546e667e58267177', 'ed17929e66da7f8fbc3666bf5eb613d302ddde0c', '4ae0c4a511697e960c477ea3e37b3e11bf3e0e02', '49b64383fe36268410c430352637ed23b16820c5', '4e0bb8c1c683b43357c5d5216f6b74ff2cb32434', 'fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', 'f723eb3e7159f07b97464c8d947d15e78612abe4', 'c940cec9b56a5766c316fb6fc1e4195d70d39ecf', 'f466157848d1a7772fb6d02cdac9a7a5e7ef982e', '4feef0fd284feb1233399b400eb897f59ec92755', 'eb35fdc11a325f21a8ce0ca65058f7480a2fc91f', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '915c4bb289b3642489e904c65a47fa56efb60658', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '25d0fa49ca846370ff4796a6ac6688a42cf50f77', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '642e328cae81c5adb30069b680cf60ba6b475153', '0def290ae38abb4a04e35e0bcdc86b71d237f494', 'c8b25fab5608c3e033d34b4483ec47e68ba109b7', '639174f32a71ecfe9041ad05ff30eb39bd4977bf', 'b91180d8853d00e8f2df7ee3532e07d3d0cce2af']}
{'paperID': '0808bdbb02e60fd3ac3c13f454346ea47067b987', 'abstract': 'Top-$k$ predictions are used in many real-world applications such as machine learning as a service, recommender systems, and web searches. $\\ell_0$-norm adversarial perturbation characterizes an attack that arbitrarily modifies some features of an input such that a classifier makes an incorrect prediction for the perturbed input. $\\ell_0$-norm adversarial perturbation is easy to interpret and can be implemented in the physical world. Therefore, certifying robustness of top-$k$ predictions against $\\ell_0$-norm adversarial perturbation is important. However, existing studies either focused on certifying $\\ell_0$-norm robustness of top-$1$ predictions or $\\ell_2$-norm robustness of top-$k$ predictions. In this work, we aim to bridge the gap. Our approach is based on randomized smoothing, which builds a provably robust classifier from an arbitrary classifier via randomizing an input. Our major theoretical contribution is an almost tight $\\ell_0$-norm certified robustness guarantee for top-$k$ predictions. We empirically evaluate our method on CIFAR10 and ImageNet. For instance, our method can build a classifier that achieves a certified top-3 accuracy of 69.2\\% on ImageNet when an attacker can arbitrarily perturb 5 pixels of a testing image.', 'bibtex': '@Article{Jia2020AlmostTL,\n author = {Jinyuan Jia and Binghui Wang and Xiaoyu Cao and Hongbin Liu and N. Gong},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations},\n volume = {abs/2011.07633},\n year = {2020}\n}\n', 'references': ['2e025461fa02b3939f151ad17690ecfe3be728bd', '60cb22635e8d05a986fa6de2fc7090a9451e2de3', '01203b7235355cbb99090afab02ab97ef1807034', 'bbcfa13ced06dedc9c346e0bdf84d8b3abcbebee', '2defb08c8b968ec2c616ed973b9950d33425a637', 'e2d09159e54a53bb3b02e9265c0e63631d8d7d8a', '4c9ee8358d82afa960708391e2b8e83c4a737ae9', 'b1ef79a7bbe51a7f1f28e403c8bde35a3b54d985', '4a08a4f5818b8ce9e4eb5c3910788b965d61193d', '7ea9ff6dbf22ed2327eb44e04412dddb443e41c5', '75170439ccfe2271367e4ed7298f360b0443fde2', '7d8472fe362b829dc105cf63a905339ee72e630d', '479240dac852bbf9cdda25e991ddadf4c24327e7', '3f0ed6866620f76cffcb4b3653d9161a2d4aac5a', '4136cbc5f7f1fa34b91bf7bd335b173afaaf68d6', '947a4b2e31dcaeffa8d86bc8d6888665ec33c5f6', '26dd808175870b5fc7426b11353acdcc0066304a', '469bae685ae7fa6dc7ff73b9076041b79aa083e0', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '50c5763d2d35f2c4eaa5cebea310faf2cf0a10dc', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '4a416cb1cac9fb4fecad49af1ff07e2523c99cfe', '7ad8c18994108a630c4564400f6137bf4d8b7818', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', '750fd4f2a6139387b4f6245d3fd1013a8c8cf702', '75339d34bdac0d21a41461228ec6088eecdf857a', 'bfb0d179916c000d54f27e7a9ea18b6269963e74', '20f85256555ad612148e52f9363e52f9d661728b', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', 'd21fde0f55ee0285c66334d37b8920c867959784', '2410923ed90b099e3f5565b63e789f10bf70ec4c', '9db631435f7f79646a4e0a1841fbeb3340e44261', '06b98537324dbf11c7de2040e519b4d110f5d622', '8d35663a80199b173d8cbd12dbf2300a9f86a021', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '2f201c77e7ccdf1f37115e16accac3486a65c03d', '8b9127bee0f7d109da2672ba06d0f39a5a60335a', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', 'a18ada04d93981178234d9c8907fb99ea92fddcb', '1cf361d02f5ad84567e48754f1a8f895653bc701', '9a089c56eec68df722b2a5a52727143aacdc2532', '4b23012689e0f17912fb38d4984775e567cff8d6', '91f4ebdfb4618e9a7bbcefc8b64e2f7d6e176545', 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd', '59ea59d73eea51f80b60ba6ea47dac0197029336', '69092affc3461a38eb05cf7982f104eb30b0492c', '45a710be199c8eb43f465c88fc4b343267c35d38', 'ee8bc379985788544e44cf63887cf75a03e08b64', '99cb08c76c120599abd1d1637e32aaf577f38d39', '333416708c80d0c163ca275d1b190b1f2576fa5f', '9f92a0ccc8b039a83bd5ba5482facb5829c712aa', '061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', 'd997beefc0922d97202789d2ac307c55c2c52fba', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '7c8a51d04522496c43db68f2582efd45eaf59fea', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'bb92676f9ec13783ac664c268191f20944718f95', 'e225dd59ef4954db21479cdcbee497624b2d6d0f', 'e34b8e55b1cd9786deaf9f89191cdb77912c87e7', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'a05f5a5c9fe1d8a44f5960571cc6f4fbb75d0d36']}
{'paperID': 'd4aa4fd1d0ea6da1905640adb17c67db435f9f12', 'abstract': 'show deep neural networks (DNN) are vulnerable to adversarial which aim to mislead DNNs by adding perturbations with small magnitude. To defend against such attacks, both empirical and theoretical defense approaches have been extensively studied for a single ML model . In this work, we aim to analyze and provide the certiﬁed robustness for ensemble ML models , together with the sufﬁcient and necessary conditions of robustness for different ensemble protocols. Although ensemble models are shown more robust than a single model empirically; surprisingly, we ﬁnd that in terms of the certiﬁed robustness the standard ensemble models only achieve marginal improvement compared to a single model. Thus, to explore the conditions that guarantee to provide certiﬁably robust ensemble ML models, we ﬁrst prove that diversiﬁed gradient and large conﬁdence margin are sufﬁcient and necessary conditions for certiﬁably robust ensemble models under the model-smoothness assumption. We then provide the bounded model-smoothness analysis based on the proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble model can always achieve higher certiﬁed robustness than a single base model under mild conditions. Inspired by the theoretical ﬁndings, we propose the lightweight Diversity Regularized Training (DRT) to train certiﬁably robust ensemble ML models. Extensive experiments show that our DRT enhanced ensembles can consistently achieve higher certiﬁed robustness than existing single and ensemble ML models, demonstrating the state-of-the-art certiﬁed L 2 -robustness on MNIST, CIFAR-10, base justiﬁcation of the regularization-based training approach DRT. Extensive experiments showed that DRT-enhanced ensembles achieve the highest certiﬁed robustness compared with existing baselines.', 'bibtex': '@Article{Yang2021OnTC,\n author = {Zhuolin Yang and Linyi Li and Xiaojun Xu and B. Kailkhura and Tao Xie and Bo Li},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the Certified Robustness for Ensemble Models and Beyond},\n volume = {abs/2107.10873},\n year = {2021}\n}\n', 'references': ['bef771d2430af7be7525201bd677e82cec38510f', 'c526dcd91152cc4b155e4e76c6ee3ec931b321df', '3fa6da03eb6c4d1ecb5560ffba299e7cc8826477', 'ebb3f729b9a12a4af3f82c44599812e66580922e', 'b3b88cb29938a5445edd543b0498a51c4931f840', '06aaece45f8284de309d4d9d8772305fb848a66d', '14becdb8b29de410fc54cd80307dc08512391edb', '3ac7669e6e4488080d046a9c07078ffcbaddbab1', '62cf842a62bf9c78ec40faae72b60398dc87a576', '6d0036bae18aa441a19b63fc4b2daf91f63e8029', '0281b89a956906b7f78a5c964ab1a9b93ac8409e', '6cfca09be8522e56c4aa75e7a86dc36d505f9bda', 'e2d09159e54a53bb3b02e9265c0e63631d8d7d8a', 'b1ef79a7bbe51a7f1f28e403c8bde35a3b54d985', '18a9bb863e3110e2e981b53618b214585a32f877', '7ea9ff6dbf22ed2327eb44e04412dddb443e41c5', '75170439ccfe2271367e4ed7298f360b0443fde2', '58c143069444c7dff4be53531a47efefc40be497', '4136cbc5f7f1fa34b91bf7bd335b173afaaf68d6', 'ef042af146283c59fa9e9990ce37df538fc12faa', 'c9d239db4ab86522a6fdecb86116d1083a48823c', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', 'd66c7ec5cdbf4df77789748d9173e2c4775933f0', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'd4473a41c9f7b4095599bec14ea0a88e7041e737', '676e40050453ddeb1387f8314478c0ac3681a8c6', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', '8bac2716cd208cb8041650a001ab72ba81b559cd', '66f72219e9870250b67bf70833bbcbcd163365a3', '5548307d3ad1c2ec777e5084ecd478964da3947b', '75339d34bdac0d21a41461228ec6088eecdf857a', '9db631435f7f79646a4e0a1841fbeb3340e44261', '8b9127bee0f7d109da2672ba06d0f39a5a60335a', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', 'a18ada04d93981178234d9c8907fb99ea92fddcb', 'd3c071dbbb4520ed5875f7e064a9da87240534db', 'd5577abcc1fbf57d66017e3b5b2211a82022842c', '4b23012689e0f17912fb38d4984775e567cff8d6', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '78aa018ee7d52360e15d103390ea1cdb3a0beb41', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', '177bc509dd0c7b8d388bb47403f28d6228c14b5c', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '75bc3edb3655a777b8a12e0237c18d571aa610fd', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '802f2afd13b209d4c6d6d3622efc48a7256d7207', '1a585a498551e9ba55f89207f8a735cfd79cf807', '4af31c2819d3f2c7f01942f053750ad1a87253db', '166c42895882039e4252f7c943efa13d0505109f', '43bbc1b737b5f29cd58cf80de0e6378ced93381a', 'a2a591d5a8d399b14d5ab9171629b7b9eaae05ad', '37eb32915b7767685ec3c9e9728ca9a50a379b8d', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4', 'a05f5a5c9fe1d8a44f5960571cc6f4fbb75d0d36']}
{'paperID': 'f1057950e69b736c35a4fe9c013083d6118811ed', 'abstract': 'Robust subspace recovery (RSR) is a fundamental problem in robust representation learning. Here we focus on a recently proposed RSR method termed Dual Principal Component Pursuit (DPCP) approach, which aims to recover a basis of the orthogonal complement of the subspace and is amenable to handling subspaces of high relative dimension. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers, as long as the true dimension of the subspace is known. We show that DPCP can provably solve RSR problems in the {\\it unknown} subspace dimension regime, as long as orthogonality constraints -- adopted in previous DPCP formulations -- are relaxed and random initialization is used instead of spectral one. Namely, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach will succeed with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of PSGM algorithm that allows us to perform RSR without being aware of the subspace dimension.', 'bibtex': '@Article{Giampouras2022ImplicitBO,\n author = {Paris V. Giampouras and B. Haeffele and R. Vidal},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension},\n volume = {abs/2201.09079},\n year = {2022}\n}\n', 'references': ['d33b2544781c15e257ee8a1c3bf2e2d7307ebfef', '7cf8ae0fbbd22b8c935e8d7af675dd4673c36e78', '06a17b17268551232123ddded65e5b46d65d93cf', '15411d45ac13f432c3505c3ce825dfa00d88f8a6', 'ddbc185ed132f09673ef048aab0a10a25ee6d27e', '33416f2dc49db24cca520a3b234f02463a4e833e', '14c7d66a3ae24949ec01d51e4095e5141550f824', '65f233478263b77b48dc435d73bf3647f4ae4027', '13eb4d8d61fa320fde0d049a6b4bcde6e550118f', '5bc875d65df812f9617d8ba508c1c85f4d219b19', '9dc822d8c41d1091b3ae200fe5efa6f98bd4ecf3', '9c7da2c47c66afe9a723af75908f0b5c45832e61', '67d77348984b1793ebc65be0684a50bae90bb39e', '07f87b91ddabb7f71f43f20ab1b7d572904fe801', '4406c54f40e0f73db2180704d454951649df32f2', '01625cba9f8a783994377d4f35aa765242faab4f', 'c8831d7d318b8d59f9b958d250a58f253f08bd8a', '4f37468a95ccc62debb9e5a4cb0d73489ca61190', 'c0962af9155f6cd146e6566d3b8f7f7c04df9d98', 'b45a4b9bb65c64cf9eb1adcc08aa2e71cdf54718', 'c72572e628004e83ffafb362a1aa6a37550a1bf3']}
{'paperID': 'ca1a8b92898c4e57aebcbd6c6f57e1cb9a4d0804', 'abstract': 'While adversarial training has become the de facto approach for training robust classiﬁers, it leads to a drop in accuracy. This has led to prior works postulating that accuracy is inherently at odds with robustness. Yet, the phenomenon remains inexplicable. In this paper, we closely examine the changes induced in the decision boundary of a deep network during adversarial training. We ﬁnd that adversarial training leads to unwarranted increase in the margin along certain adversarial directions, thereby hurting accuracy. Motivated by this observation, we present a novel algorithm, called Helper-based Adversarial Training (HAT) , to reduce this effect by incorporating additional wrongly labelled examples during training. Our proposed method provides a notable improvement in accuracy without compromising robustness. It achieves a better trade-off between accuracy and robustness in comparison to existing defenses.', 'bibtex': '@Article{Rade2022ReducingEM,\n author = {Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli},\n booktitle = {International Conference on Learning Representations},\n title = {Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off},\n year = {2022}\n}\n', 'references': ['2aab97e35c43d961d645e650808d5b052ec180ab', '1bcbf1efb3f81f0e777b4b754cf5b9789841d12f', '99a599d8fe56529f47e78243ed61250190f96196', '764eff31d9596033859895d9513b838d2c57a6fb', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '1b434b5cf7b2d139f9576e29c2c8daec01fca27e', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', 'b27da51d2b33c67b1b366f6f3a1e61e84dbab230', '65c63d4143b70ba718c423743bb1a4c43513e7fc', '6d4a87759917132913319960389f17fa1fe8b630', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '49b64383fe36268410c430352637ed23b16820c5', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '5ae786deaa875613e85ed2df0dbeec4301109f74', 'd6263d976be8753c4c6779eb8e986311a23b6cbf', '1b9c6022598085dd892f360122c0fa4c630b3f18', '804fb9542f4f56e264dd2df57c255a9a2011c00f', '9e21d177a7dcfa4acfb674b93103b3d12bbb5b32', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '136dee73f203df2f4831994bf4f0c0a4ad2e764e', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '0c908739fbff75f03469d13d4a1a07de3414ee19', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '54d2b5c64a67f65c5dd812b89e07973f97699552', '62a6316235d36b71139579ad0588c9cfef16b9e9', 'b245959da6bdaa0b711341844aeaa473b7706453', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', '8d3a318b62d2e970122da35b2a2e70a5d12cc16f']}
{'paperID': '8bcb5534227214b83255f5b9dedbc0d46a44794a', 'abstract': 'While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to 7.5% and 6.7% in $\\ell_{\\infty}$ and $\\ell_2$ threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by 7.6% on the CIFAR-10 dataset. We further demonstrate that different generative models bring a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact of individual generative models and further provide a deeper understanding of why current state-of-the-art in diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.', 'bibtex': '@Article{Sehwag2021RobustLM,\n author = {Vikash Sehwag and Saeed Mahloujifar and Tinashe Handina and Sihui Dai and Chong Xiang and M. Chiang and Prateek Mittal},\n booktitle = {International Conference on Learning Representations},\n title = {Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?},\n year = {2021}\n}\n', 'references': ['ccbadf4270417de29c4c6805a58e7b0ec819d751', '762752eb9a9a92b028026b17c46d50474ddf3f06', 'de18baa4964804cf471d85a5a090498242d2e79f', 'b4beb15b524c583cd828300605bab66dc3caf386', '68118a9d2c29bd6bb6565cf21381ecdf1940a7ee', '57acaf4538d1a6e26c77cfae5640e359e763952e', '2aab97e35c43d961d645e650808d5b052ec180ab', '1bcbf1efb3f81f0e777b4b754cf5b9789841d12f', '014576b866078524286802b1d0e18628520aa886', '67f74fe9d46f88661573003f8f1f12967ae49fa3', '78ba6127fabb7056afc6f97924852bdd8b653b71', '569ef4e3c9f5ae968fc94000c12d212a2b679907', '289db3be7bf77e06e75541ba93269de3d604ac72', '670f9d0d8cafaeaeea564c88645b9816b1146cef', '4c0b4fe0fb05daba6deb12cb042d8ba2829c853d', '29858b40a15704398aecdca6bd2820f2fcc99891', '71ea8f105803703893b5c2d01f0c9508643b6554', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', '3805147a98dab8f0c7667fed25490adbd2300fbd', '58c143069444c7dff4be53531a47efefc40be497', '755a392fe813c9ba3282f60c0e1f1ec81e68f263', 'fbc5486a1ffb9039dbb5046b84f0eb32e4ce8eea', '0495d9df8eb84dcdab4e5536179823cd26279949', '5474ddca920f59c4ec3c243345a5b9248e64065b', '3297a6f172904430d5b4c6db84ad1a26cb706a1d', '2f255dcc2f28c11ea3b2888f537dbf9916abcc02', '4538789bb5a4b3fb67123d905692800908061724', 'f3c7e853bb77d1ad360464aea81676cc9e3ca1fe', 'ff22e140a0423f1cf0595d213f36402668084014', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '63c022ae3b385d1d49c119142bfabb5cdb5ec90b', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', '30fabb3369cda7c1d515c87bf453a3ebd61e149e', 'ca42e4d7021d4e563bbeae7db35c1ce09fe38bfa', '0d94293388417458eae73632e33840a772375900', '3f7bc67330b3eff749459568e7995f0017dfe645', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', '49b64383fe36268410c430352637ed23b16820c5', '9ae649ab5201e06dbef50f005a5a35705a04bcdd', '4e0bb8c1c683b43357c5d5216f6b74ff2cb32434', 'fa2bfdf70e43b84a632b679a8f35999816245919', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'b4c1cd3f391c34bab08d6094c91793cb59f2da81', '22aab110058ebbd198edb1f1e7b4f69fb13c0613', 'af1841e1db6579f1f1777a59c7e9e4658d2ac466', '88311ee3fbb9d8d307386c0fb53aaa0283c5eb74', '20f85256555ad612148e52f9363e52f9d661728b', '0f885fd46064d271d4404cf9bb3d758e1a6f8d55', '804fb9542f4f56e264dd2df57c255a9a2011c00f', 'f7bb1636ced9036b3d0edafc7d82ad43164d41a3', 'e77171024bf7dc2ff33db89710f1184543c694e5', '651adaa058f821a890f2c5d1053d69eb481a8352', '2d015c62f258eaad5b578a3d9bef762e7943ec47', '2c20e7220269b28fb1935a83d0e7f2db330aa691', '4b23012689e0f17912fb38d4984775e567cff8d6', '8760bc7631c0cb04e7138254e9fd6451b7def8ca', '231af7dc01a166cac3b5b01ca05778238f796e41', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', '1c4e9156ca07705531e45960b7a919dc473abb51', 'a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '1ced31e02234bc3d1092ffb2c7442ffbd51cb309', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '0302bb2d5476540cfb21467473f5eca843caf90b', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'deecd9ff87d3f7daac4dfb2057335f7190287e93', 'ca1a8b92898c4e57aebcbd6c6f57e1cb9a4d0804', '660eb0ec1d00570c92eba6d3e2416ed7d45c2002', '0e0c4a16e11b843f2b5d841ddfb60e97a376d5af']}
{'paperID': '7d56ea205f9a8f83225569e3acf4c6c8c5b0977e', 'abstract': 'The lack of adversarial robustness has been recognized as an important issue for state-of-the-art machine learning (ML) models, e.g., deep neural networks (DNNs). Thereby, robustifying ML models against adversarial attacks is now a major focus of research. However, nearly all existing defense methods, particularly for robust training, made the white-box assumption that the defender has the access to the details of an ML model (or its surrogate alternatives if available), e.g., its architectures and parameters. Beyond existing works, in this paper we aim to address the problem of black-box defense: How to robustify a black-box model using just input queries and output feedback? Such a problem arises in practical scenarios, where the owner of the predictive model is reluctant to share model information in order to preserve privacy. To this end, we propose a general notion of defensive operation that can be applied to black-box models, and design it through the lens of denoised smoothing (DS), a first-order (FO) certified defense technique. To allow the design of merely using model queries, we further integrate DS with the zeroth-order (gradient-free) optimization. However, a direct implementation of zeroth-order (ZO) optimization suffers a high variance of gradient estimates, and thus leads to ineffective defense. To tackle this problem, we next propose to prepend an autoencoder (AE) to a given (black-box) model so that DS can be trained using variance-reduced ZO optimization. We term the eventual defense as ZO-AE-DS. In practice, we empirically show that ZO-AE- DS can achieve improved accuracy, certified robustness, and query complexity over existing baselines. And the effectiveness of our approach is justified under both image classification and image reconstruction tasks. Codes are available at https://github.com/damon-demon/Black-Box-Defense.', 'bibtex': '@Article{Zhang2022HowTR,\n author = {Yimeng Zhang and Yuguang Yao and Jinghan Jia and Jinfeng Yi and Min-Fong Hong and Shiyu Chang and Sijia Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective},\n volume = {abs/2203.14195},\n year = {2022}\n}\n', 'references': ['21e6dd66d5cb82c5cd19d86bdacaaa9bce336a07', '6d3fc40b741054422acf55a26c756d7e61e706f3', '12cdc1423ac0eb27a26d3883cf485b2f0a4f4e50', 'b4075b25bf107270fc589784aaa6d933c1ce918d', 'ebe970bbf1ead67dfb2b857173fbc79bb0edb161', '9afd4501100f3abbad90f570331ebe09d3e375c4', '3a0c1300a72337871bd60178e1347084fb8bb5f0', '5ad2a01ffb5408fdf14e442f80279a6e84159d0c', '32e0f1bf36ccee6ed552909c2c76f9d6b1c760ef', '24cf86a418c9471e8001961c87697c825f0bba8f', '9d13c6b39f7ea80b5b91dff29f5e682ed1436893', '9848db2098c0d3db6ee6f10a177402bc4ec67f83', '102a1eb3cdd2014caa8f523093e192f14017024a', '8cc7f2d88d193fd8ecf1fdfa0a6345c3e79e1887', '962a8ffc7d72990a28d505f49a39108b4803c223', 'a33daa0f2ed0e5bdc610be01d3ba014a2a8458d1', '18939eadc9c4460c8385e0591cde214a1ead067b', '0a06a8da7667a70bf1f3836f7e7ca72b5ab72176', '58c143069444c7dff4be53531a47efefc40be497', 'e5e41ca6d5ebbf5f8ef1c77791fa4c75ef1ceb1b', '6d4a87759917132913319960389f17fa1fe8b630', '8403eb1de9cd5ee56f8976d6b9846cb4adc282cf', '3f2095bbbf3e47fbac26da70e95a219c23e3bac9', '3d2cfca77ebe773532f9f178b726c20fcf6d4ee4', '358b779448da4e5b57f39bf8e58f70c4d2656fab', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', 'd33deae7f654b07ac8a5c437a4fa018c29e6af17', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', 'ded6d20b751a7b0a8b94175c12b64a40904d80d0', 'ac644a74a0ebc8cfbe1b0af8120004909828d283', 'f8a2e5ebf96f55035ebf215156c87a5a9b3be1c5', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1', '10ab21b120e305b6d3cbf81c5a906d36521152f1', '41f48a3b065cb038fa98f91c4775bf1a91f47764', '20f85256555ad612148e52f9363e52f9d661728b', '60bc5831f9aea45bc63608cb1af74cabadc39eb7', '1b9c6022598085dd892f360122c0fa4c630b3f18', '65f0e9db55f498bb196de3950393f5ded14bcc72', '9db631435f7f79646a4e0a1841fbeb3340e44261', 'b3f83e8416010e9c3a705a0b6390d268e5ddf5c0', '8d35663a80199b173d8cbd12dbf2300a9f86a021', '651adaa058f821a890f2c5d1053d69eb481a8352', '29176632807b17bf3da444713763b4b2b568306c', '966e3c7a65ec75a6359b55c0cecaf3896d318432', 'e3b17a245dce9a2189a8a4f7538631b69c93812e', 'e8b9fa6f9e0b606ff335a0557a838dea2696b084', '4b23012689e0f17912fb38d4984775e567cff8d6', '91f4ebdfb4618e9a7bbcefc8b64e2f7d6e176545', 'c02596cee34919daeaab1beddd813f23d429973a', '9ab7319dbe80549ba80e3320d0546d741a7a5791', 'd295a620fc10a7a656dc693e1b1bf668d1508a8e', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '63a010c69f00e65c946a68b546bbd42cbed03564', '333416708c80d0c163ca275d1b190b1f2576fa5f', '9f92a0ccc8b039a83bd5ba5482facb5829c712aa', 'adb762a645b72fc4605e6fb512ef2684db37cc93', '0a77313fa10a864e14f538c73d417d7b4d6f320e', '061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '0c00a328fa7cd56ee60338c54e89bd48310db80b', '156151365037ef177ab59db1faf7b2f2eecfadcf', '18168aea48a22f6fe2fe407c0ff70083cba225a7', '53b047e503f4c24602f376a774d653f7ed56c024', '819167ace2f0caae7745d2f25a803979be5fbfae', 'd1b9a3b11e6c9571a1553556f82b605b2b4baec3', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '8424a9e5a4456a2c45a42e392b9c01cd0c5c9467', '3ae1544865a18d8649a5c4939f9eb17165b23bea', '3095ed47bf1d7218422cd842707b558b7c9789e1', 'e225dd59ef4954db21479cdcbee497624b2d6d0f']}
{'paperID': 'a9c13ada98fa493dd3552c794fdb3af9e9fbd523', 'abstract': 'In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to significant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the first to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.', 'bibtex': '@Article{Karimireddy2020ByzantineRobustLO,\n author = {Sai Praneeth Karimireddy and Lie He and Martin Jaggi},\n booktitle = {International Conference on Learning Representations},\n title = {Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing},\n year = {2020}\n}\n', 'references': ['46b990e2a9797214b185ff61839d137c2a1c0ddf', '029c33e39236f01e83a952c4203d86e07a6c1532', '693eb9ecd819712abb40cc6c059aa84c84d354bb', '2cdc50b5b4f51a9cf08a014d0ac9f8ca3a7523f5', '0a93fce82afa33b218728a19d19d0ae40401b396', 'c471da6f4a796992e6d2c5be40f026cb176c6de6', 'fc4e43efe5c005a2a57a82debdacb9f1fee3c30c', 'daf0e811fbbb08ea8263ecc57747a572fe3d6b06', '28b969d7801a3696d379fbb85a307c52284fa52a', '6629248b79ad3e87d7f9f5694a315613ed201cca', '57501373b7411336d1f97f5d431346e77b2bb2b0', '07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', 'fb4098dd30489715a7adc6a1f7839e283d1e37ee', 'a95d102ed27f62cf328ab7c5a8732502f2b69012', '4f783752a59c28df08bad9b22dd9c7bafe4efb08', 'c50e20273ac64bf59d083ef95a244ed516a5b2eb', '1d0e74bcd599a333cd1568b74c5cfe365601987a', '8f36274384f69e7e03a8a053b6ef0aed286e6caa', 'f422b11a0b56ad4b0abba81c9fcadaaee0af4067', '6df8e13f9f3a620069b4eb6148d2b6ed5f2e46df', 'c508f8ce3c84de19e41325be10ae6f9118534084', '79cf9462a583e1889781868cbf8c31e43b36dd2f', '7c22a6a07e89461178b794681c675b209332ee15', '6c66108edb9af0533309055e7b2ecb8922db03d8', '047a662fd3e5772a59d7f93f2212b8b429cfdbc6', 'dbca9dbe14e9933515d2005dc1163ae2c24d9afd', 'e0e152748ea0badfbd798dfed3ac743abb58af26', '14d8b4fdb0262c30ae9afe20ea8e7227b115c63e', '0a4230af7869cf63159a444510627b4f91e38eed', '31f8806397907e197ca1d3676f598fd197087ad6', '293346ebd2285e3ecbb297a2773830dddc4c0a34', '5ad1cfdc40f58c5ee078496312798f784fc80801', '81af3058f2df78be332f50ea3813be5aa1f02f58', '9583ac53a19cdf0db81fef6eb0b63e66adbe2324', 'b107a64c9dc343ac35231c7959090bbc3059203f', '7b9d0aeeb280ef1fbc53a90c943270184c984fd3', 'd1dbf643447405984eeef098b1b320dee0b3b8a7', '24aa2f1004aab06000f24d5b650b891d6dc68818', '6c666b51226637495953c0dfa8ba9853f2601aad', 'adb542bb749073d80af52f2038ad6980e3874337', '3e0080a34eca4eabb9b371c2b3c369dc4dc90112', '5273a3706a1cb40426a110b09eef7ad3ad684321', 'e175f23a38f7d589266b6e059ff8e2782d9a365b', '162d958ff885f1462aeda91cd72582323fd6a1f4', '3e00dd12caea7c4dab1633a35d1da3cb2e76b420']}
{'paperID': 'b284afe9a7363b898661c9b3cfb7f015b158cc63', 'abstract': 'Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that standard maximum entropy RL is robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require adding additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our theoretical results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL does possess a striking simplicity and appealing formal guarantees.', 'bibtex': '@Article{Eysenbach2021MaximumER,\n author = {Benjamin Eysenbach and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Maximum Entropy RL (Provably) Solves Some Robust RL Problems},\n volume = {abs/2103.06257},\n year = {2021}\n}\n', 'references': ['f56fecf83a24abaa4c6a7f132fa9cfb314ed1da8', '7e1d5025c55e068b3645ff326c316ab4f31b72b8', 'e4609613a9efb85103424abb0cb15e814464ccd9', '431dc05ac25510de6264084434254cca877f9ab3', '96055d058984b15a9b83024bb2e07292ee7559f5', '5015e3f9220b5569d21a5cd0ed2bd10c1c621693', '97b237ccba2fb7bf60acd5a9ee1b4adba947f61d', '1b7c242baf54b5bda48e8f2ed1259ee4b3116686', '1233591b1feebf8ddd02b015ce1cc69305c6ace0', '3d89830c0dd84bcd42b202c402aba1e0286015e7', '207c7b8ea8f94463383a089e4f7f24b64503f9c0', '976e128c32007cdaf95a4e278a1ea7aa33a2e5cc', 'fcb4d00462eefa53a496b45acb87fa0d258d3500', '46712dd04ab67286efb47a8c072360d0c25946a6', '0bc855f84668b35cb65618d996d09f6e434d28c9', 'e0889fcee1acd985af76a3907d5d0029bf260be9', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '2ef480614f3bb9034c4796404febd7f64bfc48fe', 'd638241c8df4f015d894645ed2955afcfa2495fc', 'b3b3d1d6d36ac203cd06c00bb37e66c000430275', '46c5288bfddbe2dad70f846d1e69a37052b5d77e', '3fa50569925cfecc66fed5ec616682ecf3794ad7', 'bcdb21ca1703fc6f62df420626e36d138480a6a1', '12c0751b4f51ed833172a713b7e32390032ead93', 'ef2bc452812d6005ab0a66af6c3f97b6b0ba837e', '2ed619fbc7902155d54f6f21da16ad6c120eac63', 'c4955faa27e082a80504285c28324c58eb52250c', '56eca9e361f6ab91b7eb9dad051a2c8a4e652e07', '60fc8a885083f74c7a0aea829c81a92f2107e4d1', '6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba', '36f7f407bbad234929c69c0dd3bdcfcd80298c7c', '3fee7b836b71125a5f6a3696b9c383dae18c21e8', '4debb99c0c63bfaa97dd433bc2828e4dac81c48b', 'aa42316a94e55edf562a816c3fe7520ac667a2e3', 'a8ef08940341381390d9a5672546354d0ce51328', '811df72e210e20de99719539505da54762a11c6d', '31c8082ac852693431b53afcdc3ea97ed7974e9a', '59094d64844ee21e32560fb08db6d53cc3af0c51', '0af8cdb71ce9e5bf37ad2a11f05af293cfe62172', '7a4193d0b042643a8bb9ec262ed7f9d509bdb12e', '2e7d1e21409a90e66106722506aeb434ee7a18f3', '9c4082bfbd46b781e70657f14895306c57c842e3', '96a067e188f1c89db9faea1fea2314a15ae51bbc', '9172cd6c253edf7c3a1568e03577db20648ad0c4', 'f7ac2479e686eb2a7a8afc23f99f213fcd3c5292', '9228fa3b363229780da4cb1d258942e0c13c2947', 'e86f71ca2948d17b003a5f068db1ecb2b77827f7', '7c6409ec154ba64f5eb63d8c6e9f419ce1472289', 'd358d41c69450b171327ebd99462b6afef687269', '4a026fd65af4ba3575e64174de56fee093fa3330', '759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89', '6640f4e4beae786f301928d82a9f8eb037aa6935', '024006d4c2a89f7acacc6e4438d156525b60a98f', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', 'f0c4b7568f378e652645232e66a1dab4c5b5293f', 'bcce96a2a074448953fc61a29a84afbdfc8db55a', '111b9c7263daec83fa5164fa07888fab18c6c033', 'f7e59546d75b2fe71d1fdda2773f84bb04fcc6d2', '7a7a23f2c39f9b1526bc8853c6c71a5b7f89e68c', '92c78a1eeb3fb335f4427a46c8741b37b2f98f1e', '8570302f7b63e8fcf87030f556b065fd8c260021', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', 'b107acedcf1953ff498ff459f915845962c47674', 'eb6e94817bc64429adfc786574696d6dea633939', 'b2db5541059288472ca246acdca6ead949326864', '9ea46f06b614edb9c60771e0829ef1558048a21f', 'b0dc50b566596f4b74a6317e260fee169336dc48', '8090121ad488b4af27bc59bf91b62e9c6a6f49c6', '6e4355cadf50252a10e985ac94449a574b38768e', '16c97a8a29b0d63fdb119daefabc47df92ff6c24', '2a65434d43ffa6554eaf14b728780919ad4f33eb', '427ce4ce5f64295f39c8714de6ff09fb3bb7d187', 'b25a8003a4e62d2db21560c52fb7030547834e87', '85c851b739b4c7fae13bc7554f34f0ceec00f510', '4645bec4bf86a94265039a357166db457b3f7aad', '6bc8db0c7444d9c07aad440393b2fd300fb3595c', 'a973eb892f233acb3093589393181ae633d3a244']}
{'paperID': '95088cb4b573cde65e748d9764dbbced0157309c', 'abstract': 'Stochastic differential equations provide a rich class of flexible generative models, capable of describing a wide range of spatio-temporal processes. A host of recent work looks to learn data-representing SDEs, using neural networks and other flexible function approximators. Despite these advances, learning remains computationally expensive due to the sequential nature of SDE integrators. In this work, we propose an importance-sampling estimator for probabilities of observations of SDEs for the purposes of learning. Crucially, the approach we suggest does not rely on such integrators. The proposed method produces lower-variance gradient estimates compared to algorithms based on SDE integrators and has the added advantage of being embarrassingly parallelizable. This facilitates the effective use of large-scale parallel hardware for massive decreases in computation time.', 'bibtex': '@Article{Cameron2021RobustAS,\n author = {Scott A. Cameron and Tyron Cameron and Arnu Pretorius and Stephen J. Roberts},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust and Scalable SDE Learning: A Functional Perspective},\n volume = {abs/2110.05167},\n year = {2021}\n}\n', 'references': ['b2f3f828ef7d711c6d2a3174e6745f8f833ee158', '6c21ca5e4c2524b7373488c5674d4b3856643b98', '03058f9a39d37a8bee635969eed227d59bbc8152', '8540780e6b9422f7a1264edb70f39d3ff79bb8c1', '57784e5db3504d549d16382de8f7f4ad222b3d71', '98fc7a351bbb07fb3a304508e1a5ffcab03babba', 'bb758228488bd5a67a8bc7369bf090b3d2bc4cc3', '5d6563505d1da1dc4ae3b13e29fd77f03bd667be', '1ea024f76115c1f6d9c3bbe1889ff9941f333241', '166ffd62f4f095efc475fc6ae9cb7ab0c059d31d', 'c9e976ddbe75a2f42d21e68846055a9952a04544', '449310e3538b08b43227d660227dfd2875c3c3c1', '5dd88e69bdc35636eab242eb24cc718e1d8e67ac', '922bfe9407a0d2681c7cb75ab7017f9aebff1df7', '767d0625c4767c6b8afa0b1b30deafed7e0e8f08', 'd85071fa91d57ab389043db37f4e22ff71b80d2d', '69b450c38e1b9e455cc9730709c14ed36a88d74f', '824de969e5b8512a24db44b2c12a3930ff20a597', 'be0dd2e91bb104494feeb5da2761cf930564f650', 'ec3e5379119a676c126353c92f4a2953d76890f6', '7b1eafc158d341831c6a7645413a557fff5dd52a']}
{'paperID': 'c875d546411c598df075ae555fbb3108fea02910', 'abstract': 'Recent work suggests that feature constraints in the training datasets of deep neu- 1 ral networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). 2 The representations learned by such adversarially robust networks have also been 3 shown to be more human perceptually-aligned than non-robust networks via image 4 manipulations (Santurkar et al., 2019; Engstrom et al., 2019). Despite appearing 5 closer to human visual perception, it is unclear if the constraints in robust DNN 6 representations match biological constraints found in human vision. Human vision 7 seems to rely on texture-based/summary statistic representations in the periphery, 8 which have been shown to explain phenomena such as crowding (Balas et al., 2009) 9 and performance on visual search tasks (Rosenholtz et al., 2012). To understand 10 how adversarially robust optimizations/representations compare to human vision, 11 we performed a psychophysics experiment using a metamer task similar to Freeman 12 & Simoncelli (2011); Wallis et al. (2019); Deza et al. (2017) where we evaluated 13 how well human observers could distinguish between images synthesized to match 14 adversarially robust representations compared to non-robust representations and a 15 texture synthesis model of peripheral vision (Texforms (Long et al., 2018)). We 16 found that the discriminability of robust representation and texture model images 17 decreased to near chance performance as stimuli were presented farther in the 18 periphery. Moreover, performance on robust and texture-model images showed 19 similar trends within participants, while performance on non-robust representa- 20 tions changed minimally across the visual ﬁeld. These results together suggest 21 that (1) adversarially robust representations capture peripheral computation better 22 than non-robust representations and (2) robust representations capture peripheral 23 computation similar to current state-of-the-art texture peripheral vision models. 24 More broadly, our ﬁndings support the idea that localized texture summary statis- 25 tic representations may drive human invariance to adversarial', 'bibtex': '@Article{Harrington2022FindingBP,\n author = {A. Harrington and Arturo Deza},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks},\n volume = {abs/2202.00838},\n year = {2022}\n}\n', 'references': ['9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342', '9ebc9bcde129a7a78141379a6b476d3e8b95eedc', '0e100c06d9fbdf32c434fd40469939a4aaab6c24', 'b61dc7fcb46c4c5bfa2bd6df7640a07d6b3a6ff2', '762752eb9a9a92b028026b17c46d50474ddf3f06', '5cee90b85b88e4de1d51b2963613a48b68916ac7', '18bb573ab638f5ed7f1b65c92fc866736a757d19', 'd20234588c318328bafdd421514667d12dad0cfa', 'd3aa93e3453ea2d5ea0e39324d8f2d9a0ac0aa43', 'f24f62fabec6bca02658c320ff9b43c84947c5de', '12a2eb89765f547ecc1cc08819f1e458bd6f99a0', '7323ff36df929ecf1b877c8d0daadffae384c3e3', '8b6d18a00a78174864bc16fd705ca6c9241aefb3', 'aae016c76f0f88353d8930522c2344804bdb635f', '21549620b6c67a3a9bf10d23e4e16863bf901076', 'a948c242a71b470b1aa55f986737d39507bcff9e', '411c0ebf6acf722f1da05e1568bbbf65e1bc1660', '508cd4f136c196b14c6b67e4e3c7f4eaee2a902a', '5d28bdfa02f0766febff32b7a6b287611d6f2995', '6748d363ae384b8675ade6ba2e2de75d2d215368', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '9115482a981c73627aa624d54aa3712401e210d7', '2f477392205e146fc705b017bb7069d9aad76cac', 'b32d735f0807bb92da06c3a2669c69bc525ce0ea', '1b9c6022598085dd892f360122c0fa4c630b3f18', '6f4afaa1ec7528c59aba86def531df6c524229b2', 'be4a4f7f65d397a4e07dc83b95da6b414e0634e2', '9811229959e14dd9578bd8d8336dfc40e128668c', '03c34b161fc7f953325dd451d97dc51d47914afb', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '7f3df93baabaf66bebbf7a24257f4ada252bc959', '20a6a3c123cd6fdb2ffb495fb55ff8c33c40b767', '692009b1a91bcf71b5f222ec55a2ccfa73483d23', 'a6a9dad014f29c6f8eb19f4dfa1461bf207c4a29', '0d0eeb46fc5ec778a62bb94aa2ef261b08e6f8c6', 'a4cec122a08216fe8a3bc19b22e78fbaea096256', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '4d790c8fae40357d24813d085fa74a436847fb49', 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', '676e15313be9e5e3111e6af360238763c6835028', '193edd20cae92c6759c18ce93eeea96afd9528eb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '4536ccb185343cc3eaa2fee395fe8f684f8369eb', '1ef9dd95d3f10f22cd1de602810adf5d07aa906f', '134819e2d3a9bdaf04308c32488f46264436c02b', '6bc9175ac08569f48c8024990fe630794598cd3c', 'd5553af1c623d85e8ad4d36d185902f60c5a2264', 'a7886ec9d38ff28020e5e7e280ac930759a64483', '57a1857f4a066fd433e4790c5aca411769f0b401', '37afeac49518877dc96a3ca2ec3ebdfc5305e0a9', '85abadb689897997f1e37baa7b5fc6f7d497518b', '4568b728237772007404032e9a06b6ee92751b49', '7d7721e2c556e02f35654428953ed83cfa8adff8', '83074157d165b6245915508d891b2d0cd066f3ad', '16985805560f8c4456367b6245def7482519089a', '2eb5c3f2489c2c7b8456610d4c4ef84c53d1f06d', 'acd519500bf1de81f7e46516c5f5e7fb8cbdbb92', 'a7ed15f8113c4d4c5a994f475e6c87c54342154d', '1c46943103bd7b7a2c7be86859995a4144d1938b']}
{'paperID': 'f01b92624046cadffe6d8a26805e03502ed3fd81', 'abstract': None, 'bibtex': '@Article{Bao2022TowardsUT,\n author = {Hongyan Bao and Yufei Han and Yujun Zhou and Yun Shen and Xiangliang Zhang},\n booktitle = {International Conference on Learning Representations},\n title = {Towards Understanding the Robustness Against Evasion Attack on Categorical Data},\n year = {2022}\n}\n', 'references': []}
{'paperID': 'c24f4fcec3f1fba7a741f6348aafe5899d06ef77', 'abstract': 'Vision transformers (ViTs) have recently set off a new wave in neural architecture design thanks to their record-breaking performance in various vision tasks. In parallel, to fulfill the goal of deploying ViTs into real-world vision applications, their robustness against potential malicious attacks has gained increasing attention. In particular, recent works show that ViTs are more robust against adversarial attacks as compared with convolutional neural networks (CNNs), and conjecture that this is because ViTs focus more on capturing global interactions among different input/feature patches, leading to their improved robustness to local perturbations imposed by adversarial attacks. In this work, we ask an intriguing question:"Under what kinds of perturbations do ViTs become more vulnerable learners compared to CNNs?"Driven by this question, we first conduct a comprehensive experiment regarding the robustness of both ViTs and CNNs under various existing adversarial attacks to understand the underlying reason favoring their robustness. Based on the drawn insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that fools the self-attention mechanism by attacking its basic component (i.e., a single patch) with a series of attention-aware optimization techniques. Interestingly, our Patch-Fool framework shows for the first time that ViTs are not necessarily more robust than CNNs against adversarial perturbations. In particular, we find that ViTs are more vulnerable learners compared with CNNs against our Patch-Fool attack which is consistent across extensive experiments, and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool, indicate an intriguing insight that the perturbation density and strength on each patch seem to be the key factors that influence the robustness ranking between ViTs and CNNs.', 'bibtex': '@Article{Fu2022PatchFoolAV,\n author = {Y. Fu and Shunyao Zhang and Shan-Hung Wu and Cheng Wan and Yingyan Lin},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?},\n volume = {abs/2203.08392},\n year = {2022}\n}\n', 'references': ['ffbcbced0ec14a9267f185be87d9386407640a11', '3c2622daa8a658d5c85ea9869cb460a70b0f878d', '7a9a708ca61c14886aa0dcd6d13dac7879713f5f', '800cfb3d23115cdcd4d114234b65bbdf2080f798', '48418b285a92376a38daafa664a2dd07d42e3fe3', '7b664a306b7d2f68dd816ea1d6586cf3472d75c1', '94eae578e6af3382f6449506965639f18aab3fa0', 'b70bb1855e217edffb5dfa0632e8216860821870', '0918125daacb6c2b3a2d3f155ad095d5ae8fb9b9', '922e5be564dc51dc645bf312fced4e97198942f8', '100f2e2a810394503472f50938522930bd07b834', '68f080e0ac836ea230cb5316fbed273c70422d75', 'db33c408174eef1e40661e8279afbbbf6db2352c', '5faf75b5c5a4d83bd6407b4aba8fb0bccd7fa31d', 'ad4a0938c48e61b7827869e4ac3baffd0aefab35', '6709d5583f658f589ae6a2184805933aceb18849', '8754533bead3996f20440e4a1d0220d4971d00d7', '18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6', '739ceacfafb1c4eaa17509351b647c773270b3ae', '003326a15fc4a8833785a47a741d7712474fa256', '43e51c1bfd69df518e2907f7a955e485985ba423', 'b6382a7351c0c595f91472ac71d3b2d87b3c4844', 'e775e649d815a02373eac840cf5e33a04ff85c95', 'd2a3bb6356d439146cd8d8e72dc728a1e3d93e7f', 'a52d17eac54b145cbc2b2c823f32b9e76be2595d', 'd29430adccb805ab57b349afa8553954347b3197', 'ad7ddcc14984caae308c397f1a589aae75d4ab71', 'd2e54b3a596a1dce0def9d035dfe1fb7c0c6142a', '2ac7999cce9f415ee87643f56631b55ed26aa10e', '2d1c8d502d759e2948018a9633c4cadd6e8d1bc4', '8a7d037abb7285740c58f578ef201eb6bcf18a58', 'ed73a4b875fd5efd4066fa514d7857ec7de1e7ea', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', '962dc29fdc3fbdc5930a10aba114050b82fe5a3e', '0ebf5a0208ec2d930673e5e852cab44f9e1c8811', '18939eadc9c4460c8385e0591cde214a1ead067b', '449c5660d637741f7aa7ff42549c32b43c9968bf', '6d4a87759917132913319960389f17fa1fe8b630', '8733fe2371b615609b04e2e910b1ecfa8e77cbc2', '7a87ab984ca45aae2c5768d22cd6df3b5fd509f9', '309b906fed883e5efe4acf676c655ead21f6c17b', 'e8c46dade1aaedce96ecd03178379b5921a90306', 'a3347bbd82938788ec085772813c095de17a0b37', '1adfa30bf112de20cb959014e44626d760aa8e4e', '65fd9ded2c411d90bcf6d38132463797754d2d21', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', '4f53bb893ca92f0a1b9b3d1bd2ee5de3cdb7c0da', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '5ca131d97019ff1e40a92fa1c9c4c5179632744a', '8bf18d546ae7aea144663136c5704049953ce4e2', '869fdb53a40290a3941fd6ab808835e9b5184d62', '10ab21b120e305b6d3cbf81c5a906d36521152f1', '27d2f8a0601ce4bd73339ddffc83977e858c5503', 'b3f83e8416010e9c3a705a0b6390d268e5ddf5c0', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', 'b514949ad8344071c0f342f182390d2d88bcc26d', 'e3b17a245dce9a2189a8a4f7538631b69c93812e', '9a089c56eec68df722b2a5a52727143aacdc2532', '9ab7319dbe80549ba80e3320d0546d741a7a5791', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '405b6ff2ea2ec9a7c7d6b18ac951dc778892ffcf', '061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '819167ace2f0caae7745d2f25a803979be5fbfae', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'eb42cf88027de515750f230b23b1a057dc782108', 'dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71', '62c76ca0b2790c34e85ba1cce09d47be317c7235', '0def290ae38abb4a04e35e0bcdc86b71d237f494', 'c8b25fab5608c3e033d34b4483ec47e68ba109b7', 'f737f6490d96d7d64e4389463592b93df8c2daec', 'e3f41f4b6b4e3ab740176f022bcad522ad4c38ec', '976a609cf540d1ded373b872d34779f7164d840a', 'e225dd59ef4954db21479cdcbee497624b2d6d0f']}
{'paperID': 'a01ac66f5f66a2b23152f631b920972e4407275c', 'abstract': 'Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we find out via extensive experiments that off-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations. The performance gap between balanced and imbalanced pre-training with SSL is significantly smaller than the gap with supervised learning, across sample sizes, for both in-domain and, especially, out-of-domain evaluation. Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks. In contrast, supervised learning has no incentive to learn features irrelevant to the labels from frequent examples. We validate this hypothesis with semi-synthetic experiments and theoretical analyses on a simplified setting. Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples.', 'bibtex': '@Article{Liu2021SelfsupervisedLI,\n author = {Hong Liu and Jeff Z. HaoChen and Adrien Gaidon and Tengyu Ma},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Self-supervised Learning is More Robust to Dataset Imbalance},\n volume = {abs/2110.05025},\n year = {2021}\n}\n', 'references': ['771e0af5535a0122004c265d8d17931c710677b6', '704ac069de5a539ef42489ddb6cee0bd1650d54c', '3f1774d91c7c219f063f40b894e837e6c48b2bb2', 'bfca930e7ca822ea098dab35a1fe0b624e15d17b', '51a33b04933f932c3a1425339c4412be89a2bdb5', '689d3394c674b8cb2906fa8ffb1c80ecc76e69d0', '4f3a7aaedeacc76c414273c670b46c809590a1c2', '0f8aa47ff8c6c49a347e192debe20ce4e5a4caea', '129509ddad6ab944ff77d6da971fa3d3adb1524a', '2bceaa105b3d31c7d539f4a316f013a062ae7c15', 'bf9386df7a517cb888f0229a1815b76e3826f150', '0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d', '4027fe2fcb6026601487c298fe2fc45d3df5e7df', 'd618752d2e666d7b25f1bd6c7c3bd7c056e19d96', 'a2cd073b57be744533152202989228cb4122270a', '24a07bc3826ae3518ccbd0e004c65319896e0c5d', 'fa5853fdef7d2f6bb68203d187ddacbbddc63a8b', '1ee3fc3d32a0b56f6e568fb0590cf635e1249f42', 'a504b45e2cff77abcc9d78cc95159c08305e44d1', '596637f664b6fc1a5ad694378bb555598fac40ac', '4f65f604d3bf5fa91634484a3232b426267b71ef', '542ca5253b1a5ac1e1a55d9ee777def330e9334f', '10161d83d29fc968c4612c9e9e2b61a2fc25842e', '82b20ed50126e106091dd16aaeb538cbb3bfddb9', '5099d47408251626a4adc6a0f5e93678d8188732', '38f93092ece8eee9771e61c1edaf11b1293cae1b', '60e81c0b58bf765a1cf77d9734a8e315f30c4cde', '297dbda128756e2816130a8de057f96760e4b89c', '4815e465f223ce66e6eea29b5b12bb39fc531538', '507a0be8e4b33b8578f69e999bf6fd009422e1c6', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', 'add2f205338d70e10ce5e686df4a690e2851bdfc', 'dcc4c760c3f1cb17f953c487190b735030c33b78', '87f6a7c014ce206ac5b57299c07e10667d194b39', 'bcfba69c2fadf2efea83be12fda2601f8d4681af', '96f8e5820c5b59e65c2331d577aa738676e8c605', '73c07e0a998576bb9d9409e5eed713788c0be037', '403227333329b36183004f04db72362b604adef3', '4c909ca74217234831bf2900aa83a4761823f2b1', '54036f43acc6c9b49b334270c7237217685f52fb', 'b661520bf0061b7d96ccf12016e351dd3a6ee780', '9c985db0a4a255a06e0fbf2ce147ea741720f9f0', '5786917220aab2f6d0b00606eee9fe0ad0700f1b', '576f4fd5f50c58c755eae73a983939bdccf8fac6', 'c5420ef59d7508d82e53671b0d623027eb58e6ed', 'aab368284210c1bb917ec2d31b84588e3d2d7eb4', '9757ef31095227cb289af22b0a4010eda754d100', 'f8599ad5332cdf2c9919988ba300bb4b438b5834', '1a857da1a8ce47b2aa185b91b5cb215ddef24de7', '2f5250a5b54c5be194f984c3fd2b69029528dd85', 'e7eef2ac4136ec93bd306d2c9c353a13729a4553', 'f537f1bc527bf33cc5fd8da34275106329de1802', '2ec8f7e0257a07d3914322b36072d1bbcd58a1e0', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'fc1b1c9364c58ec406f494dd944b609a6a038ba6', 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', 'a83cec6a91701bd8500f8c43ad731d4353c71d55', '522d65a3db7431015aeaa201a7fc4450a57e40c3', '84b50ebe85f7a1721800125e7882fce8c45b5c5a', 'be9a17321537d9289875fe475b71f4821457b435', 'c7c8134ea51302e1a956035008f3049a8fc883e3', '48234756b7cf798bfeb47328f7c5d597fd4838c2', 'ae341ad66824e1f30a2675fd50742b97794c8f57', 'da2a9df914ff14552ae1747b5bad5bbb5d66ec62', '7e67c9964a9defedd4f9dbe50f6e38ee58d52d62', '6a7364f6ed2846ea2b705336a4c49dd287102a50', '93f9607034c9b7b7693c60e9d2631adc15a2a524', '34f25a8704614163c4095b3ee2fc969b60de4698', '5d90f06bb70a0a3dced62413346235c02b1aa086', '8cb44f06586f609a29d9b496cc752ec01475dffe', 'ebc3914181d76c817f0e35f788b7c4c0f80abb07']}
{'paperID': '6e9577c4b4518b9976b9d421755ce37f8ea3ed7f', 'abstract': None, 'bibtex': '@Article{Wei2022CertifiedRF,\n author = {Colin Wei and J. Z. Kolter},\n booktitle = {International Conference on Learning Representations},\n title = {Certified Robustness for Deep Equilibrium Models via Interval Bound Propagation},\n year = {2022}\n}\n', 'references': []}
{'paperID': '9420398975d0989c638d47c6f059d09272b6992f', 'abstract': 'It is well-known that deep neural networks (DNNs) are susceptible to adversarial attacks, exposing a severe fragility of deep learning systems. As the result, adversarial training (AT) method, by incorporating adversarial examples during training, represents a natural and effective approach to strengthen the robustness of a DNN-based classifier. However, most AT-based methods, notably PGD-AT and TRADES, typically seek a pointwise adversary that generates the worst-case adversarial example by independently perturbing each data sample, as a way to"probe"the vulnerability of the classifier. Arguably, there are unexplored benefits in considering such adversarial effects from an entire distribution. To this end, this paper presents a unified framework that connects Wasserstein distributional robustness with current state-of-the-art AT methods. We introduce a new Wasserstein cost function and a new series of risk functions, with which we show that standard AT methods are special cases of their counterparts in our framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distributional robustness AT-based algorithms. Extensive experiments show that our distributional robustness AT algorithms robustify further their standard AT counterparts in various settings.', 'bibtex': '@Article{Bui2022AUW,\n author = {Tu Bui and Trung Le and Quan Hung Tran and He Zhao and Dinh Q. Phung},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {A Unified Wasserstein Distributional Robustness Framework for Adversarial Training},\n volume = {abs/2202.13437},\n year = {2022}\n}\n', 'references': ['7fe935a147cce08933eb5a9d1dc123fcc3b2b8bd', 'b3594a146adea04a2728e9a8802537e7047a2f4f', '648098a11da32ffbdf857f320492e668abbaa11c', 'df872e72e87a85f9b5cd28da06ace46386462fde', '2c2496c3efc191feec5f163ddc80fd2370263c51', '57acaf4538d1a6e26c77cfae5640e359e763952e', '2aab97e35c43d961d645e650808d5b052ec180ab', 'b82ebd94bcff28d702eede7227c3d3a06055aaaf', '99a599d8fe56529f47e78243ed61250190f96196', '67f74fe9d46f88661573003f8f1f12967ae49fa3', '6cfaa925c881ba5aa8efe8980a9ebe6509f0ddaa', '2d79b9f90596008d4004ae8109902f4b34638367', 'eeb771507ebb1cb934caeaeab1fe9590521fa1a0', '2d44b08cd19c8099c626d5da4aa94160c7de9e0b', 'a2a349218b7889425c005880cc4b16b0a9e54dd8', '764eff31d9596033859895d9513b838d2c57a6fb', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '2bde080abcb051340586bfabb87f6fa80b34e446', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', 'b27da51d2b33c67b1b366f6f3a1e61e84dbab230', '5e7f38a31083634c95b1a3df4f0c8da4cc09125d', '58c143069444c7dff4be53531a47efefc40be497', '29cde68a09645f166cc582c99219e4452611dfef', '469bae685ae7fa6dc7ff73b9076041b79aa083e0', '7e69287ac94ecb2af7fbec1cad0c773ea3193de9', '4a0d35989d91b3b7d2318802b1de6d10e4e6e830', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', 'a38d273eb6f8eb0fc0b3eca232c41e80a9e54432', '4c84df2281280c31a190138a7bf8e8285d41bbef', '2c1006c856fefdbd6cd710e840e57153f2d6cd04', '91a05cb84f1c7dbb0354da2ff11ae92549152435', '31c260401df5df509c4889fb9387e361ae551f45', 'acb9017f0d55ab0e8c8863b0b5dbdb373e4aee7b', '3f7bc67330b3eff749459568e7995f0017dfe645', 'daf8cd0f2c159d022477914bfacee9ff6da70c8b', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', 'ffb976dc7e2623c34e7636ddd4a9e2fa36e09ca4', '676e40050453ddeb1387f8314478c0ac3681a8c6', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '2d693c19d29e7a7670aff0784ab2f2bbf8134f35', '651adaa058f821a890f2c5d1053d69eb481a8352', '3d61c35547b3fe5a8529a46d0ef6f8e6c5bcb153', '97fe017a57819219055aa7c239628c0f7c10cdc9', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'f7c0b4333f914546b144ba15ea38d0e8f29bb622', '4b1c6f6521da545892f3f5dc39461584d4a27ec0', '86869b1f62ef30a34fe71cccd844dad970968f6c', '352056cf4cf7e44fb8ebf863c4b632b759a39344', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '631449eb1cb2c33c8781ba64a9ff2da197dc14dd', 'f701b58e41d928cdcd8d733b638fd65a73623b72', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '39a364217eecde950fe1c3f6c8cd359f8586899d', '51dcc0c6c8ea27f0a5a3071fb8c4b32004cd55d8', 'd450b0f12ae0437048e4047a630c31d902002d0c', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '583b55367f787eb0c4e295707b642e63547b9806', '76f07cec4b53a965b3d0641e138acaa0cd72fb79', '9a534fd4a5ee8bf37b6d4608403e67f94e7ca7c0', '9bac32b81d772cd48bfbbeb6c97a1fa5d58c7a53', 'cb05b19743265358030e4501b33c204ae2e4e863', '68d12857f76645a86417139eb6078db1ba76a7bf', 'babe2d7073dcac97a6ca53b87b85b9506e23e1ce', 'e4350e816a350662ddb5f9ef92437aa8f3fd44f6', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4', '8d56d4bc69a8c562434b9a129542bb79e9d6f1d6']}
{'paperID': '6d3b65097096bb0ec52896e9d8a8ad194f18d290', 'abstract': None, 'bibtex': '@Article{Han2022DeSKOSR,\n author = {Minghao Han and Jacob Euler-Rolle and Robert K. Katzschmann},\n booktitle = {International Conference on Learning Representations},\n title = {DeSKO: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator},\n year = {2022}\n}\n', 'references': []}
{'paperID': '7f2f8042750df1be7562023760148a391d247904', 'abstract': 'The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on static supervised learning tasks such as image classiﬁcation. However, DNNs have been used extensively in real-world adaptive tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in provable robustness in RL seek to certify the behaviour of the victim policy at every time-step against a non-adaptive adversary using methods developed for the static setting. But in the real world, an RL adversary can infer the defense strategy used by the victim agent by observ-ing the states, actions, etc. from previous time-steps and adapt itself to produce stronger attacks in future steps (e.g., by focusing more on states critical to the agent’s performance). We present an efﬁcient procedure, designed speciﬁcally to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step. Focusing on randomized smoothing based defenses, our main theoretical contribution is to prove an adaptive version of the Neyman-Pearson Lemma – a key lemma for smoothing-based certiﬁcates – where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Building on this result, we propose policy smoothing where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Our robustness certiﬁcates guarantee that the ﬁnal total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack. We show that our certiﬁcates are tight by constructing a worst-case scenario that achieves the bounds derived in our analysis. Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice.', 'bibtex': '@Article{Kumar2021PolicySF,\n author = {Aounon Kumar and Alexander Levine and S. Feizi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Policy Smoothing for Provably Robust Reinforcement Learning},\n volume = {abs/2106.11420},\n year = {2021}\n}\n', 'references': ['5ae274cc9ca0fc8a3c089d7320d103f0876bde4f', '3fa6da03eb6c4d1ecb5560ffba299e7cc8826477', '1a627d2a169d71563109546da590a7cceb0b349a', '06aaece45f8284de309d4d9d8772305fb848a66d', '317b1dec6d4f950d4607a80df32447827da4799a', '6d0036bae18aa441a19b63fc4b2daf91f63e8029', '4c9ee8358d82afa960708391e2b8e83c4a737ae9', '3d89830c0dd84bcd42b202c402aba1e0286015e7', '26edef02b17ae334e322e73caa97b8420255f118', '07398e448180ad75c44d30f23a65289d40ff6f52', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '6ff50528f3d7c72772f8c0e3f8398f9dd8e06575', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '5c31ff945ef663b491eedd06fc2b2232adc1d2e2', '7ad8c18994108a630c4564400f6137bf4d8b7818', '43a4a354b67ab6d5531355a368094815d2d2593d', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', '75339d34bdac0d21a41461228ec6088eecdf857a', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', '2f201c77e7ccdf1f37115e16accac3486a65c03d', '8b9127bee0f7d109da2672ba06d0f39a5a60335a', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3', '38fb1902c6a2ab4f767d4532b28a92473ea737aa', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'adb762a645b72fc4605e6fb512ef2684db37cc93', '0a77313fa10a864e14f538c73d417d7b4d6f320e', 'c8c16a56d2a9520197da9a1546f517db5f19b204', '1d65848c563b2c3a7f0153551c1b39e0e5c2d776', '7dfbbc101cf9e7359a336df25dab90d66e249255', 'e2a85a6766b982ff7c8980e57ca6342d22493827', '2b10281297ee001a9f3f4ea1aa9bea6b638c27df', '0e3cc46583217ec81e87045a4f9ae3478a008227', '69e76e16740ed69f4dc55361a3d319ac2f1293dd', '846aedd869a00c09b40f1f1f35673cb22bc87490', '024006d4c2a89f7acacc6e4438d156525b60a98f', 'b6b8a1b80891c96c28cc6340267b58186157e536', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', '449532187c94af3dd3aa55e16d2c50f7854d2199', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '2319a491378867c7049b3da055c5df60e1671158', '8a7acaf6469c06ae5876d92f013184db5897bb13', 'a9ce5f674749ac21be036a9a4f05398eb000d826', 'd8febb0df3d6caa480ac45ba9fd3d1d64606fd89', 'e225dd59ef4954db21479cdcbee497624b2d6d0f', '874b3a63422eeaf24c14435ee6091ed48247bff3', 'a05f5a5c9fe1d8a44f5960571cc6f4fbb75d0d36']}
{'paperID': '7cd5d4942a5cd6f1299841a5fec96eb65cdb677b', 'abstract': 'Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature. In this paper, we present a theoretical analysis on the convergence of IBP training. With an overparameterized assumption, we analyze the convergence of IBP robust training. We show that when using IBP training to train a randomly initialized two-layer ReLU neural network with logistic loss, gradient descent can linearly converge to zero robust training error with a high probability if we have sufficiently small perturbation radius and large network width.', 'bibtex': '@Article{Wang2022OnTC,\n author = {Yihan Wang and Zhouxing Shi and Quanquan Gu and Cho-Jui Hsieh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the Convergence of Certified Robust Training with Interval Bound Propagation},\n volume = {abs/2203.08961},\n year = {2022}\n}\n', 'references': ['dcfb420412d76600eb124625f62fb28499af1e8c', 'ca45cce58cd1c6ea7eb638fed17b6c6f44bd9c6a', 'baee6bd21ac34e4e96479b928917e0c4f78c9c14', '525d1f68539c436072a3cb6f3b8a88e3b124260d', '493152ae263d92de457b4be7745c20bf475eb4ea', '71ea8f105803703893b5c2d01f0c9508643b6554', '18a9bb863e3110e2e981b53618b214585a32f877', '58c143069444c7dff4be53531a47efefc40be497', '724f81cbc1197c83f546568b2aee2f0b9c4e4fd9', '8f1b6eca0bdeb8cf0173d950b1157f290439cead', 'e91b3347c8b411211a68ed68556298d12406cc83', '0fdf1a213ed08012d5d21067544b860f40c08e8f', '1822e737dfbc933c95ab44f1dd123756120dfaa4', 'ff22e140a0423f1cf0595d213f36402668084014', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '649de559f530aab8f22f6022d40cdfd3bb4e1039', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'f20de741e2d4ece239261de010d6d9beca3b26b9', '313b368457e54e6a7482b008d5eb4182eb1b4d1c', '03e7e8663c69e691be6b6403b1eb1bbf593d31f2', '63b4347cd65c00efae69726996752a2b1c869fa5', '7ad8c18994108a630c4564400f6137bf4d8b7818', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', 'd6f6d1504cfedde4efb23e7ec0f42f006062c6a0', '98cc371f4e3a39b5c69b4e8980a5990f9011f223', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', 'de49430578bb3f8de3e610423255662c45f17610', '42ec3db12a2e4628885451b13035c2e975220a25', 'ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6', '75339d34bdac0d21a41461228ec6088eecdf857a', '7a84a692327534fd227fa1e07fcb3816b633c591', '20f85256555ad612148e52f9363e52f9d661728b', '54afe5cde4d4140e728dde299d4d66b2c0eda6da', '2410923ed90b099e3f5565b63e789f10bf70ec4c', '9db631435f7f79646a4e0a1841fbeb3340e44261', '6ea8cbf0cc4cda3d981348a279b464524a8485cc', '651adaa058f821a890f2c5d1053d69eb481a8352', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '4b23012689e0f17912fb38d4984775e567cff8d6', '91f4ebdfb4618e9a7bbcefc8b64e2f7d6e176545', 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '99cb08c76c120599abd1d1637e32aaf577f38d39', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '0ae5e6f0a656aa5a133485b498a2386942821786', '9cbc9de49f77f9476f923ff86aa9d5fcf33011e6', '5d3b1a36d56f1d5207332f9146bd8a0d91dae1f0', 'bb92676f9ec13783ac664c268191f20944718f95', 'e225dd59ef4954db21479cdcbee497624b2d6d0f']}
{'paperID': '11417522f57c13898e24d87ef22f9e45fa197cf8', 'abstract': None, 'bibtex': '@Article{Xiao2023DensePureUD,\n author = {Chaowei Xiao and Zhongzhu Chen and Kun Jin and Jiong Wang and Weili Nie and Mingyan Liu and Anima Anandkumar and Bo Li and D. Song},\n booktitle = {International Conference on Learning Representations},\n title = {DensePure: Understanding Diffusion Models for Adversarial Robustness},\n year = {2023}\n}\n', 'references': []}
{'paperID': '7ca41cc5fc364b713aba5b573ae4ada801fd788a', 'abstract': 'Identifying near duplicates within large, noisy text corpora has a myriad of applications that range from de-duplicating training datasets, reducing privacy risk, and evaluating test set leakage, to identifying reproduced news articles and literature within large corpora. Across these diverse applications, the overwhelming majority of work relies on N-grams. Limited efforts have been made to evaluate how well N-gram methods perform, in part because it is unclear how one could create an unbiased evaluation dataset for a massive corpus. This study uses the unique timeliness of historical news wires to create a 27,210 document dataset, with 122,876 positive duplicate pairs, for studying noise-robust de-duplication. The time-sensitivity of news makes comprehensive hand labelling feasible - despite the massive overall size of the corpus - as duplicates occur within a narrow date range. The study then develops and evaluates a range of de-duplication methods: hashing and N-gram overlap (which predominate in the literature), a contrastively trained bi-encoder, and a re-rank style approach combining a bi- and cross-encoder. The neural approaches significantly outperform hashing and N-gram overlap. We show that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours. The public release of our NEWS-COPY de-duplication dataset will facilitate further research and applications.', 'bibtex': "@Article{Silcock2022NoiseRobustDA,\n author = {Emily Silcock and Luca D'Amico-Wong and Jinglin Yang and Melissa Dell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Noise-Robust De-Duplication at Scale},\n volume = {abs/2210.04261},\n year = {2022}\n}\n", 'references': ['55c36748f2a7c060c3313349c730b053ed03fbf7', '002c256d30d6be4b23d365a8de8ae0e67e4c9641', '4566c0d22ebf3c31180066ab23b6c445aeec78d5', 'eb5fe6a806c30a9eae3f5430c6704780b230bdb7', '1adadbfa95e43a70fcd17e6ce947a0652b86bfc3', '807600ef43073cd9c59d4208ee710e90cf14efa8', '300cbaf1644920b15a97692e6309f5d58e1abc0e', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', '273e7a33ec437dab8c1c4640f54891dcfe8d5fab', '38643c2926b10f6f74f122a7037e2cd20d77c0f1', '0e002114cd379efaca0ec5cda6d262b5fe0be104', '79cd9f77e5258f62c0e15d11534aea6393ef73fe', '43f2ad297941db230c089ba353efc3f281ab678c', 'ebac77b8a0951dc91dbf63c906d346c7cfd3a2e7', '592a6691373f3936631bc4ac122f69df09c842bd', '93d63ec754f29fa22572615320afe0521f7ec66d', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', 'ba1382a0574baa0345fd727f259bc86797fe1381', 'cf0835767ddc35ff024f1b2a0ea5e5cd415ae498', '435553998fbef790b5bed3491a8f634d9ec5cfa2', '1b31cd40f6669ca8b1a1b1986fd41d3162424f2f', '1a0912bb76777469295bb2c059faee907e7f3258', '2cbb8de53759e75411bc528518947a3094fbce3a', '151c2d91a3fb03b9f3a8a91a70474de6e53b69b1', '0dd8ceaef659bde339bdcedfe9b111d75be59a9f', '0e6824e137847be0599bb0032e37042ed2ef5045', 'f2f85ec20bcae8d67035fd446b855a75b9a0aa8d', '2a8c49f13c3d2919c5aa4d8dc9c7803e369eb529', '46f30e94dd3d5902141c5fbe58d0bc9189545c76', '9405cc0d6169988371b2755e573cc28650d14dfe', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992']}
{'paperID': '31c4ad6867aa4fccdf0df6c6d27b8d956cf8d1e4', 'abstract': 'As machine learning models are increasingly being employed to make consequential decisions in real-world settings, it becomes critical to ensure that individuals who are adversely impacted (e.g., loan denied) by the predictions of these models are provided with a means for recourse. While several approaches have been proposed to construct recourses for affected individuals, the recourses output by these methods either achieve low costs (i.e., ease-of-implementation) or robustness to small perturbations (i.e., noisy implementations of recourses), but not both due to the inherent trade-offs between the recourse costs and robustness. Furthermore, prior approaches do not provide end users with any agency over navigating the aforementioned trade-offs. In this work, we address the above challenges by proposing the first algorithmic framework which enables users to effectively manage the recourse cost vs. robustness trade-offs. More specifically, our framework Probabilistically ROBust rEcourse (\\texttt{PROBE}) lets users choose the probability with which a recourse could get invalidated (recourse invalidation rate) if small changes are made to the recourse i.e., the recourse is implemented somewhat noisily. To this end, we propose a novel objective function which simultaneously minimizes the gap between the achieved (resulting) and desired recourse invalidation rates, minimizes recourse costs, and also ensures that the resulting recourse achieves a positive model prediction. We develop novel theoretical results to characterize the recourse invalidation rates corresponding to any given instance w.r.t. different classes of underlying models (e.g., linear models, tree based models etc.), and leverage these results to efficiently optimize the proposed objective. Experimental evaluation with multiple real world datasets demonstrate the efficacy of the proposed framework.', 'bibtex': '@Article{Pawelczyk2022ProbabilisticallyRR,\n author = {Martin Pawelczyk and Teresa Datta and Johannes van-den-Heuvel and G. Kasneci and Himabindu Lakkaraju},\n booktitle = {International Conference on Learning Representations},\n title = {Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse},\n year = {2022}\n}\n', 'references': ['c3c973d1073e14fd8712db042ef4b3dca7b05c0b', 'b1fe45eda204847f5f4c0b3b8eafaecaf184859c', '3acff13163f51765bb36147f6107967765509d9b', '7b700dbbdc38b714916a93065c1d11ea16728e7c', 'ff2f098e2ad415841836e985ee6dfa2fbad7e0c7', '5fd221aa10d0b0582c1cf1f186a0052d855472e6', 'be659a7747137ef85239d849f84c5a25dcb77639', '16eb4e90f7d43b6c484cdac8b1bbb3774faee960', 'cfc879af857200504ca0eefba0a2d14f998412be', '13bcee171baaa9ea589a7cfbb3124c3146a8e7dc', '6068d39e92aef1bb0e1291e9931894c35692a85e', 'c5dfc5fe7102fd8647edd1c9483aded82557e544', '029a1c21accc0cbf36536cad02a12630b1c6e131', '8d0bf0e36d831c13e6a9522d2473e77a47f2bf0d', '06de04ade91f35446c21fa26c5621c8c88458edd', '48ace0d03aa14792922faf4a4a256385b023972c', '148209fa0c9279bb40322794833aa075769de95b', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', '337b96b55034e6ead100d26e547c942489ff2e93', '5e34d223b1b8070a6090943382e80bebdb9aa4cd', 'c3ba9fffca667d8a64d6486b686abff833c7d3d3', 'f77acbc2beef4ea85b333798621c14a6e3422502', 'c3d846a3c51dc6423381257b95a4b821e778dce0', '324d098c5294c69735ae1670707b2f21b605b8e5', '247d6c9cb92d4afabee20b0ea29ac3d8ea92f120', 'c2413fa296543159b32d16350d9e29f7db528790', '86841a74f0fd99ba369f635715ecae3007f22611', '1b9c6022598085dd892f360122c0fa4c630b3f18', '2862241b8364ee309525a1f4f7859aa38f999258', '4f309712e705210df5695240a5d5fb53ea1f8641', 'a25cf5c0974cfd62c1369ea75120097f118da301', 'cb38e3f2fe39ef689a4f969758719f9ce94e1555', '873259438b927a32db66682f34665b517efc4dc0', 'e96506ee4baab43fa81cf1870cf7befb4a71fec7', '0c908739fbff75f03469d13d4a1a07de3414ee19', '168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74', '30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9', '1679beddda3a183714d380e944fe6bf586c083cd', '27b7bb095832ecb64e99a683ced3084b35247ff1', '8e0be569ea77b8cb29bb0e8b031887630fe7a96c', '1a9a39da9d4fc937bc455705d508674a205620aa']}
{'paperID': '3e23b594fcc710f534d35b46c6017d3c6ec20cd1', 'abstract': None, 'bibtex': '@Article{Sapkota2023AdaptiveRE,\n author = {Hitesh Sapkota and Qi Yu},\n booktitle = {International Conference on Learning Representations},\n title = {Adaptive Robust Evidential Optimization For Open Set Detection from Imbalanced Data},\n year = {2023}\n}\n', 'references': []}
{'paperID': '9d242b5c0c00fc36002161deb6a124f2c624e982', 'abstract': None, 'bibtex': '@Article{Jiang2023OnEN,\n author = {Ziping Jiang},\n booktitle = {International Conference on Learning Representations},\n title = {On Explaining Neural Network Robustness with Activation Path},\n year = {2023}\n}\n', 'references': []}
{'paperID': 'eb776076b4653f45a399e0afef781919d94b0578', 'abstract': 'Many existing group fairness-aware training methods aim to achieve the group fairness by either re-weighting underrepresented groups based on certain rules or using weakly approximated surrogates for the fairness metrics in the objective as regularization terms. Although each of the learning schemes has its own strength in terms of applicability or performance, respectively, it is difficult for any method in the either category to be considered as a gold standard since their successful performances are typically limited to specific cases. To that end, we propose a principled method, dubbed as \\ours, which unifies the two learning schemes by incorporating a well-justified group fairness metric into the training objective using a class wise distributionally robust optimization (DRO) framework. We then develop an iterative optimization algorithm that minimizes the resulting objective by automatically producing the correct re-weights for each group. Our experiments show that FairDRO is scalable and easily adaptable to diverse applications, and consistently achieves the state-of-the-art performance on several benchmark datasets in terms of the accuracy-fairness trade-off, compared to recent strong baselines.', 'bibtex': '@Article{Jung2023ReweightingBG,\n author = {Sangwon Jung and Taeeon Park and Sanghyuk Chun and Taesup Moon},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Re-weighting Based Group Fairness Regularization via Classwise Robust Optimization},\n volume = {abs/2303.00442},\n year = {2023}\n}\n', 'references': ['e54e0d9eaa922cefb1c69e105979399fd34497b1', '9b3a071734cfecb2880105d84bde6791b48bf4a3', '216d093cb2ad81bf55c21dbce2217f2b9032e67b', '9eed0825348e7065c5dcc2effe609ce47f07f30e', 'c502b3d801a687ad26f592aa3416a6b2520d0f7c', '6f9918ad83b2e6058acfc15ed66601879b855494', 'c1bdf40e98e52fcd31bd5f68165e5c43655117f7', '3470256133fe185031791e600f84376262bd9015', '40848b41ed8c9c255ecd8a920006877691b52d03', 'd688594d2aac080f657b7be251e89cca6a7df165', 'fee8f63972906214b77f16cfeca0b93ee8f36ba2', '18c72c585e3ba48f99f46e9485d75c822aebb189', '8ac7f60714087e6548edb008f33e401163bdc982', '648dce875272ba601b36a164a10648decdb3044d', 'af4ec9e525671319fab3feb19b6f9306d94e5dd9', '3621fff4a1c791901ea4a1359c10575193ec712d', '10565078981924f27d38cdbb37353706ac9eba7d', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', '193092aef465bec868d1089ccfcac0279b914bda', '1fa9ed2bea208511ae698a967875e943049f16b6', '77568c594470f9aa029f92774e2c12ab0451d9bb', 'f7f5a101985e66c7440deb9286f7c4602294f29b', '1eb7b1cafe1891712a4f764c78399a53182cdcd1', 'b1e6716d068ee0d98aa213ad496ec189c10f9250', '5be821609481fc46fe61ea404a9322a35f84a446', 'eb649971a40c7b0a28cff1d5aba14f8a3a37c773', '0f7245155d3d2dbe588b9870c6edb1d9f6b1f546', '59c47e49d8211953b1acd68984650b807ce69a71', '7e96d0a69f5d81c353ca12ffb201441867a311df', 'c8541b1dc813f3a638d7acc79e5f972e77f3c5a7', '28930f1472417b25ca42d251c772e38c75395d84', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', '19cb02117084b023c28da2fb356679806a299890', 'c7330852a07170cd0e6990f5fbde5fca12b6ccd6', '18858cc936947fc96b5c06bbe3c6c2faa5614540', 'd07284a6811f1b2745d91bdb06b040b57f226882', '9eacd7d43c95be4c4771bf1a324e200918e6c0cd', '7bcebd481bb1161843efdd135e4ba59dfac4b61c', '5fb808edfaae574ee7f4553ad7090ca1fed9b585', '37f5d47019f467c74acff22a38ffd4b98bdcb5d4', 'd42b11ce90c9c69a20ed015b73dc33e0e4100a7b', 'b022f2a277a4bf5f42382e86e4380b96340b9e86', 'cff3c2226d4a5106c639b7f90f73f0330cc94685', '7b814e39159da8bef0a26308ddf3e4315c687675', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '39a364217eecde950fe1c3f6c8cd359f8586899d', '4c64ee852f082f8f4e0113cfc1302b34e31539a2', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '3ac3c11bf6cd8ccc657eb629148d6e346e52c8e0', 'adaa0523a5c9d5f92aa2009a51226391d8e62380', 'd64a0840bea331caf9b1a61611b8eb8d15ec4f9f', '96167ed3ebc9a2c3270f6ae96043e6f086eed4de', '397306cada03c29ab9c3d5a7991a343cae92f2e3', '98eed3f082351c4821d1edb315846207a8fefbe9', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '68d12857f76645a86417139eb6078db1ba76a7bf', '6eb79424b000e7cd0d47d480f2d7bace0e9720c4', '5fc0c7afc6bb27fb3752eae0ea5869413b1259b7']}
{'paperID': '7e9e38a9101fd6e9ee3cd44f72d9be6842b7a42d', 'abstract': 'The vulnerability of deep neural networks (DNNs) to adversarial examples has attracted great attention in the machine learning community. The problem is related to non-flatness and non-smoothness of normally obtained loss landscapes. Training augmented with adversarial examples (a.k.a., adversarial training) is considered as an effective remedy. In this paper, we highlight that some collaborative examples, nearly perceptually indistinguishable from both adversarial and benign examples yet show extremely lower prediction loss, can be utilized to enhance adversarial training. A novel method is therefore proposed to achieve new state-of-the-arts in adversarial robustness. Code: https://github.com/qizhangli/ST-AT.', 'bibtex': '@Article{Li2022SqueezeTF,\n author = {Qizhang Li and Yiwen Guo and W. Zuo and Hao Chen},\n booktitle = {International Conference on Learning Representations},\n title = {Squeeze Training for Adversarial Robustness},\n year = {2022}\n}\n', 'references': ['3793754efaa4a5ae2174b72956dcb0fe8375f445', '35c0800e657faa18cf3fc3629bdbeafbb976b006', '564ea2fc978cc3a2b031b89e3ae80cec3f965cd3', '9523ccb0179e53151c8d92b21f1ff24f80980ad2', '9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342', 'ba451a3f88b0f1e08bd5cda55c99d96a88a39c71', '2d36eaa618da03d28a48a03e562a9fbc314609c4', 'eb5ccb3f7744ea265b068d5a7f54e8d61de1027a', '5c493a976f724ea8f238509f6fe087a7bde8c93d', '118c81057cdecce85514c9543e01ae88ffe56448', '5e4f03f68c6867d850f457dc5cc36738e5dff6c1', 'b8cee43a51c44f8f4448e78e41ecf081987707cf', '762752eb9a9a92b028026b17c46d50474ddf3f06', '4dea1701757513fdf7a52e64cb85750dbf0ef662', '6e8dc247f2a9341915ea81acca5fc2aa7d14c938', 'bcac1b00badd7c8830e10ba4eb813ad79a3222b5', '5f9eb88409bfc6e171792668fecc83d1d9e3c8cf', '2aab97e35c43d961d645e650808d5b052ec180ab', '1bcbf1efb3f81f0e777b4b754cf5b9789841d12f', '99a599d8fe56529f47e78243ed61250190f96196', '99931b7b794f7d98c7b3f3eee862129133d123d7', 'a2a349218b7889425c005880cc4b16b0a9e54dd8', '764eff31d9596033859895d9513b838d2c57a6fb', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', '8733fe2371b615609b04e2e910b1ecfa8e77cbc2', '948839277bface5780896e8e8791906818aa41ac', '91a05cb84f1c7dbb0354da2ff11ae92549152435', 'edabab811aaf44b7b626efe5278a32bddd3bb77f', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '5ae786deaa875613e85ed2df0dbeec4301109f74', 'b3f83e8416010e9c3a705a0b6390d268e5ddf5c0', 'f78a911f516625d6b7b76a9a33c1eb14613341c4', 'f2c5c3cfe1675dd9239121f1f09069438f047aea', '6baca6351dc55baac44f0416e74a7e0ba2bfd03e', '8e37a3b227b68953f8067215828dc8b8714cb21b', '9ab7319dbe80549ba80e3320d0546d741a7a5791', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '1c4e9156ca07705531e45960b7a919dc473abb51', '77f0a39b8e02686fd85b01971f8feb7f60971f80', '53b047e503f4c24602f376a774d653f7ed56c024', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'eb42cf88027de515750f230b23b1a057dc782108', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '54d2b5c64a67f65c5dd812b89e07973f97699552', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '2fff703a2cfe081b86ea3dc877a0a068da9ae503', 'abstract': 'Representational straightening refers to a decrease in curvature of visual feature representations of a sequence of frames taken from natural movies. Prior work established straightening in neural representations of the primate primary visual cortex (V1) and perceptual straightening in human behavior as a hallmark of biological vision in contrast to artificial feedforward neural networks which did not demonstrate this phenomenon as they were not explicitly optimized to produce temporally predictable movie representations. Here, we show robustness to noise in the input image can produce representational straightening in feedforward neural networks. Both adversarial training (AT) and base classifiers for Random Smoothing (RS) induced remarkably straightened feature codes. Demonstrating their utility within the domain of natural movies, these codes could be inverted to generate intervening movie frames by linear interpolation in the feature space even though they were not trained on these trajectories. Demonstrating their biological utility, we found that AT and RS training improved predictions of neural data in primate V1 over baseline models providing a parsimonious, bio-plausible mechanism -- noise in the sensory input stages -- for generating representations in early visual cortex. Finally, we compared the geometric properties of frame representations in these networks to better understand how they produced representations that mimicked the straightening phenomenon from biology. Overall, this work elucidating emergent properties of robust neural networks demonstrates that it is not necessary to utilize predictive objectives or train directly on natural movie statistics to achieve models supporting straightened movie representations similar to human perception that also predict V1 neural responses.', 'bibtex': '@Article{Toosi2023BrainlikeRS,\n author = {Tahereh Toosi and Elias B. Issa},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Brain-like representational straightening of natural movies in robust feedforward neural networks},\n volume = {abs/2308.13870},\n year = {2023}\n}\n', 'references': ['750b78fdf83c8bfcfd219a74873ddf4edbc11deb', '3b2b0547af85be326302198a40cf434614c14f96', '5f51508a9c86ad9c35b56a0a480721c67a6f4ebe', '93538fea732a77e7ff8a5281eeba0aaaa9094876', '8a9d84d86ac0d76e63914802f9738325c3bece9c', '47b5b4c7d00cfcd980f67e62ed4f27193c088296', '3ecc7ac21c22af073f37294df341c51e5d2d576d', 'e692ab8e6d1b1efb11b24ca31bd9ac53caa021a6', '7323ff36df929ecf1b877c8d0daadffae384c3e3', 'a1b8a8df281bbaec148a897927a49ea47ea31515', '5d28bdfa02f0766febff32b7a6b287611d6f2995', '38a547b5933758a2018f0a40f2c3d538077c4d01', 'ef2b790aae2c1e1c7eb8a8777515266b5094de88', '0d7f467c247540009ab1abfb1e1a5d5dc0689783', '363db281e33509fe99151a7e99ad1efe664cf3c3', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '988a378f640eb7fb681f977d6cb1e0c830c07b4c', 'aa0ad8dd35f7b385feba07cedb30d7e3a1935381', '67b72e427187b1113c787f9265926322e3d123e8', 'cb41b78c1da335d4980c272d2fe5b2c8ad819dc4', '31619c8f4f9557136a675514b22ca8ece65ed38c', '726dd697000e7b2fe5b388e1e72bd88ea81ff397', 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', '11e7c4182a7813d5acf1be198c8c96d164fb95a2', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', 'c4c06578f4870e4b126e6837907929f3c900b99f', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '74907c562f9e55b44836bc1aebea87c617c29ad8', 'ad367b44f3434b9ba6b46b41ab083210f6827a9f', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'dd2fa69647160bb2ea25dcc7b2f6409b01e40222', 'e5e3a4a13e719ce770e036b4eeb82c95527c3296', '2bd2b120ccd5aa88a5927889a973b2204732e435', 'bbab6293783b741729f4f1636b9267e085aa329f', '3bc8c19cd2257790f2c92c0b6b757c0550a8404b', 'a878886efacc6a5d742bf98bfc25c0734ce502b1', '2af18c7b3495955d1ffa4f1b4c4ae5ea786cdedb', 'eae2e0fa72e898c289365c0af16daf57a7a6cf40', '0e11ffcd440073a3b244f67c9278f4a0eaae75bb', '67a5d4fc254fcbd751a0151ba3562eacfc36082e', '8012c4a1e2ca663f1a04e80cbb19631a00cbab27', 'ff1152582155acaa0e9d0ccbc900a4641504256d', '720a1543d94291240f34aac6ecd726f90c60e27d', 'a424ec3b8846f57b8ffdb566d272e28d5a525909']}
{'paperID': '85d08a213e9533c515601451cd78f971e547b1ae', 'abstract': 'Robustness evaluation against adversarial examples has become increasingly important to unveil the trustworthiness of the prevailing deep models in natural language processing (NLP). However, in contrast to the computer vision domain where the first-order projected gradient descent (PGD) is used as the benchmark approach to generate adversarial examples for robustness evaluation, there lacks a principled first-order gradient-based robustness evaluation framework in NLP. The emerging optimization challenges lie in 1) the discrete nature of textual inputs together with the strong coupling between the perturbation location and the actual content, and 2) the additional constraint that the perturbed text should be fluent and achieve a low perplexity under a language model. These challenges make the development of PGD-like NLP attacks difficult. To bridge the gap, we propose TextGrad, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP. Specifically, we address the aforementioned challenges in a unified optimization framework. And we develop an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables and leverage an effective sampling method to establish an accurate mapping from the continuous optimization variables to the discrete textual perturbations. Moreover, as a first-order attack generation method, TextGrad can be baked into adversarial training to further improve the robustness of NLP models. Extensive experiments are provided to demonstrate the effectiveness of TextGrad not only in attack generation for robustness evaluation but also in adversarial defense.', 'bibtex': '@Article{Hou2022TextGradAR,\n author = {Bairu Hou and Jinghan Jia and Yihua Zhang and Guanhua Zhang and Yang Zhang and Sijia Liu and Shiyu Chang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization},\n volume = {abs/2212.09254},\n year = {2022}\n}\n', 'references': ['d0e4d0203721a07191be2cc61a98b4ff71ba6fe2', 'cf3e2e46515bb1f4ccbc54b339f56c0939161def', '40cfec74524cf766f6c0030bc1f1736041a128e2', '8436897e713c2242d6291df9a6a33c1544d4dd39', '31e46ed4722a4895a19eda37dbc02da55572783a', 'f659031ceb7bbdcb7b0690742f35e2924fd1ed75', '59c2b4ef91d4ce23cd4f270c8750a00de9054ec2', 'c5662edb2182b5e27eb73d1187c37db28c98fba6', '2dc7741c3cd3c7fc0d0ae7b60cf7358f612e175b', 'a3dbe3e4512ae002efd52ec92ed5905d859a696e', '472cd41fa2ba2e520706f232cae12db4a7b5e60a', '25e9fa483a048607131a5a0e3287e8f457fb4807', 'c9b56cb026a38e39bb0228faac57accd6f65e6f7', '06a427e1688f92053a38c73cb4e0da25177c89e7', 'dc0ce66f5ab4c5173cdef951649044e4c4c05076', '962a8ffc7d72990a28d505f49a39108b4803c223', '18939eadc9c4460c8385e0591cde214a1ead067b', '6d4a87759917132913319960389f17fa1fe8b630', '309b906fed883e5efe4acf676c655ead21f6c17b', '7a064df1aeada7e69e5173f7d4c8606f4470365b', 'd01fa0311e8e15b8b874b376123530c815f52852', '6ad5f1d88534715051c6aba7436d60bdf65337e8', 'ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', '1adfa30bf112de20cb959014e44626d760aa8e4e', 'afd975a296886e89722891ad13c8dba0d26b1ed2', 'edabab811aaf44b7b626efe5278a32bddd3bb77f', '341880efaef452f631a4a5cd61bef5dae47741d7', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', 'd33deae7f654b07ac8a5c437a4fa018c29e6af17', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', '44d43bfbd23d55b1e7c4c4fd91fe101c0eaf1a06', '6b1b5079dec9b1164d3415a925c1b4aca53a5e10', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'f91175950edf3804ff1573f570b03db9b108dece', '6398cb8f2af1c988a097ed1e1cefb380195edfb8', '5bc67a8a47c796053d5ed77aaecd3cbbd4c5d4c1', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', '451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c', 'f78a911f516625d6b7b76a9a33c1eb14613341c4', '651adaa058f821a890f2c5d1053d69eb481a8352', '514e7fb769950dbe96eb519c88ca17e04dc829f6', '8e37a3b227b68953f8067215828dc8b8714cb21b', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '5ded2b8c64491b4a67f6d39ce473d4b9347a672e', '99e5a8c10cf92749d4a7c2949691c3a6046e499a', '29e944711a354c396fad71936f536e83025b6ce0', '16aa01ca0834a924c25faad5d8bfef3fd1acfcfe', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '819167ace2f0caae7745d2f25a803979be5fbfae', '51a55df1f023571a7e07e338ee45a3e3d66ef73e', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '687bac2d3320083eb4530bf18bb8f8f721477600', '62c76ca0b2790c34e85ba1cce09d47be317c7235', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', 'b7919bcfa38aa97514187501a23c983e8eb5482b', '68c03788224000794d5491ab459be0b2a2c38677', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '9405cc0d6169988371b2755e573cc28650d14dfe']}
{'paperID': '4678c9f7ce7d91bc1770e64856b4ab9e050af2e2', 'abstract': None, 'bibtex': '@Article{Sun2023CertifiablyRP,\n author = {Yanchao Sun and Ruijie Zheng and Parisa Hassanzadeh and Yongyuan Liang and S. Feizi and Sumitra Ganesh and Furong Huang},\n booktitle = {International Conference on Learning Representations},\n title = {Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication},\n year = {2023}\n}\n', 'references': []}
{'paperID': 'd60709222aa772732b9ea644bdb364971c0dea13', 'abstract': None, 'bibtex': '@Article{Zeng2023TowardsRC,\n author = {Yijun Zeng and Zhouxing Shi and Ming Jin and Feiyang Kang and L. Lyu and Cho-Jui Hsieh and R. Jia},\n booktitle = {International Conference on Learning Representations},\n title = {Towards Robustness Certification Against Universal Perturbations},\n year = {2023}\n}\n', 'references': []}
{'paperID': 'b6f933577c0e535ace8dcbab62a7308a2d04b980', 'abstract': 'Neural networks can be drastically shrunk in size by removing redundant parameters. While crucial for the deployment on resource-constraint hardware, oftentimes, compression comes with a severe drop in accuracy and lack of adversarial robustness. Despite recent advances, counteracting both aspects has only succeeded for moderate compression rates so far. We propose a novel method, HARP, that copes with aggressive pruning significantly better than prior work. For this, we consider the network holistically. We learn a global compression strategy that optimizes how many parameters (compression rate) and which parameters (scoring connections) to prune specific to each layer individually. Our method fine-tunes an existing model with dynamic regularization, that follows a step-wise incremental function balancing the different objectives. It starts by favoring robustness before shifting focus on reaching the target compression rate and only then handles the objectives equally. The learned compression strategies allow us to maintain the pre-trained model’s natural accuracy and its adversarial robustness for a reduction by 99% of the network’s original size. Moreover, we observe a crucial influence of non-uniform compression across layers. The implementation of HARP is publicly available at https://intellisec.de/research/harp.', 'bibtex': '@Article{Zhao2023HolisticAR,\n author = {Qi Zhao and Christian Wressnegger},\n booktitle = {International Conference on Learning Representations},\n title = {Holistic Adversarially Robust Pruning},\n year = {2023}\n}\n', 'references': ['dfacf0c04048c0a8c105976e9d2237ee269843ce', 'e23d0bf71547029968504de2ee1bb8888344c3b7', '753301a624a53029e16953d582b4e6f166ed4d99', '7a9ac680f286f56cd9206111416733bf0845618b', 'bc4c65be9564abb0c40f5e754398bba450889568', '532035e132e27ddc2a1469e771fa658cbcaed4b3', '9227d5897abbf297a34d447e94a802a714b8eab2', '99931b7b794f7d98c7b3f3eee862129133d123d7', '67f74fe9d46f88661573003f8f1f12967ae49fa3', 'a554a0aae55be5597de8f6ece0a4dd0bd5a0e5f4', '764eff31d9596033859895d9513b838d2c57a6fb', '18939eadc9c4460c8385e0591cde214a1ead067b', '3805147a98dab8f0c7667fed25490adbd2300fbd', '8eb599d5d7f1821b205e3b56fef5340b1622ba52', '2e3002f131e1815bda7a10303eff97f79dea01ec', 'b3ea2d9c8e5ea3b87ace121f0bece71565abc187', '8d5ed715390944c3bf07f826377ade48de4fba1a', '91a05cb84f1c7dbb0354da2ff11ae92549152435', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', '1eff01027877843f1b492c4abecdbbc112497d29', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '3636d3f0562f3ab5f5df68c1c9a23530c0fbce64', 'ecba2826cd7a51d4d8b9820591ff0fa6b41d66a6', '4a1004ecd34118116344633c7cdcc34493c423ee', 'a1e4e65842ebd88bfda7fcf35b2e275724a6216c', '1717255b6aea01fe956cef998abbc3c399b5d7cf', '2709a495f1a9d49bb852bbc512dd513ab158d0ad', 'ee53c9480132fc0d09b1192226cb2c460462fd6d', '8e0de06951b55273667db85a65cc1a6aff158a56', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '34cc3ceae5c3f7c8acbb89f2bff63f9d452b00d5', '3ac1df952ffb63abb4231a4410f6f8375ccdfe79', 'c2a1cb1612ba21e067a5c3ba478a8d73b796b77a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '7601b995303f953955004db7b9b8b206c0e02ff8', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '642d0f49b7826adcf986616f4af77e736229990f', '1ff9a37d766e3a4f39757f5e1b235a42dacf18ff', 'cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487', '5aa26299435bdf7db874ef1640a6c3b5a4a2c394', 'eb42cf88027de515750f230b23b1a057dc782108', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '8a097304b1c087d8ad5d25db48bcf6ad91e79ea5', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '47cbf414220626972ce2b609bf13514f98beef43', '1b7db8ad49f94da9b90db89bede5f27644bb9911', '28135fd3e80dda50a673cd556f10b9b972005d27', '384ce792cf2b2afbe001f2168bfe7d5e7804c736', '02227c94dd41fe0b439e050d377b0beb5d427cda']}
{'paperID': 'f70801100c8d523bed156895120d53286d7aa49e', 'abstract': "Generative adversarial networks (GANs), trained on a large-scale image dataset, can be a good approximator of the natural image manifold. GAN-inversion, using a pre-trained generator as a deep generative prior, is a promising tool for image restoration under corruptions. However, the performance of GAN-inversion can be limited by a lack of robustness to unknown gross corruptions, i.e., the restored image might easily deviate from the ground truth. In this paper, we propose a Robust GAN-inversion (RGI) method with a provable robustness guarantee to achieve image restoration under unknown \\textit{gross} corruptions, where a small fraction of pixels are completely corrupted. Under mild assumptions, we show that the restored image and the identified corrupted region mask converge asymptotically to the ground truth. Moreover, we extend RGI to Relaxed-RGI (R-RGI) for generator fine-tuning to mitigate the gap between the GAN learned manifold and the true image manifold while avoiding trivial overfitting to the corrupted input image, which further improves the image restoration and corrupted region mask identification performance. The proposed RGI/R-RGI method unifies two important applications with state-of-the-art (SOTA) performance: (i) mask-free semantic inpainting, where the corruptions are unknown missing regions, the restored background can be used to restore the missing content; (ii) unsupervised pixel-wise anomaly detection, where the corruptions are unknown anomalous regions, the retrieved mask can be used as the anomalous region's segmentation mask.", 'bibtex': '@Article{Mou2023RGIRG,\n author = {Shancong Mou and Xiaoyi Gu and Mengsi Cao and Haoping Bai and Ping-Chia Huang and Jiulong Shan and Jianjun Shi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection},\n volume = {abs/2302.12464},\n year = {2023}\n}\n', 'references': ['9e93ea471ade297fa55d836241428e2174d43fbe', '41486714f22dfa7837366d253479104b8b053ca4', '13fab6dbb9d0f3eaac0b45a52c140165ae25b8b6', '11709bfadfd6bbb371f4077bccb7c26d93c39cdd', 'fdf7012ebe9d4c4d2d93004613e7a19f69a83a93', '90cf38a7431d920843c25f4bc8ea8feca99e83ff', '95a26eafabf06b1fc5dec6c460a927cf5964e97e', 'fc086bf5f6d1627153b68abdd5a4450e141b4ca3', '23ad8fc48530ce366f8192dfb48d0f7df1dba277', 'd16e066ba0c50522c96a937a8f33068bf5722f90', '913d43b69fd4ddee2ff64d3a1e6402d3787e2b7e', '39c2ae603902ea664adf9e74914117f79df2e612', 'a3ef5a321876738a6b257de5e1eebc4a8aa5b907', '2e8d62277e40d465343e8dfb32ecc246f320540e', 'c160e1e5431d2965b0a38d68de630a5867b18bc5', '4cc32db67ff82cf1aa160631c35bb315c5add749', '29858b40a15704398aecdca6bd2820f2fcc99891', '20334266b089240d6d723efdc17511d4929a434a', '4106abc70ced2ff666e94ea9b668128b0f982cce', 'd82800c79dd335297336fe10b1a60d47706e4296', '2f57b14bdedc16dd87aad2d9be6bc60690392fbf', '7101bc1c316740d99cd87185586829291a983a1d', '88bd997588ebcbade1f05b9e823503ce61fa8c80', '1a00dc525da31292e3734cbae2de681f114e30b1', '7ad9b3ace5e2b9783100cd4205c94d7873723207', '56e3b48e72f9452cb862de1b76c51ade2b681c43', '3a3eeceba9dd6dc4fcc7971eeae2d39af5e51215', '3aa681914a7da79f7d7293f51a058eefe61c8bb7', 'b0f49ada8e9454048faf17f66d5e7520d5e46e98', 'f88cfc38dec02dcf050eb1f56d2d59d90b24e04c', 'd468363414b9ec6507ff24cbeefecb4828f52f6b', '62931b3e0dce8748364e19c87ef318e22ec59c7f', 'd65eb30e5f0d2013fd5e4f45d1413bc2969ee803', '2bc1e26c454045e88a6db8cf41c4741fc734fe06', '906b374e53357650477eebb80c6a8896b884b4bf', 'ed516c62512a5189c185738172ec0b151912e931', 'f6284d750cf12669ca3bc12a1b485545af776239', 'ceb2ebef0b41e31c1a21b28c2734123900c005e2', '308898a6bcb180f8fde1901a89e8baa213eb4954', '9c24454b071bc8e96ea46c5064a7bddf07cca464', 'a997f1ecd85e1467d11252741d188fac8db22722', '0535625be630c6a67f4c244ebf3aa61ad088fc70', '2a417a16473e2bcb1c98cd7814bc106760925e60', 'eb60fe884c53b420edbce57059b242cfcbae0f7c', 'b3acb6f183b5f4b651f53c0eec5cb5c805224ac1', '904decf94f5495d1488e2bf22e3ed4df500ce4d5', '6b0bbf3e7df725cc3b781d2648e41782cb3d8539', '80b2dbefdef33858dc6c78b0df1f8179e6c6d77a', '34e38559df559bfedfc3e1edc2b120ab2b5d444e', '809c713a43e67ff60f2d9bf8b6571ca2a293ef08', '744fe47157477235032f7bb3777800f9f2f45e52', 'd21ebaab3f715dc7178966ff146711882e6a6fee', 'b0351087a2b85f70e60fc79dfa4110b4985cc00a', 'e163a2e89c136cb4442e34c72f7173a0ff46dc79', 'c27824577439a90d4e5d05ed9a7147b132d7e92c', '3b9bfdeeb9ac6ada5a5833b1f179cb97f3e6804b', '559a52d27ff8e3ae0cdf1e7948c137ff566285c8', 'fc7822f56dd255a872326b9536a0821bbf0277dd', '8a3bf4d403a39ed33f0fa8cf78dc906d6130595f', '7d0effebfa4bed19b6ba41f3af5b7e5b6890de87', '915c4bb289b3642489e904c65a47fa56efb60658', '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '3e385b6e1b2f43464e9a0e60fdfb0cda108fe60b', '261a2cb5ac0b550f8174d3d75f436c6d7b73e872', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', 'a83cec6a91701bd8500f8c43ad731d4353c71d55', 'a657d39356c9d3b9c3c70796ad6f8b34c53d2da2', 'c014c79fa37cb8dc4565d5db9c4879d98d5f904b', '2f087cdde2ea34b911b3e9917b90fd3d42070eeb', '01625cba9f8a783994377d4f35aa765242faab4f', '3a6c9b81cc77f2d66dfec3d567a125b526c58ddd', 'c8831d7d318b8d59f9b958d250a58f253f08bd8a', '3a1b983a87a4116803fff779ecf1cd11a4b07539', '040c161f21e0fa57ac192ac826310f55d60277b0', '53c480c422d528fc379ba1cb160bd4cdbf38bffc', '931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04', 'df24c3011fc42b72195e876ce052a0a072a1d923', '8d23d6432c27843040f51dcf0191877f7a9994e9', 'f66cfd6fa9bbb154f9acc5a22438c04f9874faf3']}
{'paperID': '5860267c5b1761fa5ff8dbcde980edff4c3b2f2a', 'abstract': "We show that combining human prior knowledge with end-to-end learning can improve the robustness of deep neural networks by introducing a part-based model for object classification. We believe that the richer form of annotation helps guide neural networks to learn more robust features without requiring more samples or larger models. Our model combines a part segmentation model with a tiny classifier and is trained end-to-end to simultaneously segment objects into parts and then classify the segmented object. Empirically, our part-based models achieve both higher accuracy and higher adversarial robustness than a ResNet-50 baseline on all three datasets. For instance, the clean accuracy of our part models is up to 15 percentage points higher than the baseline's, given the same level of robustness. Our experiments indicate that these models also reduce texture bias and yield better robustness against common corruptions and spurious correlations. The code is publicly available at https://github.com/chawins/adv-part-model.", 'bibtex': '@Article{Sitawarin2022PartBasedMI,\n author = {Chawin Sitawarin and Kornrapat Pongmala and Yizheng Chen and Nicholas Carlini and David A. Wagner},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Part-Based Models Improve Adversarial Robustness},\n volume = {abs/2209.09117},\n year = {2022}\n}\n', 'references': ['da26630c9d931952f1d68308148f1289f5332607', '741decb682ceccec1d06e53e6bc65b159d4e80f1', '5c1dd63a45dc56009d1d499c8c2f4d7b9953a507', '9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342', 'aac329026fb5f08f54b9f06f35ea2e0c3b664a76', '762752eb9a9a92b028026b17c46d50474ddf3f06', '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', '1bcbf1efb3f81f0e777b4b754cf5b9789841d12f', '569ef4e3c9f5ae968fc94000c12d212a2b679907', '123877109e4a203b0db35984aa4808583573ab51', '5c63fc87400a4d3afea63ab8a068a47249f815c2', '20bf6524fded4bb666ae68955d1e48ed93ee446e', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', '5e7f38a31083634c95b1a3df4f0c8da4cc09125d', '6b4ba8d9d4ebf0dbe645fbf5ae6c0a3ede5a598d', '59f3d57a9821e07a73b4274b7a94867517414322', 'e6c561d02500b2596a230b341a8eb8b921ca5bf2', '6d4a87759917132913319960389f17fa1fe8b630', '8733fe2371b615609b04e2e910b1ecfa8e77cbc2', '193092aef465bec868d1089ccfcac0279b914bda', 'd6dcbf0bb628658df27abd297c03147115e476e4', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '0be998265251d163d797f1b69552c5be7d3f61bc', '49b64383fe36268410c430352637ed23b16820c5', '79f36f8b6590fd59ac3e75726ccadf2f5c32b124', 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5', 'aa5741c74b7fac10680c1cfbdd49d9ffb5751a68', '676e40050453ddeb1387f8314478c0ac3681a8c6', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '41071dbbbcbb27af3fec70de045f19c28535f5b7', '87bed8d35625a75d8ba1953bb6bb71a3625020c0', '0f9b94e869e655b80cacfb24f0f5585aa929e863', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', '1769d762def8bfc29b514581ea613d226b76fb63', '1b9c6022598085dd892f360122c0fa4c630b3f18', '9217e28b2273eb3b26e4e9b7b498b4661e6e09f5', '651adaa058f821a890f2c5d1053d69eb481a8352', 'adecc9cb7c4e71a401099b26ed5420b8d4f4e90a', 'fc027fccb19512a439fc17181c34ee1c3aad51b5', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'e7867244de690a1ae11a7a6d5a021e868fa75a3c', 'b022f2a277a4bf5f42382e86e4380b96340b9e86', 'c8c494ee5488fe20e0aa01bddf3fc4632086d654', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '988c8578178fa263c2524d85292072c6a13a6121', '48e8cc2d5651053c0bc2fcd20787cca0782f2b94', '244bae85fda807361a51d4b26a14ecb2b2f8776b', 'ed9a133865295aee70c62f8764a904be0498350e', 'caf202fd5833b1ef635923e79608e1a48d7539f9', '9b9ea724dd5ad44ede5fa22bb8834ba7e42a9292', 'e79272fe3d65197100eae8be9fec6469107969ae', '0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a', 'df24c3011fc42b72195e876ce052a0a072a1d923']}
{'paperID': 'dc15ef5398d7506ccf9c7f526f6cab8816e207d5', 'abstract': "Ensembling certifiably robust neural networks is a promising approach for improving the \\emph{certified robust accuracy} of neural models. Black-box ensembles that assume only query-access to the constituent models (and their robustness certifiers) during prediction are particularly attractive due to their modular structure. Cascading ensembles are a popular instance of black-box ensembles that appear to improve certified robust accuracies in practice. However, we show that the robustness certifier used by a cascading ensemble is unsound. That is, when a cascading ensemble is certified as locally robust at an input $x$ (with respect to $\\epsilon$), there can be inputs $x'$ in the $\\epsilon$-ball centered at $x$, such that the cascade's prediction at $x'$ is different from $x$ and thus the ensemble is not locally robust. Our theoretical findings are accompanied by empirical results that further demonstrate this unsoundness. We present \\emph{cascade attack} (CasA), an adversarial attack against cascading ensembles, and show that: (1) there exists an adversarial input for up to 88\\% of the samples where the ensemble claims to be certifiably robust and accurate; and (2) the accuracy of a cascading ensemble under our attack is as low as 11\\% when it claims to be certifiably robust and accurate on 97\\% of the test set. Our work reveals a critical pitfall of cascading certifiably robust models by showing that the seemingly beneficial strategy of cascading can actually hurt the robustness of the resulting ensemble. Our code is available at \\url{https://github.com/TristaChi/ensembleKW}.", 'bibtex': '@Article{Mangal2022OnTP,\n author = {Ravi Mangal and Zifan Wang and Chi Zhang and Klas Leino and C. Pasareanu and Matt Fredrikson},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the Perils of Cascading Robust Classifiers},\n volume = {abs/2206.00278},\n year = {2022}\n}\n', 'references': ['db5c39d4ca9c4ffb33886782c4527fc5579668c5', '5eb4b7f426094cce9e959fd0075475ebbf0d4f9c', 'd4aa4fd1d0ea6da1905640adb17c67db435f9f12', '04eb7cc8b9a84999ba45d56bf87b170c9ad8082e', 'c2e83247a3a20d25f66262712673230f7cb68334', 'b3b88cb29938a5445edd543b0498a51c4931f840', '2a88cc8cc9562b4addec03ba16b35cb4d3baaa43', '656cec38e5295b94a17cf82e9ee9dd7b38296b49', 'ef042af146283c59fa9e9990ce37df538fc12faa', '064ec3a5e121cf997c3b57b23867c756cd141c04', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '988a378f640eb7fb681f977d6cb1e0c830c07b4c', 'd4473a41c9f7b4095599bec14ea0a88e7041e737', '676e40050453ddeb1387f8314478c0ac3681a8c6', 'f20de741e2d4ece239261de010d6d9beca3b26b9', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', '20f85256555ad612148e52f9363e52f9d661728b', '9db631435f7f79646a4e0a1841fbeb3340e44261', '966e3c7a65ec75a6359b55c0cecaf3896d318432', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'a0456c27cdd58f197032c1c8b4f304f09d4c9bc5', 'fc80cdf18198dc5677ccabe501b6f7209c28483c', 'f0421492495a14eeb1d97bef61456afa46d2ed12', 'ac1865efcaa3ae8992049f103225b4ec990bd7f3', 'f6aee5a366a798ca08c12013cffcbb78740bfd01', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '8c87dcaba827e5c1683086c3118fd9bffa7cff5e', 'abstract': 'Models trained via empirical risk minimization (ERM) are known to rely on spurious correlations between labels and task-independent input features, resulting in poor generalization to distributional shifts. Group distributionally robust optimization (G-DRO) can alleviate this problem by minimizing the worst-case loss over a set of pre-defined groups over training data. G-DRO successfully improves performance of the worst-group, where the correlation does not hold. However, G-DRO assumes that the spurious correlations and associated worst groups are known in advance, making it challenging to apply it to new tasks with potentially multiple unknown spurious correlations. We propose AGRO -- Adversarial Group discovery for Distributionally Robust Optimization -- an end-to-end approach that jointly identifies error-prone groups and improves accuracy on them. AGRO equips G-DRO with an adversarial slicing model to find a group assignment for training examples which maximizes worst-case loss over the discovered groups. On the WILDS benchmark, AGRO results in 8% higher model performance on average on known worst-groups, compared to prior group discovery approaches used with G-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and MS-COCO -- datasets where potential spurious correlations are as yet uncharacterized. Human evaluation of ARGO groups shows that they contain well-defined, yet previously unstudied spurious correlations that lead to model errors.', 'bibtex': '@Article{Paranjape2022AGROAD,\n author = {Bhargavi Paranjape and Pradeep Dasigi and Vivek Srikumar and Luke Zettlemoyer and Hanna Hajishirzi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {AGRO: Adversarial Discovery of Error-prone groups for Robust Optimization},\n volume = {abs/2212.00921},\n year = {2022}\n}\n', 'references': ['de4be9e0fa2f660eefb2d4c2a27c146d3e654a85', '53ed38af7df82242e0d7b13c19c814762de75bca', '14a3aae8060338e3fbefc2af694890b019874d4f', '3d112021bb96568c830c03c779f232da3788067c', '0567131ec1f839240179927ceb61f51bdd173055', '1cfe67bad95a16bc249941b829d113d830031cf5', '54020e5fe48ebb250f27d744e20a63cac2988a84', '1420c75750c06c5eee473118389a6901847ad18b', '56b30c6bd9dc4a2416ab3b74ad97dbb7a2904229', '1e57462f93d78279549a8508e691dc4920151b35', '9289826beb6206eeaf500105f7329d6d5a495d8a', '216d093cb2ad81bf55c21dbce2217f2b9032e67b', '05b2b28ebd8bcf0de4fe1cea9d096f20bbd3ab5f', '722ad6ac92286507437b31486f47987d6ece05c9', '714fc6626c527e05f2a31626d067d46520c6740e', 'ef685a63f00ebd44fa7b4c10282ea264807a08e0', '1e58f1e94a03ef6434ce5e3360781d546f8a2f5b', 'c5e4eafd85949e6aac9d8e98d5e03b2acf444046', '2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c', '15e71e497a67423bfedd0d63efe423c4660e5053', '40848b41ed8c9c255ecd8a920006877691b52d03', '6116cde7fc65eff29a1b8118b034f87909f2237f', '734f85727161f27bc7b295f0140a905363202d3f', '8c96b865bbe1f597cf2c644e20ae46eab8e7caad', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', '00325cb5408da77827951abd3fa93ec3bd019608', 'ff070fc5eff18a737545a0f96a068e9ab5a0f234', '767c6702045f2290012a259744db9edb4d55bcb8', '04422085a52050516b9741e0fd1fda964b73dd53', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '05f5f8b2065a520846d89771ebaea2bb1534e9c6', '08cb7d416bcd1b1e4d6492a0cd0b01424abd9515', '35e6783307f82d1faa39be0653431305abec7271', '1d8dcf5e99557c7d6b7015a280e4043439ecd1a5', '193092aef465bec868d1089ccfcac0279b914bda', '207da6d2c07289bf72a2b5974bb3f011ebb5dd0d', '4ce2f55585f3156e332721b8ab4f449389dc2a3c', '72a1d0256b38dea6c3e7d10a63eacc51abdc96da', '77568c594470f9aa029f92774e2c12ab0451d9bb', '4e14cf96c60e3d35b05e3a740c7c6bbe52f14677', '1d738fa77de08592d9b77754e48cc63e276e5c0d', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'fc09d6486be1c9bbfbef4165ce3c1ab664e5d084', 'b611a8095630557229dc5fb6b07c272f1cd614da', '42ed4a9994e6121a9f325f5b901c5b3d7ce104f5', 'fac1c95993e86f92c7adeec7f72e06503e4190d5', '4299f004df8a927dc4132f5f9a98be574c74c2a7', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', '175b58fe7e49bb5c0c771b73f8834bcff21b59c7', '641076d8786511559cd31b96fc2c93426120ad47', '413a03a146e6f7b16c11e73243d83e6f1a6627a3', '8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6', '451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c', '2997b26ffb8c291ce478bd8a6e47979d5a55c466', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '5ded2b8c64491b4a67f6d39ce473d4b9347a672e', '07b5093aace8e485e7d23b83edb6351618138127', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', 'fc24d32ab6acd5f1d2c478a6d0597c85afb28feb', 'f04df4e20a18358ea2f689b4c129781628ef7fc1', '71b7178df5d2b112d07e45038cb5637208659ff7', '687bac2d3320083eb4530bf18bb8f8f721477600', '649d03490ef72c5274e3bccd03d7a299d2f8da91', '583b55367f787eb0c4e295707b642e63547b9806', '9360e5ce9c98166bb179ad479a9d2919ff13d022', '359c56a068e4a84a9f3b78f43b53fe3b333c0ba0', '68c1112480798d6d184853512d5c32c345741fc0']}
{'paperID': '7195ed3c7f11220f29634cecb68b1d39db2e36d9', 'abstract': 'Neural text-to-SQL models have achieved remarkable performance in translating natural language questions into SQL queries. However, recent studies reveal that text-to-SQL models are vulnerable to task-specific perturbations. Previous curated robustness test sets usually focus on individual phenomena. In this paper, we propose a comprehensive robustness benchmark based on Spider, a cross-domain text-to-SQL benchmark, to diagnose the model robustness. We design 17 perturbations on databases, natural language questions, and SQL queries to measure the robustness from different angles. In order to collect more diversified natural question perturbations, we utilize large pretrained language models (PLMs) to simulate human behaviors in creating natural questions. We conduct a diagnostic study of the state-of-the-art models on the robustness set. Experimental results reveal that even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop on the most challenging perturbation. We also present a breakdown analysis regarding text-to-SQL model designs and provide insights for improving model robustness.', 'bibtex': '@Article{Chang2023DrSpiderAD,\n author = {Shuaichen Chang and J. Wang and Mingwen Dong and Lin Pan and Henghui Zhu and A. Li and Wuwei Lan and Shenmin Zhang and Jiarong Jiang and Joseph Lilien and Stephen M. Ash and William Yang Wang and Zhiguo Wang and Vittorio Castelli and Patrick Ng and Bing Xiang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness},\n volume = {abs/2301.08881},\n year = {2023}\n}\n', 'references': ['e262e1aa4d1efb909705a2dfcf9df53d76cb4e12', '13a0d8bb38f739990c8cd65a44061c6534f17221', '094ff971d6a8b8ff870946c9b3ce5aa173617bfb', 'b46b91eaa9dcea3a50e19c726bca29cd4d25e389', '51000d9f79be0eefd7972fe94e3c71dddc90d2c6', '1b6e810ce0afd0dd093f789d2b2742d047e316d5', '2b5d234efd26e7377698cf16c901601a3d3c4e56', '56b30c6bd9dc4a2416ab3b74ad97dbb7a2904229', '8436897e713c2242d6291df9a6a33c1544d4dd39', '18ad0da02b2207288a3fe7c19ee8d223a9ee3ef4', '0b3863c21a7fb5ac61a447611cba0ec9ce1ab4a4', '5fbcfccd3736969d95ed660d8e6962c86b7a9113', 'acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269', '9cd3d6eef7c574830be410598c3024191ee974d4', '7747ecbc26b1688e6cad1a6ce83914efa2a3c04c', '84b26030b648b6d79177bdafd3e896b1dda9f91e', 'b769b629c8de35b16735214251d6b4e99cb55762', '10b15a695f837fbdc2babe0c38f8702c10af7bfb', 'f4cc7fa4f6a638cf79bf52341a05edcb5434fe97', '9b54941de1e21826ecc28b32730ac3f69991ede4', '232b40980acb55afa89ec50dd9806a5e551f699b', '29184b8befb6107e5a4d41bb8da4ace92fdd79b5', '3ff5f99c7924b4b28669f09ce3bfbf26fd92abc1', 'acf8a1040034820bf99379a3422815f4e0859ec9', '5868a7bfe6a4590d332ca66b8097dbe5490c8a73', 'a2462ff5546b45b1cc7cb50ffc50c7ddececca65', '6b2ccee1ac5823ed3d0cf97c75470ea335916c07', '8b2cbb2f101b025c16e12d0d7628f65e5378e10d', '399e7d8129c60818ee208f236c8dda17e876d21f', '7c41e58832f3af5fd9e09674924d6b5f822e8eac', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', '33ec7eb2168e37e3007d1059aa96b9a63254b4da', '35e6783307f82d1faa39be0653431305abec7271', '0c5bc409e62e65f86838968a2a7cdae5fa0b288b', '207da6d2c07289bf72a2b5974bb3f011ebb5dd0d', '395de0bd3837fdf4b4b5e5f04835bcc69c279481', '3cfb319689f06bf04c2e28399361f414ca32c4b3', '1fa9ed2bea208511ae698a967875e943049f16b6', '47f1eb0dc42189ba7cf21b76598c8217eb1b6e05', '75acc731bdd2b626edc74672a30da3bc51010ae8', '5019dbe8d1da5f128f4f373d6849095cf18fd519', '077f8329a7b6fa3b7c877a57b81eb6c18b5f87de', '8e773b1840b894603c06b677a0f15ebcf0f26378', '2b110fce160468eb179b6c43ea27e098757a56dd', 'c8efcc854d97dfc2a42b83316a2109f9d166e43f', 'cbd569036fc72ae7ff747350b91816440282596b', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', '5ded2b8c64491b4a67f6d39ce473d4b9347a672e', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992']}
{'paperID': '3cd5fbf89d3aa9a1872b0b89bae4aece9bb4392c', 'abstract': None, 'bibtex': '@Article{Gu2023MinMaxMB,\n author = {Alex Gu and Songtao Lu and P. Ram and Tsui-Wei Weng},\n booktitle = {International Conference on Learning Representations},\n title = {Min-Max Multi-objective Bilevel Optimization with Applications in Robust Machine Learning},\n year = {2023}\n}\n', 'references': []}
{'paperID': 'c5283314492adacea2de8a056f5529c506d0fef4', 'abstract': 'We introduce Noise Injection Node Regularization (NINR), a method of injecting structured noise into Deep Neural Networks (DNN) during the training stage, resulting in an emergent regularizing effect. We present theoretical and empirical evidence for substantial improvement in robustness against various test data perturbations for feed-forward DNNs when trained under NINR. The novelty in our approach comes from the interplay of adaptive noise injection and initialization conditions such that noise is the dominant driver of dynamics at the start of training. As it simply requires the addition of external nodes without altering the existing network structure or optimization algorithms, this method can be easily incorporated into many standard problem specifications. We find improved stability against a number of data perturbations, including domain shifts, with the most dramatic improvement obtained for unstructured noise, where our technique outperforms other existing methods such as Dropout or $L_2$ regularization, in some cases. We further show that desirable generalization properties on clean data are generally maintained.', 'bibtex': '@Article{Levi2022NoiseIN,\n author = {N. Levi and I. Bloch and M. Freytsis and T. Volansky},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Noise Injection Node Regularization for Robust Learning},\n volume = {abs/2210.15764},\n year = {2022}\n}\n', 'references': ['9ed96bd21a2a70cbb3466f88c84e965b702eb114', '6cd064b3fb736cef8404ab24227add67c7c23779', '4150b24323de1ae871887cccd66f4b759242a5e5', '5b04e1f31c671e19cdd752c4c9a521c62b3ee6c7', '30ddacf5db38b5c9b4788c97bfc2dd73ab6ca040', '02b1607af35b48f0bd716367caf6a7428b969369', 'cdfee4355cae3299b06f3f98718df9bb64a899bf', '3636d3f0562f3ab5f5df68c1c9a23530c0fbce64', 'ba618ec05a9dbef75310c5e4bcce8a559e0270b5', 'a39149da8f9ec8fc398b9f1506c61e17650e3118', 'f152cfd441a52c9ceb2ae724d601fb4fb9ec77ea', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'aaed2a884af95852580fdedda4ea768f2effeb46', '54ddb00fa691728944fd8becea90a373d21597cf', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '6f5a0fbb1473e95d0a14ef2081985d16bc063bfb', 'd7db42b8ef68e84526922ae50420b062c2397dad', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'eb42cf88027de515750f230b23b1a057dc782108', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '829b5fb0d890b4adc40b03e1ebb02d5214ac5fe5', '38f24db7abe5c2a3ffe900a56142bf000c0a7672', '52b7bf3ba59b31f362aa07f957f1543a29a4279e', '62a134740314b4469c83c8921ae2e1beea22b8f5', '48e1de7d085808004d5f0493d486669a3d2930b5', '3e6bea2649298c68d17b9421fc7dd19eeacc935e', 'f68b81423b8c3d4a3aabf32d001ae6289a6226d4', '34f25a8704614163c4095b3ee2fc969b60de4698', '5f88150e657345dffdf3e6dea42fb18080d66531', '162d958ff885f1462aeda91cd72582323fd6a1f4', '11823e4fd9aa67ff429c6a63d99495187547c093', '2ab1cd775d54c9a33a1827292d286a98c64e52f7', '9f660a2067f08f561afa55c2d5a2e757e6e75bc9']}
{'paperID': 'd10e3ef7fa49e13915d0e93c0877f2f2a26cd97a', 'abstract': 'We study dynamic algorithms robust to adaptive input generated from sources with bounded capabilities, such as sparsity or limited interaction. For example, we consider robust linear algebraic algorithms when the updates to the input are sparse but given by an adversary with access to a query oracle. We also study robust algorithms in the standard centralized setting, where an adversary queries an algorithm in an adaptive manner, but the number of interactions between the adversary and the algorithm is bounded. We first recall a unified framework of [HKM+20, BKM+22, ACSS23] for answering $Q$ adaptive queries that incurs $\\widetilde{\\mathcal{O}}(\\sqrt{Q})$ overhead in space, which is roughly a quadratic improvement over the na\\"{i}ve implementation, and only incurs a logarithmic overhead in query time. Although the general framework has diverse applications in machine learning and data science, such as adaptive distance estimation, kernel density estimation, linear regression, range queries, and point queries and serves as a preliminary benchmark, we demonstrate even better algorithmic improvements for (1) reducing the pre-processing time for adaptive distance estimation and (2) permitting an unlimited number of adaptive queries for kernel density estimation. Finally, we complement our theoretical results with additional empirical evaluations.', 'bibtex': '@Article{Cherapanamjeri2023RobustAO,\n author = {Yeshwanth Cherapanamjeri and Sandeep Silwal and David P. Woodruff and Fred Zhang and Qiuyi Zhang and Samson Zhou},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Algorithms on Adaptive Inputs from Bounded Adversaries},\n volume = {abs/2304.07413},\n year = {2023}\n}\n', 'references': ['5259300e83a2a6b93c6ce3d72bf67b9a4ab5a967', '6b559232a26086f5c2ef3724999d7b4eb02cf24f', '0792d8ad34761764bbd72edc900fa459a4933dfa', 'c7ce06fad686f60649f0ebe38702ad16ab3353bb', '6fea58dbc97782243ac3d040c4e01492da141207', 'c45f6ea40c27e5bd6f5a59827c74a217ed93a33b', 'aa9988bf55a092f8050b11818ca3647543263351', 'e43355403bbf5b6d3c11da20f1e3c743ecbd8eb3', '3562b7c2d0286b55e95d163ce3e49146695a5e9e', 'c3c424efea89be2401295fab842b149b81367c6a', '729a2228220b349ab79e58d4f295d56155d14adf', '5cf1c3b9d3a9cc68d23e783dcd28bbacac042611', '3a5af79aadeef874c4afdea3221676db027e4445', '0de0bbcee7df3f4b6255eb443ad77ef50f829e11', 'dd13a6c091a258130c903371a75f08700df04596', '67eb93329588502db6bfd86115677fc19604a90a', 'a91634efce426bb6d0ef1bd2ababb925fb503165', '7edd8f4e0fbeecc3846865b3e838140a1ffa6a0c', 'ddf2e411971b31607c5ae8dcddb72c43c9f9a0aa', '67bc33577fb46cf9a47609620af8f2e0d82ebc32', 'b2d4cdf364557c4454c1ce8de25727c9fa5b8dd0', '70792aa3bb6fb98494d7fdfbb39387966b145546', 'a953cacb510035198235a10235121e42c8eef579', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'dfd18b71f5c53ec2a95fcbe327cf7710da3b4851', '99e5a8c10cf92749d4a7c2949691c3a6046e499a', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '78aa018ee7d52360e15d103390ea1cdb3a0beb41', '53b047e503f4c24602f376a774d653f7ed56c024', 'e42b526085ac96d83bbaf6fd6918b1cc6ee85d3a', '50645e3dc912d597e89d59bffb96ccc0f8e1aefa', '81fd20c2b903d979075e0c6a59258b0a84213095', '80d43be894c2819a7a4c8908f8eaa37ad2b6cb9d', 'c8be69b5a68f443424905adda669eb3145ee24b1', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '5fd338baae2a1e2918e56064f387b47e38d8f927', '8963b5c1a754741d6599b95e47818fe3b79a6a51', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '9004e445894afbd58dc275b2b274bbd6b112c239', '2c1fbecd90a0dfc3233a871fee043065419e286a', '4f025554aae684dffed8d08ef8077c644175d7e1', 'f8558a09553fc35415b271019be9f7d44354073e', '71d1ac92ad36b62a04f32ed75a10ad3259a7218d', '801fed8c4a80c620a35606f8b75147aa0f8950ae', '5f04cd3a7c5820b3a0a2d726ee698bfb2a6bab9d', '13dec17a485061c130143ea020d1512d8cb2741c', '238a0814109dc166f1a1c0c3b5c33bc59250ae3f', 'e4ce10063cd25447dcde75c2d9ce327446ced952', '24763030fb1e9813dad51d28bea9c5d1414f9cda', 'c982cb13d2a870ca09a2a1f0bf3c602cc528724e', '0f84bab544302935f59d1ce31f02fdc42a80cb66', '1d9a1d649569aaa256fb8f41ef87f2825546ace1', '4c440d3f4f302ff130ea1fb102a603e451b08efd', 'fc041bb3d34442911794d6e3748b87d1aabba204', '6978959aa7011b437292556a923750fd91bca799', '444d70e3331b5083b40ef32e49390ef683a65e67', '3c282296103f0016403c6b5377e750b8c02cc193', '69ca0f0c77f8c065248cd9a52489b06962620b9b', '43bfd29bcc2d88f615b40c92ee91e57c826f87ce', '50afda9f6a9a92a6c1cd678bc4a659f36114defc']}
{'paperID': 'ac9c07f716e43db03b088d3e17bc9093c271662e', 'abstract': 'Post-hoc explanation methods are used with the intent of providing insights about neural networks and are sometimes said to help engender trust in their outputs. However, popular explanations methods have been found to be fragile to minor perturbations of input features or model parameters. Relying on constraint relaxation techniques from non-convex optimization, we develop a method that upper-bounds the largest change an adversary can make to a gradient-based explanation via bounded manipulation of either the input features or model parameters. By propagating a compact input or parameter set as symbolic intervals through the forwards and backwards computations of the neural network we can formally certify the robustness of gradient-based explanations. Our bounds are differentiable, hence we can incorporate provable explanation robustness into neural network training. Empirically, our method surpasses the robustness provided by previous heuristic approaches. We find that our training method is the only method able to learn neural networks with certificates of explanation robustness across all six datasets tested.', 'bibtex': '@Article{Wicker2022RobustEC,\n author = {Matthew Wicker and Juyeon Heo and Luca Costabello and Adrian Weller},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Explanation Constraints for Neural Networks},\n volume = {abs/2212.08507},\n year = {2022}\n}\n', 'references': ['ce93e2210ccb54b911d900341295157b1fde9271', 'a81374107c36b4a8d4d8f2d282722cc14a769a8a', '4ed4711cc471a0f4515b93ad9c6add184d625b22', '3c885f38e82f9c73a7de43adb1ea47b639dbfbea', 'd17761ca69acbef07f92f2477be488b99d1c63d7', '37bc09cc6c4fa10bde98614c3fc13a67306b5db1', '4db97295efb7f6e2dfabac5fcb8570140740dfe9', '20bd4b46aa53c0edc1bcee7e982b454771198a22', '41e613ebea2e5ae78899eccac9339bd524d4e6a1', '7e63be5285e6596fbbc6c56bc89f7b6fd8bbe8c5', '2034bd333f857023dbbca04db470aec0b306d94c', '92923234f562dd589e7bf5872898ecaa9ef54501', 'acca8fe674c0968b40e7329c2b53cf7048683e4c', '8bdfc8d09bfb4e7ff63faeb25d0e3514ebdc2d18', 'dfc09849e6e9481a6ad1441b4409cc271cadfb63', 'af8376f8114ce2de4440a33445b43b6579df28c5', '84e729fe76dc90521b0512df11e49ab577ce0d6e', '95d4cdaec450c0e26e45f13e231c075c1809ac7c', '31bfb37c0dc6d68ff9261b785b042a039b9156cb', '0d590420f5e0ed2fef653a89d5f2ed8a8ca1e798', 'e656b0376ca11f533ea01097c70f98c0ff655c00', '653864b10564ab4712c07a3d4043a1d794b13c46', '1b0f4bd3872bb590d457990ac2b26b29f770fc44', '3b91867718169e6ff4e4d70dd61323218111278a', 'a0f0a94927c0013fa924ee43c8ddbace1d71e3fb', '28ae8aa1cb2346036c2af817e21569003eabc862', '9972d742b8f197d2086906e7b06bb03900212bbd', 'ef7de986f5f5ed664938a5eed433a4c30dad6493', '9a044eec8de883d45bd748ef0694324f8b1a7812', 'bc00ff34ec7772080c7039b17f7069a2f7df0889', '43a4a354b67ab6d5531355a368094815d2d2593d', '8dc8f3e0127adc6985d4695e9b69d04717b2fde8', 'c708ff4a7f19b6747d775a2dc49ffda010dee339', '75339d34bdac0d21a41461228ec6088eecdf857a', 'd21fde0f55ee0285c66334d37b8920c867959784', '1d8f4f76ac6534627ef8a1c24b9937d8ab2a5c5f', '9db631435f7f79646a4e0a1841fbeb3340e44261', '651adaa058f821a890f2c5d1053d69eb481a8352', '9de69a46e6c619255eeffbfbb6c7b7163690eb48', 'e9d783c81b53ce967ae343a33cbbbcb4aaf3280f', 'ccfc078af1ca7afda886c6ee4d2d9ef7544e087d', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '6f61d15a31d6d051aeee3bf6d1482d332e68ebfe', 'f538dca4def5167a32fbc12107b69a05f0c9d832', 'f302e136c41db5de1d624412f68c9174cf7ae8be', '29069976eb7f828de91ed243cd12fd99fef56d94', 'e7eef2ac4136ec93bd306d2c9c353a13729a4553', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'cc5afe344cc7ed7acd68a28b9774ea8023a162dc', '31f9eb39d840821979e5df9f34a6e92dd9c879f2', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'fbd86e19157ea0e4ffb05f14d7b94603a5667e0a', '1d174f0e3c391368d0f3384a144a6c7487f2a143']}
{'paperID': '086586bdc6169cd6e9169ceca3c821803e3bf856', 'abstract': "Graph matching (GM) has been a building block in various areas including computer vision and pattern recognition. Despite recent impressive progress, existing deep GM methods often have obvious difficulty in handling outliers, which are ubiquitous in practice. We propose a deep reinforcement learning based approach RGM, whose sequential node matching scheme naturally fits the strategy for selective inlier matching against outliers. A revocable action framework is devised to improve the agent's flexibility against the complex constrained GM. Moreover, we propose a quadratic approximation technique to regularize the affinity score, in the presence of outliers. As such, the agent can finish inlier matching timely when the affinity score stops growing, for which otherwise an additional parameter i.e. the number of inliers is needed to avoid matching outliers. In this paper, we focus on learning the back-end solver under the most general form of GM: the Lawler's QAP, whose input is the affinity matrix. Especially, our approach can also boost existing GM methods that use such input. Experiments on multiple real-world datasets demonstrate its performance regarding both accuracy and robustness.", 'bibtex': '@Article{Liu2020RevocableDR,\n author = {Chang Liu and Runzhong Wang and Zetian Jiang and Junchi Yan and Lingxiao Huang and P. Lu},\n booktitle = {International Conference on Learning Representations},\n title = {Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching},\n year = {2020}\n}\n', 'references': ['ca64fd7bafca59d35bfd001f04efd46b9fb79748', '1a7586571ec088deea57bc43a96479f37e8d3130', '8173fdf7440dd20fa7f2c9135d8e53ad3e881d65', 'dedbb80a396b17739cd6741940130c1dc13eeb4e', '05f9689124903a6c6537421a16b7a4c6b4b122d6', '77a956512e22e37223ac6a00dcf191a086951505', '4aecd229673052c73b4713550f1f75c85739a155', '50e728460362fd298cd8760a6bc5cab2429e7560', 'b8bd415d5c294e4e928f135d19b6b6f978322d28', 'ce3a37aafab257853b931474cd7f0805f69db31b', '8ce9e511417ac35040ea7875fe9bd76ee4ac62f7', 'aac513b5f9a8538807ea80b5e2d2403c691877c5', '28cfe68822fd79fc788698aa13d0bddc09ed61dc', '339d6340960fe77bc597cad48eecf45fc237e742', '80ec527ccf5562ba0afe00fa0e0386a02784283e', '9692cb3a1e5513e7579e3f43ad385da0fa3c886c', 'fca2d693caf3e8cf8c1bf83f2057f45b831bcc52', '6572cecbe7f2fb071ac7427265f4eb312a5a5cf2', 'd46da5ef2fbe6602bcc890e6207a8c8da7933e58', 'a5d5f064a99eb51c08a3fadca2326c0bdd7504f1', 'f844925066c3de9cd9ad662059e25a9d47a59843', 'b85c74900ff716119f51852146de7a3d7d43230e', 'd682db90cf13aeee2b2672cc7b007f978432d259', 'a3814c0798a1ada633aacfeeab82e59b68b7d27d', '81203855a4551931f858564ad907933f6f70ab34', '83ad453332eee9317855662ee0cc97e2c36546dc', '3f13a5148f7caa51ea946193d261d4f8ed32d81a', 'f2b840c3c14b9f05106589c1be0a2dd4a494c0ba', 'e2c74ef6070fc21bcf97cae001d716a9ffb8c426', 'd6c1e14e8bea932f821352ea9e33928129f7d065', '0366b6396610708a77540564050a90a761a28937', '901e8381aae4b1fbb0d4dcef714d39fbf02f9681', 'e637332a8b8ba356d4d3b2c232a29fc7ea0783b2', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '1e819f533ef2bf5ca50a6b2008d96eaea2a2706e', '6a43d91c8d883e3463b358571125fa0ec7298b3a', '36eff562f65125511b5dfab68ce7f7a943c27478', '678db2df7ea05455b486dd68b805640fc2b986b5', '322cf9bcde458a45eaeca989a1eec92f7c6db984', '69e76e16740ed69f4dc55361a3d319ac2f1293dd', '4c05d7caa357148f0bbd61720bdd35f0bc05eb81', 'c6170fa90d3b2efede5a2e1660cb23e1c824f2ca', '3b9732bb07dc99bde5e1f9f75251c6ea5039373e', '9653d5c2c7844347343d073bbedd96e05d52f69b', '490d3a61db122d4d94ddcfd8d2cc20737106ea18', '449532187c94af3dd3aa55e16d2c50f7854d2199', 'e3c5498b8bea3394d0a40654607021860a700348', '687d0e59d5c35f022ce4638b3e3a6142068efc94', '2319a491378867c7049b3da055c5df60e1671158', '0080118b0eb02af581ff32b85a1bb6aed7081f45', '1fe4bba72cac39cbca6dc111a48fb4970a7ce4a3', '977d63d1ad9f03a1e08b900ba76e2f1602f020db', '41b8446d97c083f76718dc2e96900b84b9e73473', '186dce59aa6bd5bcb228c4f80f55ff032fa272b9', '2edace819fc4b52018678e44b610144f7dddb789', '692a262fab75e1cdefb8cffa5ba7be7cf988b939', '55b29a2505149d06d8c1d616cd30edca40cb029c', '8cd3277a3f3deb0f39cbe9d9ae3c59787c5e630a', '850b9a920aaff2c2d31221b071ae0be4e20dce16', '56ca893694e7a36f913d508105ce6391f7f6f7f0', 'f7c7edbd264fd80246ef3fa05fb1273051b7139c', '9899003369af99d02f699cbcbf48b79019666158', '4f37468a95ccc62debb9e5a4cb0d73489ca61190', 'ad4d553ae695a5da33ef188ac37f2a7037c303f5', 'c641d071faab5a605fe25cc115bcb82430a5f473', 'd7917db3303a702908edc94f4da4e5e01c016cfe', '87605c2054a6b581c5ea1ef9c28dbf3ee5f3064c', '73c6442a0395c2cc2b0924a467793b9de6ced88e', '97efafdb4a3942ab3efba53ded7413199f79c054', 'e9cffbd7dfa3faa4d78d6a4605a2ba84cf7cd71c', '9c281533ab0404a4f83971aaa8c89d5ed5dee418']}
{'paperID': 'f7227e206e11923e5d29a84ecc7dbd623a168f25', 'abstract': 'In this paper, we propose a new self-supervised method, which is called Denoising Masked AutoEncoders (DMAE), for learning certified robust classifiers of images. In DMAE, we corrupt each image by adding Gaussian noises to each pixel value and randomly masking several patches. A Transformer-based encoder-decoder model is then trained to reconstruct the original image from the corrupted one. In this learning paradigm, the encoder will learn to capture relevant semantics for the downstream tasks, which is also robust to Gaussian additive noises. We show that the pre-trained encoder can naturally be used as the base classifier in Gaussian smoothed models, where we can analytically compute the certified radius for any data point. Although the proposed method is simple, it yields significant performance improvement in downstream classification tasks. We show that the DMAE ViT-Base model, which just uses 1/10 parameters of the model developed in recent work arXiv:2206.10550, achieves competitive or better certified accuracy in various settings. The DMAE ViT-Large model significantly surpasses all previous results, establishing a new state-of-the-art on ImageNet dataset. We further demonstrate that the pre-trained model has good transferability to the CIFAR-10 dataset, suggesting its wide adaptability. Models and code are available at https://github.com/quanlin-wu/dmae.', 'bibtex': '@Article{Wu2022DenoisingMA,\n author = {Quanlin Wu and Hang Ye and Yuntian Gu and Huishuai Zhang and Liwei Wang and Di He},\n booktitle = {International Conference on Learning Representations},\n title = {Denoising Masked Autoencoders Help Robust Classification},\n year = {2022}\n}\n', 'references': ['455e65d4e32087f25c3bdacfd27e0408958bced7', '05c0b2d72a943dc0be26ef9e8fedd1380f2ff9ba', '9c4753ef43d2928866dc5bf6cec53d03373ec2fa', 'f56da2152c462e39dff21267602d8a1f7884134d', '6351ebb4a3287f5f3e1273464b3b91e5df5a16d7', 'bef771d2430af7be7525201bd677e82cec38510f', '722ad6ac92286507437b31486f47987d6ece05c9', '37613cdd48d6e32d995bbd2dc2e8e3902892dd76', 'de18baa4964804cf471d85a5a090498242d2e79f', '525d1f68539c436072a3cb6f3b8a88e3b124260d', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', '9d9ad34f43d9968cc58d4f8b929021f95cbd0b05', '289db3be7bf77e06e75541ba93269de3d604ac72', '62cf842a62bf9c78ec40faae72b60398dc87a576', '71ea8f105803703893b5c2d01f0c9508643b6554', 'a33daa0f2ed0e5bdc610be01d3ba014a2a8458d1', '75170439ccfe2271367e4ed7298f360b0443fde2', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '7d8472fe362b829dc105cf63a905339ee72e630d', '3f0ed6866620f76cffcb4b3653d9161a2d4aac5a', '4136cbc5f7f1fa34b91bf7bd335b173afaaf68d6', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '87f6a7c014ce206ac5b57299c07e10667d194b39', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', '63c022ae3b385d1d49c119142bfabb5cdb5ec90b', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', 'ed17929e66da7f8fbc3666bf5eb613d302ddde0c', '3f7bc67330b3eff749459568e7995f0017dfe645', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'ea9cf47573638745c9992cf9c5ebdabadd3c6849', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '750fd4f2a6139387b4f6245d3fd1013a8c8cf702', '20f85256555ad612148e52f9363e52f9d661728b', '9db631435f7f79646a4e0a1841fbeb3340e44261', '651adaa058f821a890f2c5d1053d69eb481a8352', '4b23012689e0f17912fb38d4984775e567cff8d6', '4feef0fd284feb1233399b400eb897f59ec92755', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '846aedd869a00c09b40f1f1f35673cb22bc87490', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '23ffaa0fe06eae05817f527a47ac3291077f9e58', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', 'e2b7f37cd97a7907b1b8a41138721ed06a0b76cd', '843959ffdccf31c6694d135fad07425924f785b1', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '3e982d5d3508019d4571aeda20469a12ecb9e286']}
{'paperID': '81f5859549221ecf9f42262b8c13e68d68b0a9a1', 'abstract': "Humans are remarkably good at understanding and reasoning about complex visual scenes. The capability to decompose low-level observations into discrete objects allows us to build a grounded abstract representation and identify the compositional structure of the world. Accordingly, it is a crucial step for machine learning models to be capable of inferring objects and their properties from visual scenes without explicit supervision. However, existing works on object-centric representation learning either rely on tailor-made neural network modules or strong probabilistic assumptions in the underlying generative and inference processes. In this work, we present \\ours, a conceptually simple and general approach to learning object-centric representations through an energy-based model. By forming a permutation-invariant energy function using vanilla attention blocks readily available in Transformers, we can infer object-centric latent variables via gradient-based MCMC methods where permutation equivariance is automatically guaranteed. We show that \\ours can be easily integrated into existing architectures and can effectively extract high-quality object-centric representations, leading to better segmentation accuracy and competitive downstream task performance. Further, empirical evaluations show that \\ours's learned representations are robust against distribution shift. Finally, we demonstrate the effectiveness of \\ours in systematic compositional generalization, by re-composing learned energy functions for novel scene generation and manipulation.", 'bibtex': '@Article{Zhang2022RobustAC,\n author = {Ruixiang Zhang and Tong Che and B. Ivanovic and Renhao Wang and M. Pavone and Y. Bengio and L. Paull},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust and Controllable Object-Centric Learning through Energy-based Models},\n volume = {abs/2210.05519},\n year = {2022}\n}\n', 'references': ['b67c812cc4c9a96e73e228ead44cb9939d356a68', 'c57293882b2561e1ba03017902df9fc2f289dea2', '9aa5a492c9188bcf9441af36fed69b3be74ac1b3', '95805eb7ab0edcd05128cf0256feaea8e2497de9', 'e52e9d290680d4ab492f97e035de0f619cc4ab33', 'a4727b807ae052c9dcddcf424a5d233bcc3c5a9e', '02b9e41693c0ad91dbe09c781dc6818b2c4b8a3c', 'bd4d21a608a949607799138272529b5a82472ae6', '61081d610808ddd0c741c6d61bb4edd7c89855fe', '37cf93dc3e15a1364dfd2f11a60831c0ebfcb45c', '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', 'b7d35e7107c9e090cd18dc494307be8e2c93a72a', 'd354de207393d35dd74fd8e656c837a4dbed752c', '9da628bd954e2250ec3a8aec9670eab096575c2d', '148af1bdbd8a93e085b34827d88994949be1f771', '5156381d63bb3e873533b08f203cb56c2d79b6c9', '0e10d8d17f0a81c4815af44375266c5cf404331a', 'f5d91e2908b0ba6416e16f24681b2a86e98adca0', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', '962dc29fdc3fbdc5930a10aba114050b82fe5a3e', 'bc128adcfacdb03e23c455abbc1486ca21b1d559', '476bc3ba7e953486af8c300619f5509c14dda271', 'b8eefc7a59aea78d2a410b274e6802a00ed0570e', '5a3a1aed872ef687e91bc98ea5acf5041fd342ae', '19b924dd9121f01165276a7afb764cf394acb80b', 'a50b7a45f704f30d7f97dd229d4d53433d5df3b1', 'c91907248cd832ba4f766394cd86a27946047a9e', '3e874d31af82de73f8dd2d55461c2f4ec838a07c', '9b8327b04667269fdae78cd34064eb2ee05ddee8', 'ba4cf6046d420af0ae86e2f4b587a8d50d219be3', 'd25b1edda507cba944938ec8784d8b124c2381a5', 'ef1fd3e181d66ffb18596fe03a0897963fa12ab2', '41573c31c21a7057cc8c4a35d09ca479ffc44ba1', '3aba6b43ab2cb3891557d9d61cb706ca658019e4', 'd93b0b37ee0e87a0e096ef803667b0798f465528', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'a456265138c088a894301c0433dae938705a9bec', '3532848bf87449d49fad7fdf774c3562127b0e78', '03eb382e04cca8cca743f7799070869954f1402a', 'b022f2a277a4bf5f42382e86e4380b96340b9e86', '66eb9741cee7fdeac0a437cc5e737535b7314f06', '2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1', '40c6dcb1f2c236590be8f264d88fef390cc48820', 'f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6', 'f2f85ec20bcae8d67035fd446b855a75b9a0aa8d', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'aeed631d6a84100b5e9a021ec1914095c66de415', '502e8e0d8502ff9d8cbfc565d9360afdfb2aea45', '9360e5ce9c98166bb179ad479a9d2919ff13d022', '75f063c9b32912f4b1b1f08dbcf6b0575ce16bf1', 'b6a0f30260302a2001da9999096cfdd89bc1f7fb', '0fc5a4f52a53f7d7809b7782a2aeb96da5ec6fd1', '9d2714b20d3bca952403be13b1c69b86004f91dc', 'd52a4faafc388e8c75a523422cd4d05672797e0c', '7fc604e1a3e45cd2d2742f96d62741930a363efa', '2b0752d91bde8b37e00e21219c441d9f6dfe74db']}
{'paperID': '38cad5ae86da1c728e18cc49f77e988e927768fa', 'abstract': 'The robustness of machine learning algorithms to distributions shift is primarily discussed in the context of supervised learning (SL). As such, there is a lack of insight on the robustness of the representations learned from unsupervised methods, such as self-supervised learning (SSL) and auto-encoder based algorithms (AE), to distribution shift. We posit that the input-driven objectives of unsupervised algorithms lead to representations that are more robust to distribution shift than the target-driven objective of SL. We verify this by extensively evaluating the performance of SSL and AE on both synthetic and realistic distribution shift datasets. Following observations that the linear layer used for classification itself can be susceptible to spurious correlations, we evaluate the representations using a linear head trained on a small amount of out-of-distribution (OOD) data, to isolate the robustness of the learned representations from that of the linear head. We also develop"controllable"versions of existing realistic domain generalisation datasets with adjustable degrees of distribution shifts. This allows us to study the robustness of different learning algorithms under versatile yet realistic distribution shift conditions. Our experiments show that representations learned from unsupervised learning algorithms generalise better than SL under a wide variety of extreme as well as realistic distribution shifts.', 'bibtex': '@Article{Shi2022HowRI,\n author = {Yuge Shi and Imant Daunhawer and Julia E. Vogt and Philip H. S. Torr and Amartya Sanyal},\n booktitle = {International Conference on Learning Representations},\n title = {How robust is unsupervised representation learning to distribution shift?},\n year = {2022}\n}\n', 'references': ['150bbe7a496b3e58ae705f342a6d915c194e3561', 'fa499cccb093dcb61ba5a270c5afaefc2502a241', '14a3aae8060338e3fbefc2af694890b019874d4f', '2cf49c54f038f9b033f6d625a260011730383bcf', 'a01ac66f5f66a2b23152f631b920972e4407275c', 'e95a2817efcbc12b2fa5a7ca3b6cb8c57c20715f', 'e6b401b8b56bc50618690066a4c74344dacfd5c0', '4b613c1a87223bb043ac6b593964aee85cdaa168', '370e2e712c0ffc661845636b6d14f99db59fdd8e', 'ebe510dc7b8025e496e11530192f2cccc184d002', '722ad6ac92286507437b31486f47987d6ece05c9', 'd071797499892940876a50f518d9c74d8c4e4018', '525dd120c0b5808ddcbbf703677b46346fb0729b', '2cd605106b88c85d7d8b865b1ef0f8c8293debf1', '40848b41ed8c9c255ecd8a920006877691b52d03', '0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d', '99bc80389f5957c7472906a5970e35a46281b469', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '569ef4e3c9f5ae968fc94000c12d212a2b679907', '009c42dac6316c0f72b14e353b654560906a0e0f', '38f93092ece8eee9771e61c1edaf11b1293cae1b', '0b40141779fafcedc28d83bd678807ddb5980df3', '5d0e2635a1ebe2c9347529975bc876d4286c9ab7', '1b04936c2599e59b120f743fbb30df2eed3fd782', 'a1b8a8df281bbaec148a897927a49ea47ea31515', '1ec049a29d369294a5a5ffdfb67e872dce899dac', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '22d834f7983fbd7cf2418978571f23efcd224bd9', 'add2f205338d70e10ce5e686df4a690e2851bdfc', 'dcc4c760c3f1cb17f953c487190b735030c33b78', '082e8b2667653d800b5b53d44c16357ee4fe9255', '753b7a701adc1b6072378bd048cfa8567885d9c7', '4e431827bf1ed1d8c435c01e75b12c79ba968721', '4ae0c4a511697e960c477ea3e37b3e11bf3e0e02', '810ae452a3a1f673ea241bd540f9551b2996ed5b', '4f51a64793d3b2a60e9e5846c31dae023cf5c69a', 'b79fe48ae523dc66185aa04df2dac7041afa8683', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', '118fae4b4d07453561f1eded88654f812c7c61ec', 'a90226c41b79f8b06007609f39f82757073641e2', '5694e46284460a648fe29117cbc55f6c9be3fa3c', '12d0cf8ae5ffe1b89345e1dcead22be592d844b2', '3e47c4c2dd98c49b7771c7228812d5fd9eee56a3', '1d5972b32a9b5a455a6eef389de5b7fca25771ad', '0302bb2d5476540cfb21467473f5eca843caf90b', '2a6d4a539681f9e8220efd5164d1a381ac346d73', '66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde', '9d946ed8743c14be3fd22346fae1230098d8fa0d', '111fd833a4ae576cfdbb27d87d2f8fc0640af355', '44500ee2032590019801404a4c334b1ac33ab987', '0aae10ade8fc9e58e177e034b794fce45c32fde8', 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c']}
{'paperID': '76f4416826b3393cf8f28ffd1a3191706d2b4286', 'abstract': "Semi-supervised learning aims to train a model using limited labels. State-of-the-art semi-supervised methods for image classification such as PAWS rely on self-supervised representations learned with large-scale unlabeled but curated data. However, PAWS is often less effective when using real-world unlabeled data that is uncurated, e.g., contains out-of-class data. We propose RoPAWS, a robust extension of PAWS that can work with real-world unlabeled data. We first reinterpret PAWS as a generative classifier that models densities using kernel density estimation. From this probabilistic perspective, we calibrate its prediction based on the densities of labeled and unlabeled data, which leads to a simple closed-form solution from the Bayes' rule. We demonstrate that RoPAWS significantly improves PAWS for uncurated Semi-iNat by +5.3% and curated ImageNet by +0.4%.", 'bibtex': '@Article{Mo2023RoPAWSRS,\n author = {Sangwoo Mo and Jong-Chyi Su and Chih-Yao Ma and Mido Assran and Ishan Misra and Licheng Yu and Sean Bell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data},\n volume = {abs/2302.14483},\n year = {2023}\n}\n', 'references': ['2a7e4a52707f60a0cc616f5705595727515dd553', 'ac7225c5d5aa71d686cd28d84983fa6bb9896d02', 'abb94f85a1fb77012aae2e89875b3a59cf4827ab', '0904945439e7f23b1b6ae6ce358c398accc3cd9a', 'cce61de63d6691fce13bf1a1c8d231553df12f31', '8c382d6c53a317d0011fd4affc6955186956ecc9', '07c6a2ccb65913f2478234e96dd50c9593989c5c', 'e6936c5da35d1de228c427704f97eb93ff5382cd', '09c3ca27403c2f73fbebf04729856a675f3fecaf', '85de3579e2142fafecb04b88452ee1c53d4faf3f', '37de04f1055d97acdec3d3710a8db219ba8e0273', '7413f4f695f216b013f8d7a3c5f26017da8d329f', '9257963d2b442c72eb8d17f7951da582b22d9990', '1f8abba432749862e332ded109be3b3027558677', '866169f5b40c631dc2c81bbe2378232dff728e0a', 'e9d0781e517d8dd9995079027e91c79927ec46af', '57119dde4f6c45b195d9768e00e441032da7a650', '2bb6dad01d726e4c828a05b113c7d00f4c9ec74c', '7ce8f0dda13a434314562f92d56147c7970f1c62', '72fe90517d8df2894a955ad01568ab7586495fda', 'dc9d32cab9f8856455821925eab7cb9f1fa9a18e', '0f8aa47ff8c6c49a347e192debe20ce4e5a4caea', '09acced5fcb49322f5a26ac7a4cbe9f1308657c4', '0016122bc5dfe0684baaa672c53014d48b79a65f', '4b324aafefcafbc49c70ccfcc1323e65ff2238c2', 'a21792db1c8d80c1d1f8525dab4959cc60b8e0ea', 'f975ab30c76ba364ce813b2983be38f6328dc94d', '1c45054cf0168a80a0b9e7f8171116c06a4a63f1', '669cbe118d868941172dbebfcaca9a479301d23b', '6f92dcefc5f6b4346f619ae7546a8bd2d6decade', '1d32fbfba0d85489f924bed5c6a41fdc28d08914', '3bab355a4660edcc3f6e10df5e260dcf92f630d0', '3b4fd630260685b500c50e40fd801b6689dca570', 'e1bb329621de73d08c47beae9b5439a1c244eb1a', '67f8f7fd1b8fea0214b032f94209650a73afece6', '9566da1b6af07462bc0ba54f24e47ba8ea82adcf', 'dc8b1486707e23a90d9d8e21e909d252e36cfe1c', '10161d83d29fc968c4612c9e9e2b61a2fc25842e', '3e7f5f4382ac6f9c4fef6197dd21abf74456acd1', '38f93092ece8eee9771e61c1edaf11b1293cae1b', '38643c2926b10f6f74f122a7037e2cd20d77c0f1', 'f3df804a7eef204b0c4fc4b81166dec9b4abc073', '6d2d39ff427f080ba52b976347f365b6a28b63c8', '43497fe8aa7c730e075b08facc2aa560a6d4dd85', 'a1b8a8df281bbaec148a897927a49ea47ea31515', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '299847adf3ee558a760475ffa364facac3ebbb16', '39e0a6bceec1fe851128c47fedf35a2d4f22b332', '72e16f845af7e2cfec11e73068210c62c850c193', '068eb2019cbb91421b7746af38da5e4c82f5e89c', '3448e3c55cf3b1f25aab4719eb094a95dbe7f05e', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '20ba55ee3229db5cb190a00e788c59f08d2a767d', '554aabebf17b11046ac734aac8ef5a71872e93e3', 'dcc4c760c3f1cb17f953c487190b735030c33b78', '35e8312d8bdcffb8e0c956d20d5a581cad1c1b8a', 'f8de25118af2abc4c48afb947d6ec298e05ef1e5', '6998bf98247a6ad13f005c136e0852e72b49f819', 'd782eb6e017835e9b0a04ed2b83f34ea00506576', 'c42816f497d663c681df20d48a6e66a5632600d8', '88ee291cf1f57fd0f4914a80b986a08a90d887f1', '0feea94f89d395436bf41bd10c797447eecbc128', 'cafcdab811c7834c9c09960e09f9feb045efc945', 'd98ec42c5bc64b06b5d6e259467528e0121ae69b', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', '2b1a3d7e6045dc6b544a548b372c1f8492b85967', '36653f8705b56e39642bcd123494eb680cd1636b', '1e3d18beaf3921f561e1b999780f29f2b23f3b7d', '4b1c6f6521da545892f3f5dc39461584d4a27ec0', '1342c1e1684620c019972e2679d5131f1e8a4a13', 'd2e4587744a89bad95fea69e08842cad6c8ff0dd', '4c20e7f95448ca3c1042a6d7fa5fa15ec27e9aeb', '1c4e9156ca07705531e45960b7a919dc473abb51', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '0c908739fbff75f03469d13d4a1a07de3414ee19', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '6de7c154619ad07354356fd3d54b44e719c3a466', 'ad67ccee45b801b0138016e2f44a566344e77320', '46770a8e7e2af28f5253e5961f709be74e34c1f6', '5079ea296646fbbb0c1ceb8bbadf86c698c842ef', '48ddd9101a90fe65e3061de69626741b843ff5e4', '2c455f0da2bd86a9b9ea432d1485049073d7c63d', 'a94ff616652351beab958ceb8a2f9e6fe1b70f9e', '8e10a391605da43d211f490e7eeedb3934acd76a', 'c709a3ac669aa334d6a0e9544f9191c5516da8a2', '798d9840d2439a0e5d47bcf5d164aa46d5e7dc26', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086', '818826f356444f3daa3447755bf63f171f39ec47', '1c46943103bd7b7a2c7be86859995a4144d1938b', 'd6d7de1293f880c6edc73ff06eb2a5050404fe41', '810b9ffea4c74db3923336a22dc9563679cfe564']}
{'paperID': '4ce6d229d5f44239c948fd56ad744c012aae22e0', 'abstract': 'While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We conducted chronic, large-scale multi-electrode recordings across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model “IT” representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models’ IT-likeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains.', 'bibtex': '@Article{Dapello2022AligningMA,\n author = {Joel Dapello and Kohitij Kar and Martin Schrimpf and Robert F. Geary and Michael Ferguson and David D. Cox and J. DiCarlo},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness},\n year = {2022}\n}\n', 'references': ['3307684cb55b64cce614c89a3f39938597fdc0f7', '177e957f5cd93229c9794ea652c646d2557b4a69', 'bacf294345b841586be6f128180f51b665aabb6b', 'f86e786c73891219efd24852212100992f0df764', '0e100c06d9fbdf32c434fd40469939a4aaab6c24', '7ab48793239205e7118021838e124d0419817bcc', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', '3ecc7ac21c22af073f37294df341c51e5d2d576d', '2c02895872c7aca00738a8ddd8b387da83a8acde', '7323ff36df929ecf1b877c8d0daadffae384c3e3', '19d68db5346c837bb428160619356e36045b351c', '574379e1310e1b5bb5f19a6824be34a1cfcd4616', 'e6c561d02500b2596a230b341a8eb8b921ca5bf2', '76cc14b26f01dc5e509c3f8b48181d80e1280d04', '146d40346ed686f54087386f66666adaf6a6efaf', '882728fd5424aecb7b4e63ee1dd0120599fcbe10', '32abbcb3aa75ac34a92624dc779a9f7a82ee981c', 'f3b76f7a1042972009c37302ea00daa30238934b', '31c260401df5df509c4889fb9387e361ae551f45', '47c8e35e1b4fad8eac5be6471ca521c1d0add77e', '726320cdbd04804ffa8f3a78c095bd1b55a2a695', '5ca131d97019ff1e40a92fa1c9c4c5179632744a', 'aa0ad8dd35f7b385feba07cedb30d7e3a1935381', '31619c8f4f9557136a675514b22ca8ece65ed38c', 'a11671d66c98b36d9d7684b047ae072b23e06794', 'af05afea7a7e99b8b20a743a49d5b33efc041100', '749118e990a532dddf9f573676e9d178f8442e2d', '6d86a97a7b392baac602a8df7224cb83880b071f', '2f201c77e7ccdf1f37115e16accac3486a65c03d', '8b9127bee0f7d109da2672ba06d0f39a5a60335a', 'be4a4f7f65d397a4e07dc83b95da6b414e0634e2', 'ba08dcd28837e83b54e9e7d192d756f52bdda042', '1cf361d02f5ad84567e48754f1a8f895653bc701', 'e83291498a3bc6b0efe8f9571e9c9ca1811707bd', 'b36a5bb1707bb9c70025294b3a310138aae8327a', '160a03c2890f3ef5436c25ef9b1758faa13807a0', '4f4556e75ff5a1aaad4123a46b1010eb09bfdf19', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '126b8cccd1bcf2afc442ab3856855aeecacfcb03', '8ab5d59c6534039e6854cdb60a8519e0e96bde03', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', '600a5d60cb96eda2a9849413e747547d70dfb00a', 'd6c850b41500be8cfadb5aa710934d9c5c7bc100', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '2dec4f52b1ce552b416f086d4ea1040626675dfa', 'ad367b44f3434b9ba6b46b41ab083210f6827a9f', '09eb087fc5730589e59673aafa0dfc788768611a', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '0dbd2e1018bdb61e5d39e8121747f2040bc0bad2', 'd6f2f611da110b5b5061731be3fc4c7f45d8ee23', 'e5e3a4a13e719ce770e036b4eeb82c95527c3296', 'e15cf50aa89fee8535703b9f9512fca5bfc43327', 'eb42cf88027de515750f230b23b1a057dc782108', 'f174c5f87594134ae71292e87c8263a5bb737a41', '71b7178df5d2b112d07e45038cb5637208659ff7', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'c8b31d37225e082395f212200eaaf68918195b41', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '3bc8c19cd2257790f2c92c0b6b757c0550a8404b', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '342fe6a6338e73fd4d34c4f37f41e3bbad274dd2', 'a1576850b6f0378c3e288e02c37dd1693f32d776', 'dd8a9122658ef38318d1d42e91435a6885044064', 'e225dd59ef4954db21479cdcbee497624b2d6d0f']}
{'paperID': '2e2c56325c3caac544285f46187c33439b211962', 'abstract': None, 'bibtex': '@Article{Lu2023ROCOAG,\n author = {Han Lu and Zenan Li and Runzhong Wang and Qibing Ren and Xijun Li and Mingxuan Yuan and Jia Zeng and Xiaokang Yang and Junchi Yan},\n booktitle = {International Conference on Learning Representations},\n title = {ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs},\n year = {2023}\n}\n', 'references': []}
{'paperID': '14a3aae8060338e3fbefc2af694890b019874d4f', 'abstract': 'Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.', 'bibtex': '@Article{Kirichenko2022LastLR,\n author = {P. Kirichenko and Pavel Izmailov and A. Wilson},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations},\n volume = {abs/2204.02937},\n year = {2022}\n}\n', 'references': ['82701185d8873ff1a3cdfd59c99879a6e391faa1', '3ded0685b2330103336601df1f7cfaf5a7cbf9bf', '2fe24fa62c5d57c5bd4c93b25740d0779530987f', '13a8c23a09f0fb0b10f8b096025e1df4850cf853', '53ed38af7df82242e0d7b13c19c814762de75bca', '3d112021bb96568c830c03c779f232da3788067c', '1420c75750c06c5eee473118389a6901847ad18b', '29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d', '241f8a90da2a9ec13ca44be5b602585bde4f92b7', '962466ca8a3bf432a2d45b656ab5dbcc9caf5b16', 'fa21a215468e881820266d1df362340987bc3fa8', 'c2c26d7e6b3679cd0a48ca8eace3ffed24f273a3', '88b5acbec09ed39eecdca136f75ff90feb2fc3a3', '5382d9bc17aabfd47b7c7d9873d2b64fdde48305', '49750bf1dd5e66025c18adfce5ce7fef445fb9d4', 'ab2a8ca21309859ed027928dc38e6915be0e6776', '6351ebb4a3287f5f3e1273464b3b91e5df5a16d7', '9f47fe66a23dbf48d0b2fa5fb66e378a9c51951e', '5f893ad86470cb935d702f980f5af8d8e013c7ae', '535131d7218e26360c09cd8f9a2e7198ec0e3e6f', 'c206a6e7f51f5e1b6bfc479a174b66ad88ada2db', '4f68e07c6c3173480053fd52391851d6f80d651b', 'af1809de802d36236fcc0e34d5359d544a14894e', '216d093cb2ad81bf55c21dbce2217f2b9032e67b', '0e100c06d9fbdf32c434fd40469939a4aaab6c24', '6aecc93c2d61da073b70dec19795172ca1ff3405', 'd071797499892940876a50f518d9c74d8c4e4018', '05f83f959f5b5f43d7168348328ee394656bbfbf', '3d8ff14c93a29f24d9dfb0cf908d9968d83126e4', 'a89b001c83fbc9c298e386e291c1a8e204d725be', '2263df3f2a10f43876aa280442f14090a9098369', 'fa3d0599f8a082add349b5b09a208136489dae34', '40848b41ed8c9c255ecd8a920006877691b52d03', '6116cde7fc65eff29a1b8118b034f87909f2237f', '8c96b865bbe1f597cf2c644e20ae46eab8e7caad', '29877659966f5ca2f198712a313cc653789edef1', '1b4a54670bb4fe15bcb0d06de0391d5b6d10ace2', '8246f0d3799290be5ed47254f6b88b601fa98230', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', '00325cb5408da77827951abd3fa93ec3bd019608', '24fcdaf969089e6a411f7cebc9274bbc53c25e42', '6a3cc30d5d6342d912851deb4362b8c47fa5ede3', '5baa3e00d66bc42db7e3908f0b70875cff9d0193', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '022622e024890d6e044ac50e2da6b44c59bdf418', '542ca5253b1a5ac1e1a55d9ee777def330e9334f', '009c42dac6316c0f72b14e353b654560906a0e0f', '024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb', '5c63fc87400a4d3afea63ab8a068a47249f815c2', '9086d3f7f1ddf0872a3850969b8cfaecacc0cc19', '0b40141779fafcedc28d83bd678807ddb5980df3', '1b04936c2599e59b120f743fbb30df2eed3fd782', '3621fff4a1c791901ea4a1359c10575193ec712d', '55daceb1d28be049b457ec53bc3ffa582c021317', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '0495d9df8eb84dcdab4e5536179823cd26279949', '1e6de530a183cded2373c8f0ffc54f4a9b7bd02e', '193092aef465bec868d1089ccfcac0279b914bda', 'dcc4c760c3f1cb17f953c487190b735030c33b78', '1fa9ed2bea208511ae698a967875e943049f16b6', 'ec570b827cf0cd132da7ebd37537df4f0bb7f877', '85b9e68eb27069e87181050035f40b79438dd220', '47f1eb0dc42189ba7cf21b76598c8217eb1b6e05', '77568c594470f9aa029f92774e2c12ab0451d9bb', '854eca61a57d2c1ea1019663caf022bc8fd0b909', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'bcfba69c2fadf2efea83be12fda2601f8d4681af', '43238d6f4da1318fa8f7d2dcd27fe13abafe9556', 'e91dca6e99f2d392953524986f2125be2008d9fc', '49b64383fe36268410c430352637ed23b16820c5', '810ae452a3a1f673ea241bd540f9551b2996ed5b', 'b611a8095630557229dc5fb6b07c272f1cd614da', '42ed4a9994e6121a9f325f5b901c5b3d7ce104f5', 'b79fe48ae523dc66185aa04df2dac7041afa8683', '8920c700e388a00c51bcd7ef6353c968cbf31e07', '207c073e427ff50b72a3f53975f5c6251551c4cb', '67b72e427187b1113c787f9265926322e3d123e8', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', '96b32b204a62777bef66eea595de2c47b4e9d6e9', 'a60540a8407fd117fd8e6857d4728e661f53dcc8', '74c19438c78a136677a7cb9004c53684a4ae56ff', '2d15a7546c16d5821ffa8f769eb7ec18e435e64d', 'fbda91cfacd2b792794fb726e9417aef58480c72', '715a73290f260cf2196307e59fe0b6776841f170', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', 'f986968735459e789890f24b6b277b0920a9725d', '8a8cfa45b4c0d071fbffa091c02670b19c94b693', '0f885fd46064d271d4404cf9bb3d758e1a6f8d55', 'c5420ef59d7508d82e53671b0d623027eb58e6ed', '19cb02117084b023c28da2fb356679806a299890', '18858cc936947fc96b5c06bbe3c6c2faa5614540', 'd07284a6811f1b2745d91bdb06b040b57f226882', 'b36a5bb1707bb9c70025294b3a310138aae8327a', 'f4615ae853fa0e8effbc5b36f7455c43520345aa', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '8760bc7631c0cb04e7138254e9fd6451b7def8ca', '901335712430a194b6e15d817685e5ecc72a15c1', '1a0912bb76777469295bb2c059faee907e7f3258', '07b5093aace8e485e7d23b83edb6351618138127', '37f5d47019f467c74acff22a38ffd4b98bdcb5d4', 'd42b11ce90c9c69a20ed015b73dc33e0e4100a7b', 'ed6297433cfc580837e87592f550cc96296c7d0a', '540b5b4919d345e4da3cc4f3e8a7862329bf41a2', '7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c', 'df7a7f248621c16da71007ceba7088a97204f39b', 'e96506ee4baab43fa81cf1870cf7befb4a71fec7', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'e09aa121bca3a0b9a0fe5f7ca119f44791cba1d7', '7ffdbc358b63378f07311e883dddacc9faeeaf4b', 'b5044cf4c9f06b51aebc526aef7afebac61e079b', 'a2bf2e83df0c8b3257a8a809cb96c3ea58ec04b3', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1', '6270baedeba28001cd1b563a199335720d6e0fe0', '79c286bf03ed97fb94d33511f3355770dcee0aec', '5cb309a35313308d0e75e409be84c176dc64c61c', '583b55367f787eb0c4e295707b642e63547b9806', 'adaa0523a5c9d5f92aa2009a51226391d8e62380', '168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74', '41bebd1951e57588e7829e44fab1bac0cc9251d2', 'a25fbcbbae1e8f79c4360d26aa11a3abf1a11972', '412a0bb5a3baa91b62053d82c562bc172df0439f', 'dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d', '1f0e1657063ea38cf225eaf1c1187ae7b2e4a0e0', '0aae10ade8fc9e58e177e034b794fce45c32fde8', 'a33b4a2002161a18bc7eb929566d77fd5178c2e9', '68c1112480798d6d184853512d5c32c345741fc0', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'e47868841d87efe261451a43b00d6c81cf7fb7a3', '922dc3bf6458ebab934608d064374d95ea323cd3', '4c38051f439b5e7316dbd7fc42fb40249ce6835d', 'eeb85a8b6f6a8301f8d1de35af2533def4a57f30', 'f6dac1c52d3b07c993fe52513b8964f86e8fe381', '5d90f06bb70a0a3dced62413346235c02b1aa086', '423815a794503aa04a4418015baae83f09556ea3', '162d958ff885f1462aeda91cd72582323fd6a1f4']}
{'paperID': '7ee017cf2cbd83fe23f94280b0931bbf5416b1f0', 'abstract': 'Models for image segmentation, node classification and many other tasks map a single input to multiple labels. By perturbing this single shared input (e.g. the image) an adversary can manipulate several predictions (e.g. misclassify several pixels). Collective robustness certification is the task of provably bounding the number of robust predictions under this threat model. The only dedicated method that goes beyond certifying each output independently is limited to strictly local models, where each prediction is associated with a small receptive field. We propose a more general collective robustness certificate for all types of models. We further show that this approach is beneficial for the larger class of softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on our novel localized randomized smoothing approach, where the random perturbation strength for different input regions is proportional to their importance for the outputs. Localized smoothing Pareto-dominates existing certificates on both image segmentation and node classification tasks, simultaneously offering higher accuracy and stronger certificates.', 'bibtex': '@Article{Schuchardt2022LocalizedRS,\n author = {Jan Schuchardt and Thomas Wollschläger and Aleksandar Bojchevski and Stephan Günnemann},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Localized Randomized Smoothing for Collective Robustness Certification},\n volume = {abs/2210.16140},\n year = {2022}\n}\n', 'references': ['fc1ad6fbf705bb2e45f0e832fb10325a868d87f9', '9cf8990a8b089059452cf59cc561d23b02611282', '0802e293e297a8e3b97b2716ad3591b249a8f850', 'ccae570e9a47227c4141dbc1469a555e5895137b', '3836d46d72ab198b074ade63cec4b54f0b1c4ebe', '7f26030dbdbbd398b3d79d2a09d1c6bea2a771bc', '4badd753be64c5c5b57dd2bb2e515fbe0c0720d8', '06aaece45f8284de309d4d9d8772305fb848a66d', '60cb22635e8d05a986fa6de2fc7090a9451e2de3', 'ad331b2035602c58221e9092920de0fd6ed2f629', '05a97b7215d65ed0a7c5aa513d73960f94d74646', '163ead56c23d6b0f5df5f24f7dc74ef82cb41eb3', '944dded7fcd1bb055cbdf11575e3a2de0ab42886', '7ea9ff6dbf22ed2327eb44e04412dddb443e41c5', '6189bf5f4c851ad0217a782509f8818aca4c7ff4', '458757a205dc73e17683458d1c432e9bbff42e5c', '1bfad6fd818bd64db381791efd9252e0313dc100', '50c5763d2d35f2c4eaa5cebea310faf2cf0a10dc', 'cb60351c4e2f9f244bcbed2eb44cdce86a029b4b', '6f5b1076ebacd30849d86e5f5787e3d43b65911f', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', 'ac225094aab9e7b629bc5b3343e026dea0200c70', '17555c227941654bc19d613742e2508f209c6d86', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', 'c05733052172753c8736e07fa8004dbaacfb623d', '765bdcf27ebc1eb03a14f1e47aefa4dda1e03073', '2b76b6e766547b3c6dbc2785a084ec3b72cb760d', 'ee4a012a4b12d11d7ab8c0e79c61e807927a163c', 'e7867244de690a1ae11a7a6d5a021e868fa75a3c', '01a4f33da8ad94ced3cf58548b28dbbb44148571', '36eff562f65125511b5dfab68ce7f7a943c27478', 'c8c494ee5488fe20e0aa01bddf3fc4632086d654', 'e177365c0dd642341469ba2f045405646c5dbb79', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '6364fdaa0a0eccd823a779fcdd489173f938e91a', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'a6cdbafcecbdea079d41f415ae43ef079fcf8156', '82fae97673a353271b1d4c001afda1af6ef6dc23', '0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a', '43d2ed5c3c55c1100450cd74dc1031afa24d37b2', '04f4085c0126ba29453a582cd1e62e05c8e15c82', 'df86d2a8c217776786bac9019d8b20029e4c0dd5', 'ae256391cd1f68cbaeb2eccb4e06b3ccf1952aea', '00936a6bb9d35908640da9043b3f5a955014d478', '166c42895882039e4252f7c943efa13d0505109f', '8e04c9530ea31889e2d351ae3cca4ddbb2bfac2d', '48601acaba95934762e6641f3b1c4237eed9c2bb', '43ba3e55d295d0312126c34bccfa70c5fa62d556', '7edda0f7cbbe47c66b8a231ecf50342cef3a8504', '8e223217255892d660e7752eaf568f64bc2d6e71', 'df24c3011fc42b72195e876ce052a0a072a1d923', 'b6b26564df790262abbe48fa18079d9610189b29', 'b0ebbcf713b3ddf3f94325bc58dc39ff76fdc412']}
{'paperID': 'e31b40fcb240a3b6d41890c6e30d8bd8556cdcf8', 'abstract': None, 'bibtex': '@Article{Wei2023DistributionallyRP,\n author = {Jiaheng Wei and Harikrishna Narasimhan and E. Amid and Wenjun Chu and Yang Liu and Abhishek Kumar},\n booktitle = {International Conference on Learning Representations},\n title = {Distributionally Robust Post-hoc Classifiers under Prior Shifts},\n year = {2023}\n}\n', 'references': []}
{'paperID': '6545eebb1162c70751119e173f8d51f5c112fcc1', 'abstract': 'Clustering algorithms are widely used in many societal resource allocation applications, such as loan approvals and candidate recruitment, among others, and hence, biased or unfair model outputs can adversely impact individuals that rely on these applications. To this end, many fair clustering approaches have been recently proposed to counteract this issue. Due to the potential for significant harm, it is essential to ensure that fair clustering algorithms provide consistently fair outputs even under adversarial influence. However, fair clustering algorithms have not been studied from an adversarial attack perspective. In contrast to previous research, we seek to bridge this gap and conduct a robustness analysis against fair clustering by proposing a novel black-box fairness attack. Through comprehensive experiments, we find that state-of-the-art models are highly susceptible to our attack as it can reduce their fairness performance significantly. Finally, we propose Consensus Fair Clustering (CFC), the first robust fair clustering approach that transforms consensus clustering into a fair graph partitioning problem, and iteratively learns to generate fair cluster outputs. Experimentally, we observe that CFC is highly robust to the proposed attack and is thus a truly robust fair clustering alternative.', 'bibtex': '@Article{Chhabra2022RobustFC,\n author = {Anshuman Chhabra and Peizhao Li and P. Mohapatra and Hongfu Liu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Fair Clustering: A Novel Fairness Attack and Defense Framework},\n volume = {abs/2210.01953},\n year = {2022}\n}\n', 'references': ['89e4b7ead1a64b75812b5c4eb9f979c555950ca8', '0df18a7d6fab4d7598acdc0fab7c9c300f8d6c33', '1b20a501c60d83ad0001778d654f45a7a42441fd', 'caeb2571671c50769610ba5b49a92b2f2b7bcd5c', '552d01b2a4e5f2308466485698e244ff00aa917e', 'f9018d184837ad055c8769376b199b57a911dfec', 'dd2a44ced60bf871070c3381c73e8258f922bae8', '575ea16c0ccee925498f8b7a022a635bcbf21e40', 'c48209be1906c7580b0cd0a2877daefdf7dba066', 'ff38ca20ae5cbbbc3b67063d00d9b55d67d331df', '76d8108855f30b8a66a84e65f9197592fcc3229e', 'f29bb1e3f5e98a6887c2414a2036858a076a5915', 'a520622e874ac180e7ae838e613bad4b2c6aecbe', '713c1c77e7b87d6c191b49769888baa92adcbb23', '648dce875272ba601b36a164a10648decdb3044d', '4b09b2c7205369d7dde0899f025e1e05675d790f', 'fd1201b44d28f6c2d722c8c4faada6bb82d79ebf', '7acf5709973c1f28d6b93730e251b1cd6fd0cd97', '905e690b6a77856f3b28e12b90a2d17810e48e64', '91f75567276b1b8f9a18abe27d99f24904bd5171', 'e71908585cbad83a7dd1abde85b1575d2c6e6e6d', '0090023afc66cd2741568599057f4e82b566137c', 'b8436b9989c057ff3c6fec2f69011eacb97d1253', 'a377aa14e7f7f6c432edc0d8bdd695fb338f3fe2', '3a81f485ef485fa66ffb2c5fd8c35cd6313647ea', 'dea85076038fa212d94c6441225c2d6cd6f4a99f', 'ec846f23a6b1588efaf18a82146dbde67bc332d8', '9c26bbf34bdab544a000038d628a8fb232d60cb6', '28f637cb5bf7c5bdbeb3317f563e48e95a27c92d', '9eacd7d43c95be4c4771bf1a324e200918e6c0cd', 'de5e7320729f5d3cbb6709eb6329ec41ace8c95d', 'f9cda74ec68c52d68c6bf21399d6c0d6f14828e5', 'f44ff4fc0ed0142cb18472a5ba421bb538aa837e', '9468a53d7314b16727956640f0d86a72a456d5bc', 'dbe884a0fa006fa312883886aa59f961d0851127', '98683d97b648182318944970464d8447264b29a3', 'c46ff777ade986eb56c57ae2e5d9a2039d4847ae', '5359e752a33e9416209dccfd8c2415940a46d2df', 'a2986864eb361e4e66bd4c10f8fd3bf129408147', '5d9a3036181676e187c9c0ff995d8bed1db3557d', '2d9107e6dead98af2f3099618eb58767e7d4961a', 'eda90bd43f4256986688e525b45b833a3addab97', '12e46e8bebeb36875d19bc6d61cde3531bb39ca5', '5403a5c2839fad66e3f907518b11f41a56ec4e21', '7f5b5e5b70ef61b90a030dfc26815deb6846b57e', '08b43d84e6747e370ef307e2ada50675b414514a', 'b59a8bf45add9a7bae9e8c978862f8d3c1077b61', 'b8c282f76923d89e00dcd17ec425d496ade6ddc7', '5c8fe9a0412a078e30eb7e5eeb0068655b673e86', 'ec25da04ef7f09396ca00da3f9b5f2d9670cb6fc', '65d68b7edb405a3681c412168cb28dafe3a3991f', 'cd3bd9ff2c8101b53069b7bef97117b9eb4cfcb7', '9241ea3d8cb85633d314ecb74b31567b8e73f6af', 'b6a0f30260302a2001da9999096cfdd89bc1f7fb', 'c57a214bfbaaf4b56844ccebf31694f3e2564829', '554b3fd2342476f46f4877fa235e7fb9bf67ec12', '392244f6c2edf5e157856e99ffdddb4b94eb6dd4', '6d12a1d23b21a9b170118a56386552bc5d4727de']}
{'paperID': 'fd4c076e0229ccd1a992cb89285d14fd4adcb7ad', 'abstract': 'The recent success of Vision Transformers is shaking the long dominance of Convolutional Neural Networks (CNNs) in image recognition for a decade. Specifically, in terms of robustness on out-of-distribution samples, recent research finds that Transformers are inherently more robust than CNNs, regardless of different training setups. Moreover, it is believed that such superiority of Transformers should largely be credited to their self-attention-like architectures per se. In this paper, we question that belief by closely examining the design of Transformers. Our findings lead to three highly effective architecture designs for boosting robustness, yet simple enough to be implemented in several lines of code, namely a) patchifying input images, b) enlarging kernel size, and c) reducing activation layers and normalization layers. Bringing these components together, we are able to build pure CNN architectures without any attention-like operations that are as robust as, or even more robust than, Transformers. We hope this work can help the community better understand the design of robust neural architectures. The code is publicly available at https://github.com/UCSC-VLAA/RobustCNN.', 'bibtex': '@Article{Wang2022CanCB,\n author = {Zeyu Wang and Yutong Bai and Yuyin Zhou and Cihang Xie},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Can CNNs Be More Robust Than Transformers?},\n volume = {abs/2206.03452},\n year = {2022}\n}\n', 'references': ['9f1b0e4c42a5a85d4c023030557ade4419f82ecf', '3425495ee3b6ead009f35aeb70edeac4e6eb2d10', '177e957f5cd93229c9794ea652c646d2557b4a69', '730a34374384f8abb886e464758b1a145edef938', '9653c070724e44f023e8cc3ec79f0b9e6d59480d', '6351ebb4a3287f5f3e1273464b3b91e5df5a16d7', '35c0800e657faa18cf3fc3629bdbeafbb976b006', 'f454f6b5f2ca9749ddf442eb5134612ef7f758c1', '4f68e07c6c3173480053fd52391851d6f80d651b', 'ffcd58f453f207d48075627da011f62782334c8f', '7b664a306b7d2f68dd816ea1d6586cf3472d75c1', '722ad6ac92286507437b31486f47987d6ece05c9', '53a16a2bd25c40401c7507ac8d70d61bbfb2e286', '2a805d0e1b067444a554c5169d189fa1f649f411', '5e4f03f68c6867d850f457dc5cc36738e5dff6c1', 'db33c408174eef1e40661e8279afbbbf6db2352c', 'ad4a0938c48e61b7827869e4ac3baffd0aefab35', '739ceacfafb1c4eaa17509351b647c773270b3ae', '003326a15fc4a8833785a47a741d7712474fa256', 'b364cdb02d18b9d9a3c097f5ea446f7e9ab10325', '3e398bad2d8636491a1034cc938a5e024c7aa881', 'c16835c8e535ebd9c10a550ca9455fe384a14449', 'dbe077f8521ecbe0a1477d6148c726d4f053d9c9', '2b8088253e2378fce001a090fe923b81e8dedf25', 'ad7ddcc14984caae308c397f1a589aae75d4ab71', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', '022622e024890d6e044ac50e2da6b44c59bdf418', '0cdd2925e62477be96b457b4ac778260e7d42c80', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', '43f2ad297941db230c089ba353efc3f281ab678c', 'dfa553707b215910f028d2a58ab79116626cc94a', 'e0c6abdbdecf04ffac65c440da77fb9d66bb474c', '4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9', 'ed17929e66da7f8fbc3666bf5eb613d302ddde0c', '5e19eba1e6644f7c83f607383d256deea71f87ae', '4ae0c4a511697e960c477ea3e37b3e11bf3e0e02', 'c4744a7c2bb298e4a52289a1e085c71cc3d37bc6', '0f50b7483f1b200ebf88c4dd7698de986399a0f3', 'a97bb99c1c70d3e0037c5cb66a4f19ce9cc4fb5f', 'dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4', '8899094797e82c5c185a0893896320ef77f60e64', 'd07284a6811f1b2745d91bdb06b040b57f226882', '4feef0fd284feb1233399b400eb897f59ec92755', 'e4c31c4dc29fa4bedf2cec10b01f3678eadbef7a', '2788a2461ed0067e2f7aaa63c449a24a237ec341', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '3647d6d0f151dc05626449ee09cc7bce55be497e', 'f6e0856b4a9199fa968ac00da612a9407b5cb85c', '51db1f3c8dfc7d4077da39c96bb90a6358128111', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '0c908739fbff75f03469d13d4a1a07de3414ee19', 'eb42cf88027de515750f230b23b1a057dc782108', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'c8b25fab5608c3e033d34b4483ec47e68ba109b7', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '9405cc0d6169988371b2755e573cc28650d14dfe', 'cd18800a0fe0b668a1cc19f2ec95b5003d0a5035']}
{'paperID': 'b1ab0635586fc7677f9a54590e56dcb0717db664', 'abstract': 'Standard empirical risk minimization (ERM) training can produce deep neural network (DNN) models that are accurate on average but under-perform in under-represented population subgroups, especially when there are imbalanced group distributions in the long-tailed training data. Therefore, approaches that improve the accuracy-group robustness trade-off frontier of a DNN model (i.e. improving worst-group accuracy without sacrificing average accuracy, or vice versa) is of crucial importance. Uncertainty-based active learning (AL) can potentially improve the frontier by preferentially sampling underrepresented subgroups to create a more balanced training dataset. However, the quality of uncertainty estimates from modern DNNs tend to degrade in the presence of spurious correlations and dataset bias, compromising the effectiveness of AL for sampling tail groups. In this work, we propose Introspective Self-play (ISP), a simple approach to improve the uncertainty estimation of a deep neural network under dataset bias, by adding an auxiliary introspection task requiring a model to predict the bias for each data point in addition to the label. We show that ISP provably improves the bias-awareness of the model representation and the resulting uncertainty estimates. On two real-world tabular and language tasks, ISP serves as a simple"plug-in"for AL model training, consistently improving both the tail-group sampling rate and the final accuracy-fairness trade-off frontier of popular AL methods.', 'bibtex': '@Article{Liu2023PushingTA,\n author = {J. Liu and Krishnamurthy Dvijotham and Jihyeon Lee and Quan Yuan and Martin Strobel and Balaji Lakshminarayanan and Deepak Ramachandran},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play},\n volume = {abs/2302.05807},\n year = {2023}\n}\n', 'references': ['31ab0786652bc87fbd6ef7aacf41914ab278e7d9', '666b2da70e774f06692d15470d0e30b8f8fe1fdc', '9da634823416e96417530f0a2197b9a4936eee3e', '002040f8c411fc4a077a0f9a726f80e95509a388', '8e31ce148dc7abee1a9790b3897942740aca0da2', '76a4272b3008ebeb66631681da2a6b9f77d0b6c8', '0f0a396d0479c98fece998077f0ea285791c6470', '992a067ef40893607dfc5a412265bf70c4804952', '60f2ae3b448b035e957603e84d1a073ad708879b', '14a3aae8060338e3fbefc2af694890b019874d4f', '0df927c8bfb175c0c8e20de2cd6c11f48e648be4', '1420c75750c06c5eee473118389a6901847ad18b', '9e9dc4b54b20882f871cf3b1438df33162bb5414', '4e23bcd3791b3130c54bf2be420f45ed1365df51', '49750bf1dd5e66025c18adfce5ce7fef445fb9d4', 'c8847713b5beb84bd152d59735100df5a0ac6c36', '0282c031d07bf12d807392601371af86a56cce27', 'e54e0d9eaa922cefb1c69e105979399fd34497b1', '9f47fe66a23dbf48d0b2fa5fb66e378a9c51951e', 'a5c7bd7ccd2e3db114dd467304eb4e6e928c0ef8', '04eab1a82321aa8c4a1e6f66367ce333183f25bc', '92e7aea7704fb1daf2f733bdfecfd523ebd3c034', 'a98cc4d37b0af9be710c5b6f4a3579331e6fcfbe', 'af1809de802d36236fcc0e34d5359d544a14894e', '9490bc8ae617d9816326ef759bc1efccdafd9553', '216d093cb2ad81bf55c21dbce2217f2b9032e67b', '93f9a75dc212ce2180b295b5d3feae112bfb5c41', '4c2e9b401e6fe3b3b81799ae0837048fccaef0a6', 'e757488d2e8684e3da7b14fbb000b7e4a0bab001', '714fc6626c527e05f2a31626d067d46520c6740e', '1ec19d5a07f7baa20c0d68ee53d290bf88b9dc23', '6ac8e0915df41692bdceef7f18bc1c50271630fc', '14d8d5e612028842d35cb5865f8fecf9ecc63c00', '26190ccd4753d6c0d25499ea34475205ebdd8240', 'de4904d2b2c0f460db7595cdfee394dd3d13b7a7', '689d3394c674b8cb2906fa8ffb1c80ecc76e69d0', 'df157cb42b574c3f46b269504c18375bfa5bc5b1', '6d49abd4a9d563e6cf86ae5e2a808ecd4d67c3fa', '6d0c2163b0e7de19d3dbed2ce8a2b546f4293b91', '6dd17c6354e10f437f5b8ade2aff6bec3934629f', '3d8ff14c93a29f24d9dfb0cf908d9968d83126e4', '602f1f77058b67212d1c03559ddf0e86c23a919d', '58de6cf06651017fba729cfbc37ed28ab2eaf507', 'c046a2f2498cca557c5d9353fae7d35331bef599', '1920ed0e7799410009d11cd7584550b9a57d5c93', 'b74c5b7c97ded089caa481964207ba5e0e65b659', '40848b41ed8c9c255ecd8a920006877691b52d03', '0f4374e62ae889dd2a35dc4c97b9a0510146fa87', '734f85727161f27bc7b295f0140a905363202d3f', '8c96b865bbe1f597cf2c644e20ae46eab8e7caad', 'c9367b790869e1f285ace7a2d64e7c4df439104b', '9242df9324089bd9c511211fd3f4a846d5af83e1', '8246f0d3799290be5ed47254f6b88b601fa98230', '00325cb5408da77827951abd3fa93ec3bd019608', '4ea5678069a6c4213f53972872a211d780f9f42b', '66488a38c3bae5d928bb22aa615fd0e64ccac62b', 'fee8f63972906214b77f16cfeca0b93ee8f36ba2', '6a3cc30d5d6342d912851deb4362b8c47fa5ede3', '94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b', '9207480a5cd071a3e85f408082b09283413cbfa5', '9f7e317c6ef0bb15aacc9b19f0f0d00fee6c9a36', '9e075fd385f24434b1235d33bb9be12125046612', '6d0552d5bed08b535001b4bc1cb2b9bf917ee99d', '648dce875272ba601b36a164a10648decdb3044d', 'b99bd19bed56de300dfa241b9698a35e95b95925', 'c9b08639a28ab70a06ba9a09eaae98b2fe3dc6c9', '542ca5253b1a5ac1e1a55d9ee777def330e9334f', '52bac2108400df6abbd729b1477b5e1d5aacf019', '5099d47408251626a4adc6a0f5e93678d8188732', '8ab4fd2c96e422acabc584774708d773009af8fb', '08cb7d416bcd1b1e4d6492a0cd0b01424abd9515', '3d4c4cd680da0ebd2d774c5b5236b9db3da15185', '175e3e14b7c8872f7c99638e90ca978d69b41297', '26e858cf3c82b66bbd539bb79356b0e885bdc694', '939c1554d7c8456e1a03c38be5b2cb225dd26128', '5d0e2635a1ebe2c9347529975bc876d4286c9ab7', '4c6207a203bcc7b725c41ed9c7451041530bf556', '6b7fac87b4ef98eceabfc47fd00a7190b1a48900', '21207fbe0431c6ca971af74d5fc7fe7fdaddeef4', '3621fff4a1c791901ea4a1359c10575193ec712d', '55daceb1d28be049b457ec53bc3ffa582c021317', 'af9280741ef627f0d6c8437605d002d3bfc2d1b1', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '4d8e5ea39213287d36889d9b791e6fb13313cc5e', '525cd3354a54aa9fad67c8cb91a30c7ef9e1c9dd', '632bcd5842e9d11a0ad414911c4571030c0bb529', '3cfb319689f06bf04c2e28399361f414ca32c4b3', 'dcc4c760c3f1cb17f953c487190b735030c33b78', 'ec570b827cf0cd132da7ebd37537df4f0bb7f877', '2700e81e00e241eba83ed9f73866e4ad7b0a60df', '7402b604f14b8b91c53ed6eed04af92c59636c97', 'ba783d92d0eaf6a7bff6ced7660150ce38016bbc', '4e14cf96c60e3d35b05e3a740c7c6bbe52f14677', '0090023afc66cd2741568599057f4e82b566137c', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'ad3a75fa2a26e6f69b7059466828f49492d31789', 'c2c9d0c59433102625ee4889a3e99e8710616182', 'bcfba69c2fadf2efea83be12fda2601f8d4681af', 'cf5a21684aefb1b8db6e0490167636d245396095', '1eb7f46b1a0a7df823194d86543e5554aa21021a', 'b25a30451518d372817967a72e125d638c85379e', 'e91dca6e99f2d392953524986f2125be2008d9fc', '29090beb90c184a9aaf7aa610bfed5ee1631d2f2', 'b611a8095630557229dc5fb6b07c272f1cd614da', '36aa6c01c9683783499395953c6bc856d6101feb', 'f471c7693e6f25f11bb5550a25ea1ca1e5367b5d', 'b79fe48ae523dc66185aa04df2dac7041afa8683', 'b661520bf0061b7d96ccf12016e351dd3a6ee780', '9c5c794094fbf5da8c48df5c3242615dc0b1d245', 'fd674e10770eb72e66a20e1c752c62dc7c12c0a4', '1e1855ca80e8ac3de0e169871f320416902e9ad1', '19cb02117084b023c28da2fb356679806a299890', 'c657b3fc93a24349117bf87296ea2b9b780706cc', '79cfb51a51fc093f66aac8e858afe2e14d4a1f20', '4eef0519f75911a2e132fac12427fa13bdb32a71', 'da5c65b0ac8b525c3d3d4889bf44d8a48d254a07', '802168a81571dde28f5ddb94d84677bc007afa7b', '3ad068ebde8f1d3a450a8c7e64a3428507bc2f51', '34ab95637e7723302058f6526e33dc73857b9af2', 'f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6', '45510f80bf9654e186e0ecab7c33daaf0b317d72', '90c721be4aab0aa786479a3693ab0f59af2bce96', '26efc762637b8dfe610717bea63a12be41aa80ba', '7486e148260329785fb347ac6725bd4123d8dad6', 'ca79ac12b64160f0bb30436fcb50b79df2052ddf', '06935a5c7b5a592e960f7bbac739750d8ebdcf01', 'e3ce36b9deb47aa6bb2aa19c4bfa71283b505025', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '2a2a12fc95c43363afff53d13963cbeeeaa69ad8', 'db3497b66b1c6f3f5537b3275c98e4475b6638cd', '40b949055cb08461ecce261d2bd365e77b41f22e', '413dbf16b13ea10b6b02d4b8b7d89d44574dc6d2', '02a0e07b73d34d057151a7aa716858222e8d4cef', '67a6db6990b06b00eb4f67b1efbb8961c74be54c', '1f0e1657063ea38cf225eaf1c1187ae7b2e4a0e0', '63f453bb7bed011e054cb5120c9dc21142a317cc', 'c03a3dc40550a8075d233a827fd184d62b93b274', '68c1112480798d6d184853512d5c32c345741fc0', '818826f356444f3daa3447755bf63f171f39ec47', '6bb0714bb12ea3ca708426f0d7d567a5d84ead42', 'e1ed9d24db5e8f7ab326aeb797e965a94f5ad6d3', '5f4469e2cdcbd0994ca793bf899d59be6b44bb88']}
{'paperID': '274f0cee47c8a5bbc38c32de6787adee6a08f69e', 'abstract': 'Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting. We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward. One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \\url{https://github.com/liuzuxin/safe-rl-robustness}', 'bibtex': '@Article{Liu2022OnTR,\n author = {Zuxin Liu and Zijian Guo and Zhepeng Cen and Huan Zhang and Jie Tan and Bo Li and Ding Zhao},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the Robustness of Safe Reinforcement Learning under Observational Perturbations},\n volume = {abs/2205.14691},\n year = {2022}\n}\n', 'references': ['6143e809cd2e9c7a18c3bc8819419fd3b02fbcf2', '2c988d25233d41a1535e54dc0f3cfc9f52ef2c83', '6ffa21166c99b1156612ba831ef0e5f7755ee6b3', '9c5f056c4e7986064722bb522e46e3546be8da51', '81bbf1bccd16e145ae3933e829aa41d2c8f7faa8', '04f63c8e5067a0e4ffce5eb4e11d25674728f230', 'ccd7398564a9bdf29564a7f39ac18dbe1a31d65e', 'fda6c22149d269cb2730e919ed4254a5a5f84ae0', '4b89f78987e5d0dba67a4f533945a838a5428ed9', '0ba7f2e592dda172bc3d07f88cdcf2a57830deec', 'f1e48bfb4464fedb94ced2d85b74991efcfe2856', '186cabbc394c560a458f94bf4317a5517edd8041', '5ddd0b5f141d3bfb14f5fee56c21608700095844', 'd6e783bce3b8e3ad082c2757235c34cb86c4e653', '36d55c909e8c1be84f3a4f2631e3303ef5392fb0', '4754ad07af3dce5262382ae47e496f694b61f589', '5b2370ebd3439ff60ea64a0c8db88fea2dd86a9c', 'b284afe9a7363b898661c9b3cfb7f015b158cc63', '1a627d2a169d71563109546da590a7cceb0b349a', '815bf97b451396313824f22becf5a4db315667bc', '93b2788fb1f2aed0e545d9f9d7dca1c05a63208a', '431dc05ac25510de6264084434254cca877f9ab3', '96055d058984b15a9b83024bb2e07292ee7559f5', '19e4e04e9c48bc86ddd849abdda2ec305c060694', '94b8ba09835f93fecf57d1fc3fd65181f7fc860e', 'c5de8b8d1ca58bca4f6edf4238bfb5c4332ed598', '629d0ce250581471f07083bbab95f23623b00201', '638538253332ebeba83f8de1d66f1eb4d2fe61b5', '9ec0bd60df62b997bb371e65d254434f8851ea9b', '9bb3d04c94a09e92375ae5377ab5187e1af3f6aa', '60a5be13ba59e21e4b8f335b3bcc341d93d8deea', 'c919ae4366f5cc4901b854cc259101ccc13e6f3f', 'bdf18bbec0980448b879df417e3141dc901f2e68', '893aba1e37804bf6878d3966dee747b42352ff09', '5c0d2e9caa303c51920c3d85e3acf4a64ca94414', '3fa50569925cfecc66fed5ec616682ecf3794ad7', 'bcdb21ca1703fc6f62df420626e36d138480a6a1', '43a4a354b67ab6d5531355a368094815d2d2593d', '51e7b68ca6f78e4a212af7c1d0c44382b38b9a85', '4ec94d6968bcb0a030a658d7347ce15e64588528', '13481ca12437363220282e3255eab109f0eeebcc', 'cb7c479a36520da1caeeec67db10772351a390c6', '4debb99c0c63bfaa97dd433bc2828e4dac81c48b', '18d43061bef62bace6b738fed3b1c44eea2d2147', '7f567df97dc7e099d96e6c590ddf5aef8c5b11c4', 'b514949ad8344071c0f342f182390d2d88bcc26d', '3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3', 'c27db32efa8137cbf654902f8f728f338e55cd1c', 'e4b4066e46fd160cb84e246b0895a4cd674d8ce4', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '30ff82cebce6fdc2957043c4085a426414474d78', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '7a4193d0b042643a8bb9ec262ed7f9d509bdb12e', '88880d88073a99107bbc009c9f4a4197562e1e44', '32ceb28e45a445df4d89df281bb0e3ab5aab1a2a', '236b40f3144b95cd84779484c8269092122920aa', '9c4082bfbd46b781e70657f14895306c57c842e3', 'cf8ed2793bc6aec88da5306fe2de560dc0be9b15', 'c8c16a56d2a9520197da9a1546f517db5f19b204', '9a700c7a7e7468e436f00c34551fbe3e0f70e42f', '7f57e9939560562727344c1c987416285ef76cda', '571b0750085ae3d939525e62af510ee2cee9d5ea', 'dc3e905bfb27d21675ee1720413e007b014b37d3', '1464776f20e2bccb6182f183b5ff2e15b0ae5e56', '759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89', '024006d4c2a89f7acacc6e4438d156525b60a98f', '2319a491378867c7049b3da055c5df60e1671158', 'e201e95b88a230c5d57a71bcd62b6307bfa11c1b', '9eae0c6ca4a52fc5e6b6f9eb111ab6fdbecdf9a6', 'f332ecd5d54adf0530a39dae189cf6b160ad5c0e', '9b99be8b739fe944d32724ab1eb656f832e8b618', 'ab4a1c4dfe23b3a1e3d077df467452cc68f64de8', '9ea46f06b614edb9c60771e0829ef1558048a21f', '6e986e365597a6e812298d7d86b0c7f220a2f01a', '01b723087df56234645a7330ab838345c86cdc18', 'a4a7d8716a0b541028bf702d5fb86d027bb9fceb', '3be5199010012448af287767297d81aa4a87567c', '4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3', 'c0f2c4104ef6e36bb67022001179887e6600d24d', 'd193b76178412e99e310e90668a498fa8fe11a5c', 'df24c3011fc42b72195e876ce052a0a072a1d923', 'a9e5a40b0ff5c40d2db7b73490922e115576adb5']}
{'paperID': '989830e67b55120b098afe12958b8c53d1b49f5b', 'abstract': 'Rapidly learning from ongoing experiences and remembering past events with a flexible memory system are two core capacities of biological intelligence. While the underlying neural mechanisms are not fully understood, various evidence supports that synaptic plasticity plays a critical role in memory formation and fast learning. Inspired by these results, we equip Recurrent Neural Networks (RNNs) with plasticity rules to enable them to adapt their parameters according to ongoing experiences. In addition to the traditional local Hebbian plasticity, we propose a global, gradient-based plasticity rule, which allows the model to evolve towards its self-determined target. Our models show promising results on sequential and associative memory tasks, illustrating their ability to robustly form and retain memories. In the meantime, these models can cope with many challenging few-shot learning problems. Comparing different plasticity rules under the same framework shows that Hebbian plasticity is well-suited for several memory and associative learning tasks; however, it is outperformed by gradient-based plasticity on few-shot regression tasks which require the model to infer the underlying mapping. Code is available at https://github.com/yuvenduan/PlasticRNNs.', 'bibtex': '@Article{Duan2023HebbianAG,\n author = {Y. Duan and Zhongfan Jia and Qian Li and Yi Zhong and Kaisheng Ma},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Hebbian and Gradient-based Plasticity Enables Robust Memory and Rapid Learning in RNNs},\n volume = {abs/2302.03235},\n year = {2023}\n}\n', 'references': ['c3d1b9f6f2192ede9503b3bf24888caa1a5f772b', '71e07483bc04b6771dde0ebee38036cb5c05c11e', 'b47264a0b34b940e1cc16390553a75144d3caab5', '8427455565ca12bcb6572ff7f29eb58515f36f28', '4b613c1a87223bb043ac6b593964aee85cdaa168', '86589b6286ef3c55b8b4fccfb41a3b30b7afdf61', '1a703f08da01cf737cce3fb9064259b3f4b44e9c', '5e11e806d24dd80ecf0f91e7aacedbba8d9fd6fc', '332c44793b70776b9b966128c52e694222b1ab73', '3ce9c183cb046ee3e25d1694c8dd95d354c16a45', '3a76603f03b45903bb030b2efd79984693625dc2', '22020679bc46666ac4b734164a8f2d78475ec596', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', '020bb2ba5f3923858cd6882ba5c5a44ea8041ab6', '97ab16d0e4a98f8275932f0a684e4fdcd70fb2d3', '0b322835c000dc7c351540cd9c9f70233bdbde9f', '2c819870111efb9fa70e359c15e2031e992c2b4a', '3839e3b4a64fd60e16861655acb4cb788730b609', 'a513bb6e1967f5a31ad4f38954e66d4169b613e5', '6fd5e95657e06727048e60d2e8d50a996942fa16', '339e2610de8487ccb54af05cb59b63854d25f02d', 'fc437af6204008647ea49f81058d5fdaddf75ead', '363db281e33509fe99151a7e99ad1efe664cf3c3', 'e4b64a75d321311447e11c363b45cc07bb74acc2', '4a684d8ff381be9334195e850d09b2a027447230', '146c231532d4e38de95e63368dcd09d0f8cea291', '208cd4b25768f0096fb2e80e7690473da0e2a563', '249ac07c5b87f44b85500e2d26b68a7edb93e83d', '1fbd17c2b9a60b0f37ae61bce2704c408a507bff', 'c9660db0c94edfb2b1f3ab4f08eb80acd83a1c07', 'd07284a6811f1b2745d91bdb06b040b57f226882', '37f6b9f6d45783547e0ccb3a1e3e811670391a8d', '036194c441c4c00375b08a286b9e83bc5083f785', '7e9c1e0d247b20a0683f4797d9ea248c3b53d424', 'c269858a7bb34e8350f2442ccf37797856ae9bca', 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518', '29c887794eed2ca9462638ff853e6fe1ab91d5d8', 'c91ae35dbcb6d479580ecd235eabf98374acdb55', '563783de03452683a9206e85fe6d661714436686', '3904315e2eca50d0086e4b7273f7fd707c652230', '395dd01c0d24777c660cf195c4cfadcdf51fb7e8', 'be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6', '2dec4f52b1ce552b416f086d4ea1040626675dfa', 'a03c7087175d2f7eb60b25d5e0cb742f3a908eac', 'd4021f1c446790e4db5bb3501c2aa68757f831f8', 'eca2f7495847dc3cd2afeb37202459f1e769fcf4', '75848d00cf8d019766529ae7fb8e31b2941224e8', '925ada6dcfd0d99185e9b214c2db7063dd91f3b3', '76fdd84496a52d0a9248c2628062b6b3c64c43e7', 'e22dd3f125c3044605decaabb4f59cd76a5f0a91', 'ca38bcf30beab44cce3ea387570c2b6065e6ff28', 'f9035ab5d58eae9564757995a3039dc46c325b57', '45ba546a85526d901e5979d89f3cee0c05c3189b', '579b43dc10517cdb368beb79f37535017a31f849', '68fa35b4b69a71abccf845d13bc80e06bdd90403', '9adf78008992846bcdc80cb58d7ef948ed7ba6d1', '6e7241121c688abbd9329bdcebce4b6320fc619d', 'b9164335be5808ddd59786869a9f992331af5218', 'd3fde30a26bff87d24aba05400334a8dd5aba2c3', '5d90f06bb70a0a3dced62413346235c02b1aa086', '8784f905f4f9fb6fa4a3cc9b0faa5b5479c687ec', '41351e5b997dc2555d330e54493c54583437c75a', 'bf0afaef99a2786692ccbdbe1fdca49c342586cd', 'f4356aeaaa2b7425d90ca45223097f3a8bb3503a']}
{'paperID': '6584f26629877392b0a34b7db448f7d9d7a4fda7', 'abstract': 'Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results uncover: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the inference process of GNNs significantly reduces over-robustness, while having a positive effect on test accuracy and adversarial robustness. Theoretically, leveraging our new semantics-aware notion of robustness, we prove that there is no robustness-accuracy tradeoff for inductively classifying a newly added node.', 'bibtex': '@Article{Gosch2023RevisitingRI,\n author = {Lukas Gosch and Daniel Sturm and Simon Geisler and Stephan Günnemann},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Revisiting Robustness in Graph Machine Learning},\n volume = {abs/2305.00851},\n year = {2023}\n}\n', 'references': ['fc1ad6fbf705bb2e45f0e832fb10325a868d87f9', '3efa96570a10fecba0f93e0f62e95d41ce7b624b', 'ca9f9e50e907ae4cf8be0b6d566a3083465cec5b', '9bc2a62ad82282296dd7131805fa83071469418e', '1b24fbc2189aca1d17f66eac7c8b09397eaf336f', 'd3759f651e26bffbc4ca3e41c7453d6800e0c682', '3328a42bdc552fbfba5dbd5b6c16b8aff26fea18', '70864c48e643e852355f4a79e23baf3614740df6', '05c2e1ee203be217f100d2da05bdcc52004f00b6', 'ab30672c8c5e4787f6a5985f26a8f281f0db2fb8', '9389af659f14239319186dff1cef49e8ece742c8', '8d69d51455d53344984b9719fb924685ba0145ab', 'a47424106165acd212b3233af8eb5a26cc567b4b', 'f1e5e65941617604923225cc4bf464e370fcae67', '435bc42450259a22cfba92b40217b8d26f4a7ed5', '60cb22635e8d05a986fa6de2fc7090a9451e2de3', '597bd2e45427563cdf025e53a3239006aa364cfc', '797d6b26a1a4aa98c8bbc93533707c65ad189c11', '58c143069444c7dff4be53531a47efefc40be497', '08afb316faf7b2d6c8582881458350698fd3e554', '3aab8bea2ba6bd7f076e6f92a504a1e322ca64b8', 'f5252075bb34666863cd01cc82c2d941d4ffe6c6', '1bfad6fd818bd64db381791efd9252e0313dc100', '341880efaef452f631a4a5cd61bef5dae47741d7', '26a0e0d17910c6676fcfa1cebe0cb7f5cd17080c', '6f5b1076ebacd30849d86e5f5787e3d43b65911f', '7e71eedb078181873a56f2adcfef9dddaeb95602', '5c0fe48ce1530d9757efca49d78709fc77caaf6c', '313dc4c4cdf81d8631948b9ce8195b3a6c291534', '27a1d8192fc8449a85e0b9eb3da4813df4a24b52', '7f77058976e2fe75e98280371962c43d98c98321', '1b9c6022598085dd892f360122c0fa4c630b3f18', '6c44f8e62d824bcda4f291c679a5518bbd4225f6', 'c7d141145ceb60c08d8400f4cc7357e6e288d5e4', '33998aff64ce51df8dee45989cdca4b6b1329ec4', 'ffb949d3493c3b2f3c9acf9c75cb03938933ddf0', '2b76b6e766547b3c6dbc2785a084ec3b72cb760d', '6b7d6e6416343b2a122f8416e69059ce919026ef', '41f6bbf03e9c2bf1bb2e3cea442a0f08b6d807d0', '36eff562f65125511b5dfab68ce7f7a943c27478', '6729b698fc28d191f3006690d973974a13c2f501', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'b7ccb41a2354aa9fa0223ce507d1ba024c2bd58a', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'ba40f5c23d8901ea1f8eaa7e85eb76c47d123624', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '2bdde0c06e2ac4a78a5c04809129cc0871c0ec2d', '5625bbaf7dfdf5b675c5213c917939870b5aa0a2', '7dcc27e011874c43463b80257d8ff3d797411844', '9c92232af79f81931b0ff58888eb857dcbecd8b8', 'fa25610fb8586c2b50a3654edc5bb42fa7fc4729', '46770a8e7e2af28f5253e5961f709be74e34c1f6', '0ed877bab75b32042a887715380c84ac27e64a8b', 'dce8146987557735a19771aefa1f027211a2c275', '721d0265fce28b9b8b738892c28e65206c9224d6', 'e720bbf5e2d36740dc44534fdc99134f860051de']}
{'paperID': '25ffa7c814856551843025632f092239152aa43e', 'abstract': 'Privacy and security concerns in real-world applications have led to the development of adversarially robust federated models. However, the straightforward combination between adversarial training and federated learning in one framework can lead to the undesired robustness deterioration. We discover that the attribution behind this phenomenon is that the generated adversarial data could exacerbate the data heterogeneity among local clients, making the wrapped federated learning perform poorly. To deal with this problem, we propose a novel framework called Slack Federated Adversarial Training (SFAT), assigning the client-wise slack during aggregation to combat the intensified heterogeneity. Theoretically, we analyze the convergence of the proposed method to properly relax the objective when combining federated learning and adversarial training. Experimentally, we verify the rationality and effectiveness of SFAT on various benchmarked and real-world datasets with different adversarial training and federated optimization methods. The code is publicly available at https://github.com/ZFancy/SFAT.', 'bibtex': '@Article{Zhu2023CombatingEH,\n author = {Jianing Zhu and Jiangchao Yao and Tongliang Liu and Quanming Yao and Jianliang Xu and Bo Han},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Combating Exacerbated Heterogeneity for Robust Models in Federated Learning},\n volume = {abs/2303.00250},\n year = {2023}\n}\n', 'references': ['2e533f2d1b44166277576baa8e9c159c04d668d1', '8b2ee805275ec92d032e0e5848b3196727b6c431', '37af8eee5de461d2d03eaac2fd864715016cb5a3', '01594f00b0deed32cba4fc4ea8c74b60be31db4a', 'c5f2da771e523b2fb6dc1937cff1925e3a7421ae', '157bddb85786b980c962e649bed33d8ebbf7b4a1', 'cc662dc89da9a7090c30fdb903d50ee93918020b', '6bb6359ad507653e2785d9fe924fc1feff39e0e9', '39dc3b78ca88ce661a669fbbe331c589bceadb6a', '748c4f1945b0d994ecef38c8aac01db1c6dc7029', '11df75768d755f59ec3267a280e2830e35b8e59b', '2c0f4711c9c124a8dc056eaee82a2ca5ef276da8', '255e6239bcc51047d020d41ce0179c1270f3c22f', '2173ac718365f5fd52e1f9e6bc567c5a2ce9a15a', 'dfa824c28e6f4a160e93d6b366fea97d1b0e3b52', '49f4a967f66d740ee3efb704f70b8d5da197394f', '6218de58c9191acb49e106059fa9e3b1ecc502d9', '99a599d8fe56529f47e78243ed61250190f96196', '4b6661347d5b58250130b89145dbd34ce310f2a0', 'dfb6250ae1c8f4d0ec3e28ed84596f77704485ab', '48aced1b11e6a3e16dd1f1ed9d0823e8eae48729', '764eff31d9596033859895d9513b838d2c57a6fb', '962a8ffc7d72990a28d505f49a39108b4803c223', '18939eadc9c4460c8385e0591cde214a1ead067b', 'b27da51d2b33c67b1b366f6f3a1e61e84dbab230', '07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b', 'a95d102ed27f62cf328ab7c5a8732502f2b69012', '49bdeb07b045dd77f0bfe2b44436608770235a23', 'c802ceb7a9ff904220c48ee44ae9b671be6d6379', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '6d12401822a24b2ff5542a7fa72158d891960c62', '38098abafe05fe839af09e41104998109e741e95', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '159395b0f7a2b9ea04f9a758d18887bcb970ee78', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', 'cfb40a6546904f03e74be62fe3183cea61ad5ef9', '93314b89c218c02cc1a32cad7071215693599907', '8dcbcaaf337d7bd22e580f1bb7a795ed4bb604fd', '5cfc112c932e38df95a0ba35009688735d1a386b', 'd17a3e9f34125ba6454e365ffe403ce8a91f2632', 'd3c071dbbb4520ed5875f7e064a9da87240534db', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '276194e96ebd620b5cff35a9168bdda39a0be57b', '84c95a8db377c25d4280f188e9477569ab57281b', 'e2a85a6766b982ff7c8980e57ca6342d22493827', '29e944711a354c396fad71936f536e83025b6ce0', '6ff2a434578ff2746b9283e45abf296887f48a2d', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'd1dbf643447405984eeef098b1b320dee0b3b8a7', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '5e83ab70d0cbc003471e87ec306d27d9c80ecb16', '1ab5c006caf3bf8c128fdfad80e58277cb8b1455', '2820973cf69daba62547f76de082bed5e1c646e6', '195d86d6b6a8420e9553fdbfc67cdfa4c87179aa', '9ea05ebeb9280c36da9c251c0bc9c7b24a167722', 'e3f41f4b6b4e3ab740176f022bcad522ad4c38ec', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '5e4437c0ef2bcfa06102341938d63e68762527a6', 'abstract': "Training machine learning models robust to distribution shifts is critical for real-world applications. Some robust training algorithms (e.g., Group DRO) specialize to group shifts and require group information on all training points. Other methods (e.g., CVaR DRO) that do not need group annotations can be overly conservative, since they naively upweight high loss points which may form a contrived set that does not correspond to any meaningful group in the real world (e.g., when the high loss points are randomly mislabeled training points). In this work, we address limitations in prior approaches by assuming a more nuanced form of group shift: conditioned on the label, we assume that the true group function (indicator over group) is simple. For example, we may expect that group shifts occur along low bitrate features (e.g., image background, lighting). Thus, we aim to learn a model that maintains high accuracy on simple group functions realized by these low bitrate features, that need not spend valuable model capacity achieving high accuracy on contrived groups of examples. Based on this, we consider the two-player game formulation of DRO where the adversary's capacity is bitrate-constrained. Our resulting practical algorithm, Bitrate-Constrained DRO (BR-DRO), does not require group information on training samples yet matches the performance of Group DRO on datasets that have training group annotations and that of CVaR DRO on long-tailed distributions. Our theoretical analysis reveals that in some settings BR-DRO objective can provably yield statistically efficient and less conservative solutions than unconstrained CVaR DRO.", 'bibtex': '@Article{Setlur2023BitrateConstrainedDB,\n author = {Amrith Rajagopal Setlur and D. Dennis and Benjamin Eysenbach and Aditi Raghunathan and Chelsea Finn and Virginia Smith and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts},\n volume = {abs/2302.02931},\n year = {2023}\n}\n', 'references': ['f40511f8e4453d1173102f79aa8a612827d262f7', '6d285ebd30f57c5ae6d0e6d085977aeedcab7079', '14a3aae8060338e3fbefc2af694890b019874d4f', '241f8a90da2a9ec13ca44be5b602585bde4f92b7', 'fa21a215468e881820266d1df362340987bc3fa8', '3ced5335b973fa9d4e537376c02c2df22dd5631c', 'a4e3a467804781a3c5a427f4032bf180ea8bf585', '9f47fe66a23dbf48d0b2fa5fb66e378a9c51951e', '76c2a2b883b3a801b16b8765eb54ab61a63f405c', '0be20a4976e8dd7836124b29e553f6c0c19289c2', '216d093cb2ad81bf55c21dbce2217f2b9032e67b', '40848b41ed8c9c255ecd8a920006877691b52d03', '8c96b865bbe1f597cf2c644e20ae46eab8e7caad', '00325cb5408da77827951abd3fa93ec3bd019608', '66488a38c3bae5d928bb22aa615fd0e64ccac62b', '5ffe9b1d8219438f0343995ad3ea1a888e3d9f8e', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '006e1d8ca4fba1fea02a3a0df0e226b11dbc9581', '0b40141779fafcedc28d83bd678807ddb5980df3', '26e858cf3c82b66bbd539bb79356b0e885bdc694', '47628dde478a58668ea8ba1ea9a432c5f5ddb652', '193092aef465bec868d1089ccfcac0279b914bda', 'ad2fe0bf54e8d0cbe4a1afe6260fd886db74f1a5', '1fa9ed2bea208511ae698a967875e943049f16b6', '77568c594470f9aa029f92774e2c12ab0451d9bb', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', '753b7a701adc1b6072378bd048cfa8567885d9c7', 'b611a8095630557229dc5fb6b07c272f1cd614da', '132f3e7cf9dc169d2f42d2fd678600b1de122d52', 'b661520bf0061b7d96ccf12016e351dd3a6ee780', 'a1fb7236d104ae0343c1a09e3590ee2283483240', 'a2b5d224895d96bfe2e384e2dcf1ebd136ac3782', 'af3825437b627db1a99f946f7aa773ba8b03befd', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', '1a5c4496d7e6c7c0d5c6c187505fbbc419bf3b92', '80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332', '19930147204c97be4d0964e166e8fe72ac1d6c3d', '11adc8bd35bd897502f9b5452ab7ac668ec9b0fb', 'b39b45a59c27a0cb3214d5a84547f54722d40c69', '4b1c6f6521da545892f3f5dc39461584d4a27ec0', '07b5093aace8e485e7d23b83edb6351618138127', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', '7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c', 'f701b58e41d928cdcd8d733b638fd65a73623b72', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '51dcc0c6c8ea27f0a5a3071fb8c4b32004cd55d8', '415229903f91a1f3fc7404f5e5997fde025c221d', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '6e4a2703dfcd02e5703caa388d2e0418a7e9f66b', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '104f4de69582d2d2011d5c5135cf80b2233744f2', '50d7ecd6901d6759a6bd9da7f2fc8f346e073577', '3413db5b8cc25a2ac37345d984fe87ca6b8b7354', 'c069629a51f6c1c301eb20ed77bc6b586c24ce32', '583b55367f787eb0c4e295707b642e63547b9806', '598b49e0b8e6a4c9a38b8e92164397fb83df7ce8', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', '26edc9ae4612c1743285deddf2004b5f4657fbce', '0e2f6482e7230e1d12af88d6b8afcef3d5d733e3', '0e6ce43aceab1c52056f8c41d9d6b01634620c51', '68c1112480798d6d184853512d5c32c345741fc0', '68d12857f76645a86417139eb6078db1ba76a7bf', 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c', 'e4350e816a350662ddb5f9ef92437aa8f3fd44f6', '4e3cb267bb4b0ae833c2309759234e0d4ca80074']}
{'paperID': 'c5772e031f2b7110dac99acae3425ea56b952856', 'abstract': "Deep learning models have proven to be successful in a wide range of machine learning tasks. Yet, they are often highly sensitive to perturbations on the input data which can lead to incorrect decisions with high confidence, hampering their deployment for practical use-cases. Thus, finding architectures that are (more) robust against perturbations has received much attention in recent years. Just like the search for well-performing architectures in terms of clean accuracy, this usually involves a tedious trial-and-error process with one additional challenge: the evaluation of a network's robustness is significantly more expensive than its evaluation for clean accuracy. Thus, the aim of this paper is to facilitate better streamlined research on architectural design choices with respect to their impact on robustness as well as, for example, the evaluation of surrogate measures for robustness. We therefore borrow one of the most commonly considered search spaces for neural architecture search for image classification, NAS-Bench-201, which contains a manageable size of 6466 non-isomorphic network designs. We evaluate all these networks on a range of common adversarial attacks and corruption types and introduce a database on neural architecture design and robustness evaluations. We further present three exemplary use cases of this dataset, in which we (i) benchmark robustness measurements based on Jacobian and Hessian matrices for their robustness predictability, (ii) perform neural architecture search on robust accuracies, and (iii) provide an initial analysis of how architectural design choices affect robustness. We find that carefully crafting the topology of a network can have substantial impact on its robustness, where networks with the same parameter count range in mean adversarial robust accuracy from 20%-41%. Code and data is available at http://robustness.vision/.", 'bibtex': '@Article{Jung2023NeuralAD,\n author = {Steffen Jung and Jovita Lukasik and M. Keuper},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Neural Architecture Design and Robustness: A Dataset},\n volume = {abs/2306.06712},\n year = {2023}\n}\n', 'references': ['dafa72ca28bf035c884ae4a6952791e48c76acbb', 'fd3bee898ae69bd956af9f4aabd3f7b478de2cbd', '3bc361f9fa99366e1b8e851e508c12dcefb3b3b9', '0335f6dfafb2dc8c76726ab282cb3a4ad8e0d9ec', '178a5726bccf0d9422609c74ba07b3b8b2c6ac37', '2aab97e35c43d961d645e650808d5b052ec180ab', '01dd3853c321cfc68c8e0a458018049ce1e83462', '67f74fe9d46f88661573003f8f1f12967ae49fa3', 'd01ca0b8b6d02e2833b8d1a71b28de153cdbc397', '2ed374eece816e461efc0cd9b6207c05d38df212', '20694a67d86bd98b51e1590402172ea7c8893709', 'a2a349218b7889425c005880cc4b16b0a9e54dd8', '25e9fa483a048607131a5a0e3287e8f457fb4807', '6f51319f22c3c02ab15cb26f5b332667ed1b7cd8', 'dc0def651f7ff53d7b3d764924b5c2c28024cdd7', '18939eadc9c4460c8385e0591cde214a1ead067b', '69599593f93023e2f91ef6673ee9860f85777d98', '8733fe2371b615609b04e2e910b1ecfa8e77cbc2', '908d07792e64234a083138d4b966a24bd2c604aa', '013a741927569ae9b40875a9e58d2c5ba6dbb3a8', '5e33ddbad2b96d37c669385f10347dbeb301de6f', '3242bf8767179c13c7322ccfdbe18c66c1e25a99', 'cdfee4355cae3299b06f3f98718df9bb64a899bf', '4ebce2425e231031f89a4a68dc52a151cd735d03', 'fe8907302f9d14233cd03cc2948a1c4e2a50bdb6', 'fda5f4facce9d5567c090d7ac733158e0fe93dc7', '49b64383fe36268410c430352637ed23b16820c5', '6e4fd9b4b2b673c981cda528d8039a221ad35225', '35a59bd09974c7fc78cf681f77f7301e180fd23c', 'be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5', '41071dbbbcbb27af3fec70de045f19c28535f5b7', '3f0a2de309f21a957b4741dd68007eb08d9b12e3', 'f323407464c4cd492d3fc1afd7170eab08f44d9b', '821fd5bed14d6d06c25fbf44123fd7be382f7b4e', '45b7b5514a65126d39a51d5a68da53e7aa244c1f', 'c1f457e31b611da727f9aef76c283a18157dfa83', 'f32f16ca3c27ff945198c6551a5d35fae3b1a660', '7864c8cd08ff4da9acc37de2576e9cdbabe03107', 'fe9b8aac9fa3bfd9724db5a881a578e471e612d7', '50bdda28de3dcf82a0e10f9ec13eea248b19edb5', 'e644a409b4a4c6eaedffe27efbc5c76280b34c61', '89b4111f14cdf342188f96d3962581fd0afa042f', 'f108b65fe0003e387e1cd7e50f537af0531818e4', '67d968c7450878190e45ac7886746de867bf673d', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'd6f2f611da110b5b5061731be3fc4c7f45d8ee23', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '188e247506ad992b8bc62d6c74789e89891a984f', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '2764af5084331e91e386d498ed53a2f4ce0eb354', '59d9318f07331ec15e54fe2a4218bc4a5c247a38', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '05c0b2d72a943dc0be26ef9e8fedd1380f2ff9ba', 'abstract': 'In this paper we show how to achieve state-of-the-art certified adversarial robustness to 2-norm bounded perturbations by relying exclusively on off-the-shelf pretrained models. To do so, we instantiate the denoised smoothing approach of Salman et al. 2020 by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This allows us to certify 71% accuracy on ImageNet under adversarial perturbations constrained to be within an 2-norm of 0.5, an improvement of 14 percentage points over the prior certified SoTA using any approach, or an improvement of 30 percentage points over denoised smoothing. We obtain these results using only pretrained diffusion models and image classifiers, without requiring any fine tuning or retraining of model parameters.', 'bibtex': '@Article{Carlini2022CertifiedAR,\n author = {Nicholas Carlini and Florian Tramèr and Krishnamurthy Dvijotham and J. Z. Kolter},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {(Certified!!) Adversarial Robustness for Free!},\n volume = {abs/2206.10550},\n year = {2022}\n}\n', 'references': ['2f4c451922e227cbbd4f090b74298445bbd900d0', '9b41d745fe3a76443bd0420bc5f2df28be2bd65f', '48630fa9fe7444b7fa910dab08ff23fc926f0e3b', 'f56da2152c462e39dff21267602d8a1f7884134d', 'd4aa4fd1d0ea6da1905640adb17c67db435f9f12', '722ad6ac92286507437b31486f47987d6ece05c9', '37613cdd48d6e32d995bbd2dc2e8e3902892dd76', '64ea8f180d0682e6c18d1eb688afdb2027c02794', 'de18baa4964804cf471d85a5a090498242d2e79f', '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', '014576b866078524286802b1d0e18628520aa886', '289db3be7bf77e06e75541ba93269de3d604ac72', '62cf842a62bf9c78ec40faae72b60398dc87a576', 'a33daa0f2ed0e5bdc610be01d3ba014a2a8458d1', '4136cbc5f7f1fa34b91bf7bd335b173afaaf68d6', '5812dae376cc07b955244a8e1ce11c3e4b9775ac', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '43a4a354b67ab6d5531355a368094815d2d2593d', '75339d34bdac0d21a41461228ec6088eecdf857a', '804fb9542f4f56e264dd2df57c255a9a2011c00f', 'f4b434c3ab979ecdd71bbed894b34de77590c6dd', '3e86a51d1f2051ab8f448b66c6dcc17924d17cfa', '651adaa058f821a890f2c5d1053d69eb481a8352', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', '1c4e9156ca07705531e45960b7a919dc473abb51', '2dcef55a07f8607a819c21fe84131ea269cc2e3c', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '30e8e647d10287a0f28cf733e1f15b4ad912645a']}
{'paperID': '175a2ed466f02c8fcef66b7ead61578700173e7c', 'abstract': 'Distilling knowledge from a large teacher model to a lightweight one is a widely successful approach for generating compact, powerful models in the semi-supervised learning setting where a limited amount of labeled data is available. In large-scale applications, however, the teacher tends to provide a large number of incorrect soft-labels that impairs student performance. The sheer size of the teacher additionally constrains the number of soft-labels that can be queried due to prohibitive computational and/or financial costs. The difficulty in achieving simultaneous \\emph{efficiency} (i.e., minimizing soft-label queries) and \\emph{robustness} (i.e., avoiding student inaccuracies due to incorrect labels) hurts the widespread application of knowledge distillation to many modern tasks. In this paper, we present a parameter-free approach with provable guarantees to query the soft-labels of points that are simultaneously informative and correctly labeled by the teacher. At the core of our work lies a game-theoretic formulation that explicitly considers the inherent trade-off between the informativeness and correctness of input instances. We establish bounds on the expected performance of our approach that hold even in worst-case distillation instances. We present empirical evaluations on popular benchmarks that demonstrate the improved distillation performance enabled by our work relative to that of state-of-the-art active learning and active distillation methods.', 'bibtex': '@Article{Baykal2022RobustAD,\n author = {Cenk Baykal and Khoa Trinh and F. Iliopoulos and Gaurav Menghani and Erik Vee},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Active Distillation},\n volume = {abs/2210.01213},\n year = {2022}\n}\n', 'references': ['6a8db14262ca2017cb253e12b8daeb57989a38df', '7ca1ee1f506c10dc156f5fa43d715e7a0985819a', '52c327eba81faf42f4b4bc71613cd840f33d06e7', '094ff971d6a8b8ff870946c9b3ce5aa173617bfb', '448aebe43a20f6e19e04e238e5e4d78690fc538f', '998bc35af7c44a37f0eaca96c181b6aed9b7d489', 'a6e25ca9ee9d3e45c6d1957c0dc3324a9816c34e', '5f0f4a3fa3cff7ffcedabbc9ed0dad2dd71f7028', '97d8823ca3c9bd932cec8ad6f3b194168e7cec92', '79b8ef3905a42b771248719495a2117271906445', '2cd605106b88c85d7d8b865b1ef0f8c8293debf1', 'd1a1f2cb21296d42cdcffd1141ff8aa7f6d91618', '69a1d72bac9dfb18940ff97ae91643d6c8158e6d', '1013750582c20bbdf1164127b5f26b1e06e817e3', '94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b', '5ffe9b1d8219438f0343995ad3ea1a888e3d9f8e', '3e7f5f4382ac6f9c4fef6197dd21abf74456acd1', '1728cb805a9573b59330890ba9723e73d6c3c974', '6b85b63579a916f705a8e10a49bd8d849d91b1fc', 'cd808e3e96aef9d18aae6efa842f0e5ebd329a01', '43497fe8aa7c730e075b08facc2aa560a6d4dd85', '20ba55ee3229db5cb190a00e788c59f08d2a767d', '96c6f34594279844ca8fd901649fa06491ef822c', '373edd20ffbe31850ec805c43af1dd7e4f0d7672', '8de7f044a673d1f5e3b454d0663811f91aa9811a', '7402b604f14b8b91c53ed6eed04af92c59636c97', '35e8312d8bdcffb8e0c956d20d5a581cad1c1b8a', 'cf5a21684aefb1b8db6e0490167636d245396095', '1fa6ebc976d3283a659d48aabac8246eb8a45c19', 'c5420ef59d7508d82e53671b0d623027eb58e6ed', '114c1ed1ae2e2a4b3d775c5a55d2c916f9d85972', '4feef0fd284feb1233399b400eb897f59ec92755', 'c342c71cb23199f112d0bc644fcce56a7306bf94', '3647d6d0f151dc05626449ee09cc7bce55be497e', 'da5c65b0ac8b525c3d3d4889bf44d8a48d254a07', '77f0a39b8e02686fd85b01971f8feb7f60971f80', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '0c908739fbff75f03469d13d4a1a07de3414ee19', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '649d03490ef72c5274e3bccd03d7a299d2f8da91', 'a049555721f17ed79a97fd492c8fc9a3f8f8aa17', '8291709487502991a839ae35bc90e1223413fe05', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '777b96abef29da63d9b4b1a583fa25c24a5ee029', '55801c892f628156a73e86ab271b03fe5d0c41fc', '8f13f2964c255f99cfe03bef02b74a3f1be138d8', '90d88e38b1fc555012394824d7e9a36171fc0d23', '241ecd3c3720723534505b4a03f9f9e03a3897a6', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '02227c94dd41fe0b439e050d377b0beb5d427cda', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '256d7fb66db5a835f8ba32f10f56899b7f318c17', 'abstract': None, 'bibtex': '@Article{Liu2023DiscoveringIA,\n author = {Chang Liu and Kunpeng Li and Michael Stopa and Jun Amano and Yun Fu},\n booktitle = {International Conference on Learning Representations},\n title = {Discovering Informative and Robust Positives for Video Domain Adaptation},\n year = {2023}\n}\n', 'references': []}
{'paperID': 'ef92ca5d1826b8abfc71dfb7752721de8b061ccc', 'abstract': 'We develop ShiftMatch, a new training-data-dependent likelihood for robustness to corruption in Bayesian neural networks (BNNs). ShiftMatch is inspired by the training-data-dependent"EmpCov"priors from Izmailov et al. (2021a), and efficiently matches test-time spatial correlations to those at training time. Critically, ShiftMatch is designed to leave the neural network\'s training time likelihood unchanged, allowing it to use publicly available samples from pre-trained BNNs. Using pre-trained HMC samples, ShiftMatch gives strong performance improvements on CIFAR-10-C, outperforms EmpCov priors (though ShiftMatch uses extra information from a minibatch of corrupted test points), and is perhaps the first Bayesian method capable of convincingly outperforming plain deep ensembles.', 'bibtex': '@Article{Wang2022RobustnessTC,\n author = {Xi Wang and L. Aitchison},\n booktitle = {International Conference on Learning Representations},\n title = {Robustness to corruption in pre-trained Bayesian neural networks},\n year = {2022}\n}\n', 'references': ['c1495d5c26ed9b2dac290bf64c31ff8170928a20', '7f401bb88652c5880289c24f8de214374f5df9f8', '8415db10a4e2354b7d57b75848f1485ce5d0e7ce', '70134eb7cf302d1a2c5e9ff98c21b54e04868fea', 'af309f8dc435151f3f70fdbb23e67dd5ba3e518f', '1718aa3c038b8658c3958dd83f0bb286a2428086', '883fd1cad73f37a41196f6eeb92a07e85af8ec8c', '26190ccd4753d6c0d25499ea34475205ebdd8240', 'cd8394d8b6679bb446cf154a6d123cf9a00e561e', '271f06215b6500c8a25c2f61b5316d037fe3a9ec', '0c334b610b9ce71a0e3d41ed697604412e5c2aef', 'a364503050330f95443803b6efe0e212596d9d44', '639b07626d140513a2aeac68ff52c3bb6db90bed', '6f5a417658bfa3ef86e787e837e2c47b10a2a699', '11a1e9e8e2c4913046306d5eb31216f9d54df892', '69bad7481b4c7dab2f0ac7c518c2be7256e699bf', '20df27e7f38c2481723c247be798ad3d418304f6', '3947e574353cd1a6ec436c1e8afa07addacec4f9', '9ef24a6e06da2b6e65f0a60838b5c0a692e6c5d6', '1eb7f46b1a0a7df823194d86543e5554aa21021a', '6e4e75c88a0801c87f47a171aa69a9914f9129bf', '3b656bba99dbcf6c6d8fd764d53ea64cd38c7050', '49b64383fe36268410c430352637ed23b16820c5', 'a7a8922b0b0d6f3d439520544da7db1fd6e0dce6', '4e0bb8c1c683b43357c5d5216f6b74ff2cb32434', '079ce2910925dbe4272c36c7e59cfa7ce959bc24', '36aa6c01c9683783499395953c6bc856d6101feb', 'f6d6367f8f267e78b2e3ddfb8b46a4819622c064', '459c8f314986db959fad1ab962bc0bbb0e430344', 'dc9119dffc17382793d5d5690f5df7f189b9a175', 'e8270f523375d22fd417bb583a5b50411e5d5f58', '7ffb9c2bf11e3e0f76c756ce7b0b6eead94a582d', '5606902be2e141a96458edbf9f9aca790fbf05c0', 'e1ec11a1cb3d9745fb18d3bf74247f95a6663d08', '802168a81571dde28f5ddb94d84677bc007afa7b', '6ff2a434578ff2746b9283e45abf296887f48a2d', '0e3cc46583217ec81e87045a4f9ae3478a008227', '24da6180db314619060d7b8fc798390f0c7a139a', '62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d', 'f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6', 'cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487', '7e17a3c231dc37d162b9ad74043afc1cee4ee2dd', '4d376d6978dad0374edfa6709c9556b42d3594d3', '5a9ef216bf11f222438fff130c778267d39a9564', 'aeed631d6a84100b5e9a021ec1914095c66de415', '502e8e0d8502ff9d8cbfc565d9360afdfb2aea45', '8012c4a1e2ca663f1a04e80cbb19631a00cbab27', 'b959164d1efca4b73986ba5d21e664aadbbc0457', '6498ef6ada16e3740c526a35d5ff1d48f2f64935', '180c78b132f6369a384d22a9529551d86c8788d3', '162d958ff885f1462aeda91cd72582323fd6a1f4', 'a22b36cf5dba3e85eb064220be7ef03be4efba48']}
{'paperID': '89eb2fd47d93dd15354c84c41df599669c68b829', 'abstract': None, 'bibtex': '@Article{Zhang2023ChasingAG,\n author = {Chunhui Zhang and Yijun Tian and Mingxuan Ju and Zheyuan Liu and Yanfang Ye and N. Chawla and Chuxu Zhang},\n booktitle = {International Conference on Learning Representations},\n title = {Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization},\n year = {2023}\n}\n', 'references': []}
{'paperID': '7fd631dd3783704f8a58ffbd13cf41d307169982', 'abstract': None, 'bibtex': '@Article{Li2023ExtractingRM,\n author = {Guanlin Li and Guowen Xu and Shangwei Guo and Han Qiu and Jiwei Li and Tianwei Zhang},\n booktitle = {International Conference on Learning Representations},\n title = {Extracting Robust Models with Uncertain Examples},\n year = {2023}\n}\n', 'references': []}
{'paperID': '84b9802042f29f4490eb36c2ac69a8ce9cdb2196', 'abstract': "Bias is a common problem inherent in recommender systems, which is entangled with users' preferences and poses a great challenge to unbiased learning. For debiasing tasks, the doubly robust (DR) method and its variants show superior performance due to the double robustness property, that is, DR is unbiased when either imputed errors or learned propensities are accurate. However, our theoretical analysis reveals that DR usually has a large variance. Meanwhile, DR would suffer unexpectedly large bias and poor generalization caused by inaccurate imputed errors and learned propensities, which usually occur in practice. In this paper, we propose a principled approach that can effectively reduce bias and variance simultaneously for existing DR approaches when the error imputation model is misspecified. In addition, we further propose a novel semi-parametric collaborative learning approach that decomposes imputed errors into parametric and nonparametric parts and updates them collaboratively, resulting in more accurate predictions. Both theoretical analysis and experiments demonstrate the superiority of the proposed methods compared with existing debiasing methods.", 'bibtex': '@Article{Li2022TDRCLTD,\n author = {Haoxuan Li and Yan Lyu and Chunyuan Zheng and Peng Wu},\n booktitle = {International Conference on Learning Representations},\n title = {TDR-CL: Targeted Doubly Robust Collaborative Learning for Debiased Recommendations},\n year = {2022}\n}\n', 'references': ['208b580fcee87d8b0494b5fcde4f2ca269018399', 'bb619a4013100526f7d23972ffc2d971aeca006b', '0c5c135d9ae812dddcbc07f81c7ca18bdac4f756', '50ba1ee99c52448b0abdc0143315449fa4f5664b', '4c15a32869bbd020e1f0f3a15e73665b5e14a079', '7ad5426ab5bc09c7c5b1a3bb3c8c55cf41236e30', '434afac50609d1354f7010afad6bed5dcb725adf', '9376040172d0506e2a2f44cba6880c0b066a860b', '3eede21a6b405fb7c697cd6d6982df30dafb2d16', 'afea12302cab67757e61dfb13aec6d864ccbfc4b', '1e0a6c8fb7cf57f05fbb37e701922f07ff644b2f', '6e843e6d2f863abe3f4bf55b5942376953201300', '6b62c1b97fff1c9a95964263a6036e7aea4faf41', '9779f919685adee936835eff46914d3d69d9ccb9', '138310d7f1ce03fc90437e5a250338eb692f54c3', 'ca94b305307b8df8997fc14ffaf90fa96623cc1e', '77119c507e9bb041fa06452ee193208b690cc9c3', '6deae79dec438eaaa524bca3b82c6b8d93553b20', '55e0d3689916c2d0a4ed9c7750e6110ac5f649e1', '228693df677bc83d397d72254de662e93863aaf3', '14d034b9cc6c74bd16b1388402342efb44649df0', '1eab1ac24f6fcf3d2fea34086f9b3c2bb55ff2a4', 'a278c07c8bd2921e59dd862cd91a0540dd340030', '26bdee2a6bc66ebfbccb31f11ac99cee4fe758c0', 'f45cfe62446fc2d1b5ad9c351a2438faaa9c6553', '71b12ce56bc79831d09d75eda30b2b34bb8d245d', 'c8211bc846a42de016fc6b31e17bda2d33016ef2', 'a3a2de9fb051723080b57e3e61870a5b2c0643f0', 'e108e3925e5abf9423ec95fd634a3a9417fcc3eb', '42ba8ecee544a7ed87b201bca3f88d742ecfe6a1', '1313e362d366a1f6e13c9ba8b2548f675226f4f4', 'e158ea00669155bf8614a70604f04fbb6753f447', '18205fb6d7d49e0f96b78debb396315001c33a2f', '98f3a77510de4d72a38359e61de69f22baa53970', 'f0636bad1e8d3c1a19894d1995b03b26bf6033f1', 'd4bbcc842f22547eaf5884251eaa68251895dccb', 'feaedbc49ea2791e717de76b9c8afe96a06de41d', 'fd855edabd177f903df92a82514bfaa68335ba0b', '5e85cd9baebc529281a66e0bbeaaa7f44c92c9d6', 'fe86c9ef608d5cd33c496d182879192287cf8569', '545122e2990590524459ec9b59ccac6ce71e3b6a', '9f501be340839457bd963c8dfd5096ba451594c0', 'f5f2ff8f7ad44aba3eec5d8a287cc1b6c498042b', 'ad2f23ebbd072af02d973bf8d7b01c8cd2d4addc', '209844292cabe3cfb84e2f8dbd90d3e48bc8b41c', 'f9748f18f14a4a2995e780e6ea50bd1af6768672', 'a7b6848a8b07bf21fb7936b6f21b02299056697e', '91939331adcd898603ca321060a67caf62d4d48b']}
{'paperID': '928ca39340784fafca5411d5d45a322463a1aea6', 'abstract': None, 'bibtex': '@Article{Fan2023TowardsRO,\n author = {Qi Fan and Mattia Segu and Yu-Wing Tai and F. Yu and Chi-Keung Tang and B. Schiele and Dengxin Dai},\n booktitle = {International Conference on Learning Representations},\n title = {Towards Robust Object Detection Invariant to Real-World Domain Shifts},\n year = {2023}\n}\n', 'references': []}
{'paperID': 'c1ff3661f3edf5e0571da6d8aaf852cd6df84318', 'abstract': 'In recommender systems, users always choose the favorite items to rate, which leads to data missing not at random and poses a great challenge for unbiased evaluation and learning of prediction models. Currently, the doubly robust (DR) methods have been widely studied and demonstrate superior performance. However, in this paper, we show that DR methods are unstable and have unbounded bias, variance, and generalization bounds to extremely small propensities. Moreover, the fact that DR relies more on extrapolation will lead to suboptimal performance. To address the above limitations while retaining double robustness, we propose a stabilized doubly robust (StableDR) learning approach with a weaker reliance on extrapolation. Theoretical analysis shows that StableDR has bounded bias, variance, and generalization error bound simultaneously under inaccurate imputed errors and arbitrarily small propensities. In addition, we propose a novel learning approach for StableDR that updates the imputation, propensity, and prediction models cyclically, achieving more stable and accurate predictions. Extensive experiments show that our approaches significantly outperform the existing methods.', 'bibtex': '@Article{Li2022StableDRSD,\n author = {Haoxuan Li and Chunyuan Zheng and Peng Wu},\n booktitle = {International Conference on Learning Representations},\n title = {StableDR: Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random},\n year = {2022}\n}\n', 'references': ['208b580fcee87d8b0494b5fcde4f2ca269018399', 'bb619a4013100526f7d23972ffc2d971aeca006b', '0c5c135d9ae812dddcbc07f81c7ca18bdac4f756', '50ba1ee99c52448b0abdc0143315449fa4f5664b', '098cb8e0ff46fd05b193cc465d1e577ceb365e9b', '84b9802042f29f4490eb36c2ac69a8ce9cdb2196', '4c15a32869bbd020e1f0f3a15e73665b5e14a079', '7ad5426ab5bc09c7c5b1a3bb3c8c55cf41236e30', '434afac50609d1354f7010afad6bed5dcb725adf', '9376040172d0506e2a2f44cba6880c0b066a860b', 'afea12302cab67757e61dfb13aec6d864ccbfc4b', '1e0a6c8fb7cf57f05fbb37e701922f07ff644b2f', '6b62c1b97fff1c9a95964263a6036e7aea4faf41', '9779f919685adee936835eff46914d3d69d9ccb9', '138310d7f1ce03fc90437e5a250338eb692f54c3', 'ca94b305307b8df8997fc14ffaf90fa96623cc1e', '77119c507e9bb041fa06452ee193208b690cc9c3', '8ee4a0164f4f95eb012c3f565d0866e5f4f7d64b', '6deae79dec438eaaa524bca3b82c6b8d93553b20', 'b258a20ef49bbed030dfd78c58b7b5fe284e9d04', '228693df677bc83d397d72254de662e93863aaf3', '8fdc29ceb4cc4b773c9a8f0a022191f982135762', '14d034b9cc6c74bd16b1388402342efb44649df0', '1eab1ac24f6fcf3d2fea34086f9b3c2bb55ff2a4', '26bdee2a6bc66ebfbccb31f11ac99cee4fe758c0', 'f45cfe62446fc2d1b5ad9c351a2438faaa9c6553', 'c8211bc846a42de016fc6b31e17bda2d33016ef2', 'ad42c33c299ef1c53dfd4697e3f7f98ed0ca31dd', 'a3a2de9fb051723080b57e3e61870a5b2c0643f0', 'e108e3925e5abf9423ec95fd634a3a9417fcc3eb', '42ba8ecee544a7ed87b201bca3f88d742ecfe6a1', '2592d9d68f2b29da197c16b2c7f6519ee6304ad6', '3b0c39b323a8cc1a90b3fdee83dad2fd417bd0ce', '98f3a77510de4d72a38359e61de69f22baa53970', 'd4bbcc842f22547eaf5884251eaa68251895dccb', 'feaedbc49ea2791e717de76b9c8afe96a06de41d', 'fd855edabd177f903df92a82514bfaa68335ba0b', 'a85acbe6ff39173031d877eaf79af3ca52bbc20f', '5e85cd9baebc529281a66e0bbeaaa7f44c92c9d6', 'fe86c9ef608d5cd33c496d182879192287cf8569', 'ad2f23ebbd072af02d973bf8d7b01c8cd2d4addc', '4cb2971b451749835d353989254b8b0209ab8d70', '2eb1685b35e0de1f450b870b1a31be8d58f7b284', 'f9748f18f14a4a2995e780e6ea50bd1af6768672', '97efafdb4a3942ab3efba53ded7413199f79c054']}
{'paperID': '20eaccbe4d6ffc3652594fb8783f64a653cb881a', 'abstract': 'Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data for model training. Empirical studies show that SSL can achieve promising performance in distribution shift scenarios, where the downstream and training distributions differ. However, the theoretical understanding of its transferability remains limited. In this paper, we develop a theoretical framework to analyze the transferability of self-supervised contrastive learning, by investigating the impact of data augmentation on it. Our results reveal that the downstream performance of contrastive learning depends largely on the choice of data augmentation. Moreover, we show that contrastive learning fails to learn domain-invariant features, which limits its transferability. Based on these theoretical insights, we propose a novel method called Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn domain-invariant features and can be easily integrated with existing contrastive learning algorithms. We conduct experiments on several datasets and show that ArCL significantly improves the transferability of contrastive learning.', 'bibtex': '@Article{Zhao2023ArCLEC,\n author = {Xuyang Zhao and Tianqi Du and Yisen Wang and Jun Yao and Weiran Huang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations},\n volume = {abs/2303.01092},\n year = {2023}\n}\n', 'references': ['740b343fac0e9bc5b1906c99c1071b67f4c163e1', '1de398ad15d72baef5d3fc4274c6ecf6557d7a3d', '5ce79f612034d36a955e0e4ebaf5c57fe29d4b31', '3ed2b5a9c8e42f3bf93dfd40426e4df205420ac7', '1da867c102a407171e64e2b28cb2f9ce63325ee6', 'a01ac66f5f66a2b23152f631b920972e4407275c', '0f2aeeb62b6f0e6fd6034715bac08904a59f9ffa', 'e5b2e2a284db5ba7c2c011daba9769d2c56b6586', '3743249bf829cbe0de72cc49371f51c40d7cf56c', '60571e69c2263094478efba76cc8f26f9ad84852', '5b00442bd7e12ac1c614127f1f6429c8df3747bc', '8a9d84d86ac0d76e63914802f9738325c3bece9c', '085907c9b2bfbf39bcaf6fe3d16bd1dadcef5af5', '0f8aa47ff8c6c49a347e192debe20ce4e5a4caea', '00969b4dcf8f9b21895bd038a51a038018da84f0', '0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d', 'bf5c55630b5f79806ea9ad68d86f5ff0cf1b2fb0', '49f4a967f66d740ee3efb704f70b8d5da197394f', '192029e99ae4364968803abc5885c7a159f8a9ff', 'c7316921fa83d4b4c433fd04ed42839d641acbe0', '38f93092ece8eee9771e61c1edaf11b1293cae1b', '2b5a8e6fd214e1471c5d61bf80e4ff9f774765b6', '0ad5f44fc605d1d2759a22c05a4883b9ec05b234', '0b40141779fafcedc28d83bd678807ddb5980df3', '5d0e2635a1ebe2c9347529975bc876d4286c9ab7', '3621fff4a1c791901ea4a1359c10575193ec712d', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '642c5831e2a497752475a8c9fded9dbb90513482', '98c15f7aa696f8e2eb134fb6e3f6a9263f30a804', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '41fcef711faca9013fd0980a9f6ec1d23c9c76c8', '97f4d09175705be4677d675fa27e55defac44800', '403227333329b36183004f04db72362b604adef3', 'a1fb7236d104ae0343c1a09e3590ee2283483240', 'af3825437b627db1a99f946f7aa773ba8b03befd', '04541599accc47d8174f63345ce9c987ef21685b', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', '1a60d4122ef0ac6972ef9b4a3752ac1657de482c', 'a2bf2e83df0c8b3257a8a809cb96c3ea58ec04b3', '8e3f12804882b60ad5f59aad92755c5edb34860e', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', '522d65a3db7431015aeaa201a7fc4450a57e40c3', '79c286bf03ed97fb94d33511f3355770dcee0aec', '184ac0766262312ba76bbdece4e7ffad0aa8180b', '84b50ebe85f7a1721800125e7882fce8c45b5c5a', '5cb309a35313308d0e75e409be84c176dc64c61c', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '02b28f3b71138a06e40dbd614abf8568420ae183', 'ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2', 'ff62f1782e73c4c0766c24bcaf557fe497f6469e', '137a74592aa36ffa5200c88a990ae4ef1a883fc6', 'eae500ce89f7cc5cd48a58c4ba7edb2f02826b85', 'bd716e1214d53acb88512bffd647ffbf729026f7', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': '0aec562c8cc3d1004ea86f601b6b42734454466b', 'abstract': "We consider robust clustering problems in $\\mathbb{R}^d$, specifically $k$-clustering problems (e.g., $k$-Median and $k$-Means with $m$ outliers, where the cost for a given center set $C \\subset \\mathbb{R}^d$ aggregates the distances from $C$ to all but the furthest $m$ data points, instead of all points as in classical clustering. We focus on the $\\epsilon$-coreset for robust clustering, a small proxy of the dataset that preserves the clustering cost within $\\epsilon$-relative error for all center sets. Our main result is an $\\epsilon$-coreset of size $O(m + \\mathrm{poly}(k \\epsilon^{-1}))$ that can be constructed in near-linear time. This significantly improves previous results, which either suffers an exponential dependence on $(m + k)$ [Feldman and Schulman, SODA'12], or has a weaker bi-criteria guarantee [Huang et al., FOCS'18]. Furthermore, we show this dependence in $m$ is nearly-optimal, and the fact that it is isolated from other factors may be crucial for dealing with large number of outliers. We construct our coresets by adapting to the outlier setting a recent framework [Braverman et al., FOCS'22] which was designed for capacity-constrained clustering, overcoming a new challenge that the participating terms in the cost, particularly the excluded $m$ outlier points, are dependent on the center set $C$. We validate our coresets on various datasets, and we observe a superior size-accuracy tradeoff compared with popular baselines including uniform sampling and sensitivity sampling. We also achieve a significant speedup of existing approximation algorithms for robust clustering using our coresets.", 'bibtex': '@Article{Huang2022NearoptimalCF,\n author = {Lingxiao Huang and S. Jiang and Jianing Lou and Xuan Wu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Near-optimal Coresets for Robust Clustering},\n volume = {abs/2210.10394},\n year = {2022}\n}\n', 'references': ['427f949906c52f7df1b77cdc50196d0e81b3598f', '27ba2c4350887185504c4c9e4e66152ca3a802cf', '0b02637eabcc140f865253ffc785e47903bcadfd', '19927da753e52738ee3190785cb93c82067fb3f0', '8953285591d97c68b401027194e7438a4efeb822', '049286d20f66e0ec9c2bbd9fd9116f41e0872828', '589a9140f68b800686e0100e2bd6b1b6e3c813db', '270178c8c3a65cbe5151d4b56093e1b31195e851', '6a1fa57790b975d827fd1237d6fd174eabaff7f7', 'c1b43d49dde0933d9fdecb34f40016cc19c6c711', 'e71908585cbad83a7dd1abde85b1575d2c6e6e6d', '4b70752c19a64707c7bbae348191e740800c4b67', '1b287e17483abcb98678ddede572a56312ea06f0', '721ad812f4b9211fd5e5c637bf1213c37bf20324', 'faa6d9b78cfdcf2d33979d93ca0ee80793b3a1d4', '7175950bd7427a98cd57e3148c494bd9eaca19e3', '3163c7d4562ac62573af6eb008e5a4154c0a17bd', 'ff67cceccf34fbfc5728f7cb5c3f25019fa29d91', 'b6b02cbc2979d16ec371568513a3c07afa82b01a', '160603fd5e58c82deaa3e8557a2a4dd329c70272', '06e26e8d7999af9aca0b342c4e8e553b4979cdbb', 'c8a7ac02eb26260fe5559df8d2bc98b4323ba7f7', '74d62dd6be70c7e06ac48ca5f8953cd811d857d1', '97067d88eebc098b76ff19eb498b5d414b49e474', 'ceb55d64d564dae512fa07c5cee5386dac083e4f', '04ac298d0516535289950588dfe850bbe8c68d7e', 'dcd5249802ffb7c005776c5f18c20dc12e699171', '89ffb191d8c370ca74ee57e6dcd63ae04628f84d', 'c8831d7d318b8d59f9b958d250a58f253f08bd8a', '6890040ee813e041d4777f921c743fa86db8b1a6', 'ae3f31c841c460b15a81bf51655f4c0e39cacc79', '2a380830759161fae7d10858441999c7fa000570', '0b490334ae4d06330bcfecbf7184b7c458a1f6e9', '8aabf3f17f357ccf85c2f0280a60f05ea628218c', '2c963d2f8a0bfcc0b67af9a90e95f4c77253cdec', '6060d67b191aa833923f66c09be6b73d5e56aa0f', 'd3522be0aa8ef5c05e150bb3f63aba20f21d555b', 'c9b55c422cf5f4f0a08ada022714d9695af136bc', 'e385d9f832162080025d681aaa8f6bc01ccadc4a', '41b279aa2cb4823b590f79fe0e8739b1be059297', '8d23d6432c27843040f51dcf0191877f7a9994e9']}
{'paperID': '1dd1795f6fa368b61c78c3afccf194bbcf25ed3a', 'abstract': 'Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite may be true -- Even though adversarial training helps when enough data is available, it may hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Our proof provides explanatory insights that may also transfer to feature learning models. Further, we observe in experiments on standard image datasets that the same behavior occurs for perceptible attacks that effectively reduce class information such as mask attacks and object corruptions.', 'bibtex': '@Article{Clarysse2022WhyAT,\n author = {Jacob Clarysse and Julia Hörrmann and Fanny Yang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Why adversarial training can hurt robust accuracy},\n volume = {abs/2203.02006},\n year = {2022}\n}\n', 'references': ['c1d9f0ea53ef6d9a44ff46b071fb45cd3de58fbd', '93733d2e31a9e9318282391c1962e2855698ca61', '559930ae79ea2dfcefb8658a2dcdf8c849cc897d', '16b6c80a2a113a90868ea37c1f7aabd85ce04fe7', '4cdca901e6324198501ebda9100a1877d19ebb88', '17af9510a38e4dec93398707f11d833c8af36254', '2e165871bdfe21fe8389087696030ac33edd7f17', '078ac3d05afb953585ae17e914c6b4d0571f6421', 'b4efbd4e0885d8a2e98acb5a23acbc134e08e8d1', 'dfb6250ae1c8f4d0ec3e28ed84596f77704485ab', '24cf86a418c9471e8001961c87697c825f0bba8f', '11a1e9e8e2c4913046306d5eb31216f9d54df892', '473a854a939eca4bf39420ff496f8e24e223d460', '7451d83e3eac30ba719ecea9a051af33dd10fc66', '72c19697e0edf92f838103029ce77b88cce6d685', '1378789c465d8de3e8ec5032be89ab672feaada2', '5d0e2635a1ebe2c9347529975bc876d4286c9ab7', '5f7d3a9299c78dde60106324c7a145afaba82bb2', '6e635f131c37d481f5b8778c5823a35201876150', '365fb36b15f13c0c69596a9fc98ddcaed3fe739c', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', '3f8e190773a8898ef296da67b646e40dca1374cb', 'bff4ae911351a5c7dd9af394aa7f2647bf42dd4a', '8de0b5b58f62f8fbc6c5b88692bcabcd93eadb30', '58eee79dd9e1e9aa482656fa3d256b64620dd30a', '0a16a0396a9cbe298a110b69b3b2006692f2b78b', '6d4a87759917132913319960389f17fa1fe8b630', '78fd36163d6fc32dce1ad7d5b8c1203e6b212fcf', '54c15364b5873e39257c221112fdf22299ed9e8a', '987d1d8eb739f8deb449fdc4f4c29cac9499b74e', 'a30e33d5323343fd02be10975946e14df4a962e8', '302ba3a3de585baa7c5692eebd14d5328b0a780a', '76c2679deb0b7689c658c199254963889d4d2b69', '6d15683422ffa9c044c2a90f45ea0ff845de83d9', '63c022ae3b385d1d49c119142bfabb5cdb5ec90b', '6d12401822a24b2ff5542a7fa72158d891960c62', 'b3f1aa12dde233aaf543bb9ccb27213c494e0fd5', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', '25f99278a54417a2dcb64c549416e138373082d6', '2b627185499791048681e8d24190c31dea928f16', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '93314b89c218c02cc1a32cad7071215693599907', '8bcd98bd5a451c2bbde4a22a4d1affe3c6407af0', 'a857007d5d240fe3cf6ffe96e57e163a1105e00e', '1438fab07a94351eaabcbe92983fbefaa208b2f3', '18063ed998c99bfef92fad8418610b97f863d878', 'd8b477a120798e3c8983de485c1a8cff06ff33db', 'f0c5991dbb130fa6b5de011cf7a04f6ed815ef68', 'f986968735459e789890f24b6b277b0920a9725d', '1b9c6022598085dd892f360122c0fa4c630b3f18', '804fb9542f4f56e264dd2df57c255a9a2011c00f', '516afc7d5a427f0b53ca459fe2624a1aae2ee00d', '0314e777333a63aca5735ea136c74e113aa8801d', '11adc8bd35bd897502f9b5452ab7ac668ec9b0fb', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '739915dff9f3eb1b52d435ba52b5890737e5df99', '2cf3142e55965c6dce7e3ee2f93d1fb2d5aba416', '4406c54f40e0f73db2180704d454951649df32f2', 'a48a56b0727d09f599676524fe190308d9e88bf1']}
{'paperID': '035b158f1f165930aaf7877fb54515ce29a17f9e', 'abstract': 'Finding the best way to schedule operations in a computation graph is a classical NP-hard problem which is central to compiler optimization. However, evaluating the goodness of a schedule on the target hardware can be very time-consuming. Traditional approaches as well as previous machine learning ones typically optimize proxy metrics, which are fast to evaluate but can lead to bad schedules when tested on the target hardware. In this work, we propose a new approach to scheduling by sampling proportionally to the proxy metric using a novel GFlowNet method. We introduce a technique to control the trade-off between diversity and goodness of the proposed schedules at inference time and demonstrate empirically that the pure optimization baselines can lead to subpar performance with respect to our approach when tested on a target model. Furthermore, we show that conditioning the GFlowNet on the computation graph enables generalization to unseen scheduling problems for both synthetic and real-world compiler datasets.', 'bibtex': '@Article{Zhang2023RobustSW,\n author = {David W. Zhang and Corrado Rainone and M. Peschl and R. Bondesan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Robust Scheduling with GFlowNets},\n volume = {abs/2302.05446},\n year = {2023}\n}\n', 'references': ['feaae6b90a94d2a7e6a23132fed9c5001773c892', '94f27efa3d983ce0b79954928134263768929dcb', 'cdf4a982bf6dc373eb6463263ab5fd147c61c8ca', '6e264eeff9127306775495c44f4d48839943a70a', 'b877d25296cf390cc6b63b0e477d8febb5a81e09', 'cb5323ef22a5a38cfba318abadcadee822ccf8a9', '633e2fbfc0b21e959a244100937c5853afca4853', 'd594764273c02b5a3bbdc4a8d49979a23ad1f125', '378bfce88ed3139f48fba4deeafc96846c31251d', '6b989b8327db3a7212141c59c1569f0219775058', '95cb5128f2cb9fb7fb94f2ce62cf1fb62361cc77', '289db3be7bf77e06e75541ba93269de3d604ac72', '61e27dbae190b82639c57f180ecf97e4c46fcad9', '2319e4f2d73c78820d498a347d74ed5ef3a185d3', 'bc48ecef55aeb5a31899b1995d7ae95da40ddb73', 'c4955faa27e082a80504285c28324c58eb52250c', '3f13a5148f7caa51ea946193d261d4f8ed32d81a', 'ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee', 'b4c8aef6cd1946d7aacd9524286637d2f825160b', '6989e13df80edfc6e638e8d8502cb0739d494ca6', 'df013a17ab84d5403361da4538a04d574f58be83', '7cfa5c97164129ce3630511f639040d28db1d4b7', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'fc66d024648a89e124c637c13dd5d9c9866a1ab0', 'a538b05ebb01a40323997629e171c91aa28b8e2f', 'dce8146987557735a19771aefa1f027211a2c275', 'ada56e1f7575d7f542215c48625c161ab060bed0', 'd61031326150ba23f90e6587c13d99188209250e', 'd160d309b7d6cc1eb4382bba08ca61cd302fe735', '5fae5afb1e9e959b880bff6e2cf457e112192396', '6bc70aec3415944b461bef65b8444cd84d11667c', '8665c9b459e4161825baf1f25b5141f41a5085ff', '996263c3ddbb50f0198354827445abd214f83030', 'e99553b4cc9c88084c14ec718c5011882b253a8c', '890469e625fe728adfa690a3945ebca4c11a8998', '42bfff334382a96adb376f55d1bb1acbad0a6647', 'a5aad5abb32f6b15f31b92312bb3b0f7b6470977', '1565c128b727550a73bc4e105a3353420112485d']}
{'paperID': '8b2ee805275ec92d032e0e5848b3196727b6c431', 'abstract': 'Federated Learning (FL) is a machine learning paradigm where many clients collaboratively learn a shared global model with decentralized training data. Personalized FL additionally adapts the global model to different clients, achieving promising results on consistent local training and test distributions. However, for real-world personalized FL applications, it is crucial to go one step further: robustifying FL models under the evolving local test set during deployment, where various distribution shifts can arise. In this work, we identify the pitfalls of existing works under test-time distribution shifts and propose Federated Test-time Head Ensemble plus tuning(FedTHE+), which personalizes FL models with robustness to various test-time distribution shifts. We illustrate the advancement of FedTHE+ (and its computationally efficient variant FedTHE) over strong competitors, by training various neural architectures (CNN, ResNet, and Transformer) on CIFAR10 andImageNet with various test distributions. Along with this, we build a benchmark for assessing the performance and robustness of personalized FL methods during deployment. Code: https://github.com/LINs-lab/FedTHE.', 'bibtex': '@Article{Jiang2022TestTimeRP,\n author = {Liang Jiang and Tao Lin},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Test-Time Robust Personalization for Federated Learning},\n volume = {abs/2205.10920},\n year = {2022}\n}\n', 'references': ['36084dcbe0dac8057557674a102f75b20d92aa8a', 'c8e1b5ae0373381f08007807bed3bf7f6d1c2cc3', '0430dbcbfed0a737881d22340fb044028ed851a9', '29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d', 'f27dd07a57d5f9562acc4d1acedaab763dfeeaa0', 'a880b83457c018e725a408717bd70d0483c16d81', 'ab2a8ca21309859ed027928dc38e6915be0e6776', '943905454e9b06ca6c234e71efc678766a85f154', '91ba25b062661f58db2b54407009323c36e84223', '6e2a5c59f8119374bc68e3dab075d660700b8155', '9289826beb6206eeaf500105f7329d6d5a495d8a', 'e5b2e2a284db5ba7c2c011daba9769d2c56b6586', '106cc848e51ad0938e73c1b3b2ebb90d4bdea143', '77a27cd55900bc12c9e548fffcac160227d8448b', 'a5c5fbb0679613cf21c42009ebf1c70d7ac209a0', '4c7d664761c359cffd20c9d555031271ec67ab3c', 'f356b19619e2d9ebeb96839f0e87a07071248179', '9db10692e1255fe8c8cbfb68b4a3aa84e6667a49', '4b06c7e29280b1c6bc05c9df39023b48fef02c93', '148de9bf06862825290d6801728758bc5c6aa72b', '481dd25896ac531707870c9b8c179cce20013401', '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', '1696660f0aa90803f72ee806750597162c373529', '2c0f4711c9c124a8dc056eaee82a2ca5ef276da8', 'ffd393dacee23e476bd8eb0802dec86f2296b36c', '405f2de412eff9e04ba2ae74fe21fdb692c9da60', '0b8a5e2e6590e019e7bcd2a4c6c10bcaf86f2fcd', '2064242b39e0b0285dc04a076026890a2d613a4d', '40848b41ed8c9c255ecd8a920006877691b52d03', '1f9a93240e73e2f48764dae843f4a9668baadba7', 'f18acd343c641cf1d438f5b917087d67f84bcd16', '076af769eeb1ff3e9d1d398b3eed052ecbeb9aae', '4a066d008f9013004e941c9f19b959efea3e6330', '18dd17656c42322e943deaedc4eee6400debe7c2', 'd3806e9fefce863dfec7f6f83537bfb24edb278c', '4b6661347d5b58250130b89145dbd34ce310f2a0', '679237737ab2392a87a1f3c44d62b2e37f36bf01', '1f6de95137e96872274eedae1beb1bd55f03c57a', '6a5efb990b6558c21d9fdded4884c00ba152cb7c', '1c401f906d78efb466df59d6a4e523222968c63d', '569ef4e3c9f5ae968fc94000c12d212a2b679907', '022622e024890d6e044ac50e2da6b44c59bdf418', '48aced1b11e6a3e16dd1f1ed9d0823e8eae48729', '053f4d6715a4dba6f8103456fc1bb5fd6a5266c4', '06bc35120d2775bb4c26d6fd1ba68d1befcd84cb', '70f1b279f96a9ba8e15f599635ba0e3ec449ef5f', 'd8728a567685e14e148a805b51a084d0f93763b8', '2ffc4df07df933a732d442e23ce256c52de6d7c6', 'a1b8a8df281bbaec148a897927a49ea47ea31515', '47c528344fedb6cb67a38e43d095b41c34715330', '2d470f3f86bae26460339bef85420095e04c852b', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', '1fad7869d48782062ea345b47081324b21e52f5d', '69e017d4a1a25081e2d3335dc0c19ba7c877244d', '976cb563bd7edfba984fbb6795026b31a4df3237', '07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b', '35aebe08b34e5cb0d012a16563e5c3f6fd17a906', '193092aef465bec868d1089ccfcac0279b914bda', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '355869339c2eb7cb95f341350ad7a72e854ed712', 'a95d102ed27f62cf328ab7c5a8732502f2b69012', '08bd705920792314e3ef806c28d0a7f40505634f', 'e20848622d5145744127b82367f9b671c7ddb08e', '206261db1196e4e391ca42077f6fca6b3ece34d0', '9ef24a6e06da2b6e65f0a60838b5c0a692e6c5d6', '743bb270d501bb4c4c2760ee68f8864344c83114', '46d8c9e2dc9c12615eb5f6813d18f967d61c7e0d', '49bdeb07b045dd77f0bfe2b44436608770235a23', '45557cc70cd6989ab6b03e5aeb787e34299099f7', '753b7a701adc1b6072378bd048cfa8567885d9c7', '90fdbe550c6c04b4bd082e4f5714ed40d61738a6', '5ac658ff79e30358a2f4a8ac4486090c3b7a2289', '49b64383fe36268410c430352637ed23b16820c5', '4e0bb8c1c683b43357c5d5216f6b74ff2cb32434', '159395b0f7a2b9ea04f9a758d18887bcb970ee78', 'cfb40a6546904f03e74be62fe3183cea61ad5ef9', '8dcbcaaf337d7bd22e580f1bb7a795ed4bb604fd', '76c6d39edecdb943ce0f68f5a44e6608db96e383', '93ef5b740fa1b54929ead6eb177e0698d7f19719', 'd03ca175e2b2745126e792fdc31dfadae4c63afa', 'a97bb99c1c70d3e0037c5cb66a4f19ce9cc4fb5f', 'f445493badf53febbaeab340a4fca98d9e4ab7f7', 'd08b35243edc5be07387a9ed218070b31e502901', '4514be5d1e4719c4701527ca6569899d8be297ef', 'b39b45a59c27a0cb3214d5a84547f54722d40c69', 'e644a409b4a4c6eaedffe27efbc5c76280b34c61', '276194e96ebd620b5cff35a9168bdda39a0be57b', '97fb4e3d45bb098e27e0071448b6152217bd35a5', 'd1dbf643447405984eeef098b1b320dee0b3b8a7', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '4d376d6978dad0374edfa6709c9556b42d3594d3', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '225f78ae8a44723c136646044fd5c5d7f1d3d15a', 'be9a17321537d9289875fe475b71f4821457b435', 'b78f419ace9938fad9402361e1aa0dbd288ec004', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'c62043a7d2537bbf40a84b9913957452a47fdb83', 'd12fbc23cff452074a286b099cd475fdd3dcd91a', 'ad67ccee45b801b0138016e2f44a566344e77320', '4f3bbc2f389e6650f27b14aef05c928156883a24', 'f3522f72b44ba67a04133a718a26e1c6c59fe9c1', 'fc6198f6cbd342dae590126b8aa69fb5d139f08d', '257e70eb918515135336039f5a0fb75a3fe36c31', '020f2ed0895d26bef7f8a959f84abf1a6cc7344a', '180c78b132f6369a384d22a9529551d86c8788d3', 'aef9f7e5ac09fcf155d64b0e0c93b940610d2e0f', 'ae744b40c02758525c07ba353a37df3aa3ef72a1', 'f56ae74c8c0842064654aeecb93d42bb3456b5b6', '922dc3bf6458ebab934608d064374d95ea323cd3', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'df24c3011fc42b72195e876ce052a0a072a1d923', '6d12a1d23b21a9b170118a56386552bc5d4727de', '162d958ff885f1462aeda91cd72582323fd6a1f4', '385197d4c02593e2823c71e4f90a0993b703620e']}
{'paperID': 'fb57a29cce4c6e4d6971ccc2f928289703566145', 'abstract': 'Self-supervised learning has significantly improved the performance of many NLP tasks. However, how can self-supervised learning discover useful representations, and why is it better than traditional approaches such as probabilistic models are still largely unknown. In this paper, we focus on the context of topic modeling and highlight a key advantage of self-supervised learning - when applied to data generated by topic models, self-supervised learning can be oblivious to the specific model, and hence is less susceptible to model misspecification. In particular, we prove that commonly used self-supervised objectives based on reconstruction or contrastive samples can both recover useful posterior information for general topic models. Empirically, we show that the same objectives can perform on par with posterior inference using the correct model, while outperforming posterior inference using misspecified models.', 'bibtex': '@Article{Luo2022UnderstandingTR,\n author = {Zeping Luo and Shiyou Wu and C. Weng and Mo Zhou and Rong Ge},\n booktitle = {International Conference on Learning Representations},\n title = {Understanding The Robustness of Self-supervised Learning Through Topic Modeling},\n year = {2022}\n}\n', 'references': ['704ac069de5a539ef42489ddb6cee0bd1650d54c', '3f1774d91c7c219f063f40b894e837e6c48b2bb2', '3ef9e9905d2454de61e6a257d37c0865dc227db5', '1902faede5212e62aa8f15f50c3981c1f0505113', 'a56759300364982894bad81ab08ca3642cf6b06d', '2bceaa105b3d31c7d539f4a316f013a062ae7c15', '0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d', '02f3c052a9cf675a6f033eac56c9dacb0a10ea28', '5a41200eeee536e101b6e462014e7396f4841c28', '01400290c7db96c4d665d1c29519c42ba47401e0', '5a2fb119d469358094792d8b36d8b027ae9fb737', '1ee3fc3d32a0b56f6e568fb0590cf635e1249f42', 'a504b45e2cff77abcc9d78cc95159c08305e44d1', '10161d83d29fc968c4612c9e9e2b61a2fc25842e', '3e7f5f4382ac6f9c4fef6197dd21abf74456acd1', '370b680057a6e324e67576a6bf1bf580af9fdd74', '38f93092ece8eee9771e61c1edaf11b1293cae1b', '7bd050967c18ec5161aa5ed244f3348cbd464642', '9a56ab8b1aba50dc2fea3cf4b531d30891a88ba9', 'a1b8a8df281bbaec148a897927a49ea47ea31515', '4815e465f223ce66e6eea29b5b12bb39fc531538', '43f2ad297941db230c089ba353efc3f281ab678c', '34733eaf66007516347a40ad5d9bbe1cc9dacb6b', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '97f4d09175705be4677d675fa27e55defac44800', '9b09d296059909490096e34e9df2d95314787ad5', '403227333329b36183004f04db72362b604adef3', 'b332d9aab535990a502881bd13ee15ea970b95ce', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '085c001e87f1d6a5d2a91371135ada990024db70', '7d0effebfa4bed19b6ba41f3af5b7e5b6890de87', '8085b60ce1771647f11ccc4728397275b502f359', '51a55df1f023571a7e07e338ee45a3e3d66ef73e', '87f40e6f3022adbc1f1905e3e506abad05a9964f', '197fe6c21bca61d501a611707461f057d1a52bee', 'a6ce434896d238e1b200a617815ea3e5141dfeeb', '6aae389341d01d73ddc7d2b6a5f819b1c6e0aade', '519ad2716c3e91066cd114f671493c282051e19d', 'df63264515b89242471224bbec564d7a2e9b704e', '50e7a1256aa9954c1764019b96ef38dc30fb1bbb', 'd2aa99aec727af4779c7b805fca06db56faf40fc', '0a02a033f0020cd75e8bce64c45851d02ba5e854', '6456d62f3a918d0b1171e09b648df879da481c8c', '649d03490ef72c5274e3bccd03d7a299d2f8da91', '168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74', '7a59fde27461a3ef4a21a249cc403d0d96e4a0d7', 'e981f16fde9185373634b53d94baa1f9185ff890', 'c18cf392c7d645d9d9e6cbadb8f50a4dbbabdf75', 'cc79e88e467f5fc76fe897fbe829fa43f5f6a0c4', 'f198043a866e9187925a8d8db9a55e3bfdd47f2c', '546fdb984bd63214dac8f552ef8d8ae6fa7c7d1a', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'cd18800a0fe0b668a1cc19f2ec95b5003d0a5035']}
{'paperID': 'b07ba82ede28d57e2e84f17fee8fef3edc24e451', 'abstract': 'Neural Ordinary Differential Equations (NODEs) are a novel neural architecture, built around initial value problems with learned dynamics which are solved during inference. Thought to be inherently more robust against adversarial perturbations, they were recently shown to be vulnerable to strong adversarial attacks, highlighting the need for formal guarantees. However, despite significant progress in robustness verification for standard feed-forward architectures, the verification of high dimensional NODEs remains an open problem. In this work, we address this challenge and propose GAINS, an analysis framework for NODEs combining three key ideas: (i) a novel class of ODE solvers, based on variable but discrete time steps, (ii) an efficient graph representation of solver trajectories, and (iii) a novel abstraction algorithm operating on this graph representation. Together, these advances enable the efficient analysis and certified training of high-dimensional NODEs, by reducing the runtime from an intractable $O(\\exp(d)+\\exp(T))$ to ${O}(d+T^2 \\log^2T)$ in the dimensionality $d$ and integration time $T$. In an extensive evaluation on computer vision (MNIST and FMNIST) and time-series forecasting (PHYSIO-NET) problems, we demonstrate the effectiveness of both our certified training and verification methods.', 'bibtex': '@Article{Zeqiri2023EfficientCT,\n author = {Mustafa Zeqiri and Mark Niklas Muller and Marc Fischer and Martin T. Vechev},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Efficient Certified Training and Robustness Verification of Neural ODEs},\n volume = {abs/2303.05246},\n year = {2023}\n}\n', 'references': ['8e3d88acaad40ec48b19c0f9932f6a52b43953e5', 'a2f519580930421aec9501d5cbbc365893877750', '9898d773a0bc3749e50aa2a540d9569a544aa6ad', 'd7aff3e7c84e8440b49a3cd36f686c6486094956', '18a133cbcd3048e085e7273d0e63c22eb99cc864', '7ebec0ae173fd451d09e86fe7cec2bc7c43bb19d', 'e34b654052a16ac9b2335267f7c56d9623bed19e', 'f62dd30782234a237deab6ec4e1368d452a00d49', 'bf0537b35036ebf16616aa626c169b5814c2f766', '1bfc6f9c9db5c6646f0f3e0213d407ae14d8f7bf', 'a0b05ab4f9d06890d1933dc4429cfeceebc4ce0c', '0033cbb16ea8996520f2e239298070975b3f31ae', '01fe33d7147cfb08d4542402089535ed911b4024', '8f3ee84811064fba1ab9b86d4f4bd39036263cef', 'a3d64aa7660367b9d08185a84f9d48fa6903b53b', '1f4b3283faf534ef92d7d7fa798b26480605ead9', '407f3c4ffda9e0ad4ecdb1830f062aaea88e5912', '18a9bb863e3110e2e981b53618b214585a32f877', '3c8a456509e6c0805354bd40a35e3f2dbf8069b1', '00d4f1c5f11ecbbc1a033e6675934703f09016f8', 'c9d239db4ab86522a6fdecb86116d1083a48823c', '25bb538adb125803a6f3f5632f31720d657adb13', 'ff22e140a0423f1cf0595d213f36402668084014', '63c99c04869a52ac69850e21732b26d8633852ea', 'f20de741e2d4ece239261de010d6d9beca3b26b9', '7ad8c18994108a630c4564400f6137bf4d8b7818', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', '4963fe1027aebe63217bd2904decf24f59379e1f', '75339d34bdac0d21a41461228ec6088eecdf857a', '449310e3538b08b43227d660227dfd2875c3c3c1', 'd21fde0f55ee0285c66334d37b8920c867959784', '651adaa058f821a890f2c5d1053d69eb481a8352', '9de69a46e6c619255eeffbfbb6c7b7163690eb48', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'b0dc598adda48acab590f95a5985fcc7abf2aca9', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '1eb09fecd75eb27825dce4f964b97f4f5cc399d7', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '82fe52ff73ccbd4476910da6d8389e1949b0c2ef', 'ed8235bff4cc8a1943f665f68158e30a8c3a72f7', 'abb1eb165c7df869407f801f0dbd7438782c93a3', 'b86131ef9fccfeeacf4945dcac6bc1b9ffe98088', 'df1c1cb4a547b9b207613d81765d4693dde9590b', 'dfa9667b1e5e3731b984cb6ebd683cc7ec6165af', '9b0c795343485bd39e39cac69ffc0e00ae0f8882', '69e1fab8fb4d599420cea6803f02e06fe9e0d11b', 'bb92676f9ec13783ac664c268191f20944718f95', '5d90f06bb70a0a3dced62413346235c02b1aa086', '162d958ff885f1462aeda91cd72582323fd6a1f4', '248b3f8544d016f073337330b9c4a62e6dcb37b5']}
{'paperID': '928d22cc193ccae8e1bf345c1f2ce253a261a077', 'abstract': 'A recourse action aims to explain a particular algorithmic decision by showing one specific way in which the instance could be modified to receive an alternate outcome. Existing recourse generation methods often assume that the machine learning model does not change over time. However, this assumption does not always hold in practice because of data distribution shifts, and in this case, the recourse action may become invalid. To redress this shortcoming, we propose the Distributionally Robust Recourse Action (DiRRAc) framework, which generates a recourse action that has a high probability of being valid under a mixture of model shifts. We formulate the robustified recourse setup as a min-max optimization problem, where the max problem is specified by Gelbrich distance over an ambiguity set around the distribution of model parameters. Then we suggest a projected gradient descent algorithm to find a robust recourse according to the min-max objective. We show that our DiRRAc framework can be extended to hedge against the misspecification of the mixture weights. Numerical experiments with both synthetic and three real-world datasets demonstrate the benefits of our proposed framework over state-of-the-art recourse methods.', 'bibtex': '@Article{Nguyen2023DistributionallyRR,\n author = {D. Nguyen and Ngoc H. Bui and Viet Anh Nguyen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Distributionally Robust Recourse Action},\n volume = {abs/2302.11211},\n year = {2023}\n}\n', 'references': ['536c78a8350de0d5916ba13abdb5a74627b9e1b8', '56f531eb3a8eaa03a6dbcf74a6c12c80b2e90f45', '262e060ddfa7db0563b083d31552b2855e2daa1e', '3eb495010ec95fbd6e1e15b19400b113d5955bb8', 'b1fe45eda204847f5f4c0b3b8eafaecaf184859c', '7b700dbbdc38b714916a93065c1d11ea16728e7c', '09b3b1e8d0d920644c2466adf3b3fd5d0333fa7e', 'cfc879af857200504ca0eefba0a2d14f998412be', '13bcee171baaa9ea589a7cfbb3124c3146a8e7dc', 'f9145d932ae9e454d9a45ae23fdb0ec0171e4ef4', '2a4b609a7e0df199332222c05caf4c58d2f4cb6c', '029a1c21accc0cbf36536cad02a12630b1c6e131', 'a696e13c63d70b62fd8e420dc82efd4e460b2cbb', '48ace0d03aa14792922faf4a4a256385b023972c', '0367827a9162f981ee02c4b3130f58085fba93f1', '4627d3fd86423fe911ee40b8eb35b1e389ab41fd', 'aea8658a9c9dfc54d9163705971f875b6409c212', '6ddf83bee5ca7a0542964e389a98adc1ed4a6838', '4a0d35989d91b3b7d2318802b1de6d10e4e6e830', '2a7c45c63959d3c5652f90d5bc3e97b39ea42f32', '1eb7f46b1a0a7df823194d86543e5554aa21021a', 'a6b7767ef515391f1e4efd0c5345fc9517954225', '324d098c5294c69735ae1670707b2f21b605b8e5', 'c2413fa296543159b32d16350d9e29f7db528790', '0cfd1f4a43824b88909a44c04b5edb251c0886a6', 'bc00ff34ec7772080c7039b17f7069a2f7df0889', '86841a74f0fd99ba369f635715ecae3007f22611', '16f0c508aa54e26aa18e3b0f3c91b0c143c6a605', 'b0608845e3d482e0d5d74892be6b7035e3b47672', '4f309712e705210df5695240a5d5fb53ea1f8641', 'b532b74b9112e5c37a72ad6de9cc618c48b0467a', 'ff6167e71af0f1bce3a28ddaf016a373379c742e', '6e77765dd3250fc671c413b44554087bad43ad92', 'e96506ee4baab43fa81cf1870cf7befb4a71fec7', 'e4ada56f5fdc24f405e32804d2a15a5823397758', 'fc82c98c4012d2c01a125d74c1aba45095dc9709', '40b12a42d91f494fad351cbfbf228b4da2d49981', '50d7ecd6901d6759a6bd9da7f2fc8f346e073577', '25badc676197a70aaf9911865eb03469e402ba57', '583b55367f787eb0c4e295707b642e63547b9806', '61d468d5254730bbecf822c6b60d7d6595d9889c', 'd2a859c85cfb0172a57ec636797a6dd28157eff8', '0b14178e7d79ac426d0a39700e1ac8b2c6f2e752', '172fc53432c0b1b8bd3d0da1be6f9363f27cfaa9', 'ea0d095c2baba90f0f3b0c8ce56c8983570c277f', '47706e9fdfe6b7d33d09579e60d6c9732cfa90e7', 'e016ee7bfc73cb5b8a92f6c517389be837c035eb', '4f8d648c52edf74e41b0996128aa536e13cc7e82', '53955424e3c73b463cb6049c836b86041c3202d6', '0e0c4a16e11b843f2b5d841ddfb60e97a376d5af', '8d56d4bc69a8c562434b9a129542bb79e9d6f1d6']}
{'paperID': '84dc2a159c062ceedae62c4a3c682f18ead59812', 'abstract': 'This work studies the threats of adversarial attack on multivariate probabilistic forecasting models and viable defense mechanisms. Our studies discover a new attack pattern that negatively impact the forecasting of a target time series via making strategic, sparse (imperceptible) modifications to the past observations of a small number of other time series. To mitigate the impact of such attack, we have developed two defense strategies. First, we extend a previously developed randomized smoothing technique in classification to multivariate forecasting scenarios. Second, we develop an adversarial training algorithm that learns to create adversarial examples and at the same time optimizes the forecasting model to improve its robustness against such adversarial simulation. Extensive experiments on real-world datasets confirm that our attack schemes are powerful and our defense algorithms are more effective compared with baseline defense mechanisms.', 'bibtex': '@Article{Liu2022RobustMT,\n author = {Linbo Liu and Youngsuk Park and T. Hoang and Hilaf Hasson and Jun Huan},\n booktitle = {International Conference on Learning Representations},\n title = {Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms},\n year = {2022}\n}\n', 'references': ['31c78bb85486ba8f1780b21e06be0f8f0f4dfaa0', '8d49aa31faa13efd826023aa48667ecd2489ca40', '356069daf4d4628559da92c9245aad39e578e090', '506ed2b755c8b207c4678d05c292e43b4d30605b', '35a9749df07a2ab97c51af4d260b095b00da7676', '4c29342faf58ca042fb57c4ad049ce99448b6cf7', '8fb65cfca2806827e85ca5aec2e479f102eccf96', '42062cd1b91871c26c6ac49b57a4d0eba79e468e', '1062876e599654afeaa491149abb40d95d3064df', '9628969fbe46586b6817a23243f2140ef8136811', '6d2d78d2015e6057b99691ae097a7362ec4e3bb6', '5a2b135fa17a877b0ac403074fd1aabbf5bd14b5', '002e7ffbee5539486fb607ac11301819959e47a5', 'e8c46dade1aaedce96ecd03178379b5921a90306', '6a5f817fd8be2d89b492a3592b9f4e69d818b537', '4139b78573dbf054fabadc7c948a92cc5e5e59eb', 'be48c10cc596c44fbe81fd3cdc9b04439530aaea', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '3b30a24f528da942b9306d2089088ab232d567ae', '0c1a245a67a5136d85de77d3ba38a900ab0579ab', '2fff1d71c751ad8bdaaa96b625d2b65eb2fb5eaa', '7f77058976e2fe75e98280371962c43d98c98321', '9db631435f7f79646a4e0a1841fbeb3340e44261', '921196c32213a229245a9705ee4768bc941e7a26', 'ab1f816ce79817a09487ea7866c95ce930d37497', '7a971b8e5a7bc267ee0617e9747f24b85bf5659f', 'acd87843a451d18b4dc6474ddce1ae946429eaf1', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '4eebe0d12aefeedf3ca85256bc8aa3b4292d47d9', '597cfe5df3d08a0bcd26e7d3128d8317265f9ced', 'df0402517a7338ae28bc54acaac400de6b456a46', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '80050cc9090cb42e5b26a8d0f11a1ce19d2b0fc8', 'fb07c8b50c3ff8e7f50582ca0896cafcef00dfff', 'f19b99c04b09ab5d45040cedaa3591af6ac674d9', 'cbfca32c732929ecb65853a2a61948955058bf1d', 'd84c010f48bb4dfd9ef71d96d9b50799c19d35e8', '7edda0f7cbbe47c66b8a231ecf50342cef3a8504', 'ae4df460a413f3b1d9a0dfa47917751af9db2597']}
{'paperID': 'b28083b620deb716b24797daf662d1950bcfec94', 'abstract': 'Deep neural networks are likely to fail when the test data is corrupted in real-world deployment (e.g., blur, weather, etc.). Test-time optimization is an effective way that adapts models to generalize to corrupted data during testing, which has been shown in the image domain. However, the techniques for improving video classification corruption robustness remain few. In this work, we propose a Temporal Coherent Test-time Optimization framework (TeCo) to utilize spatio-temporal information in test-time optimization for robust video classification. To exploit information in video with self-supervised learning, TeCo uses global content from video clips and optimizes models for entropy minimization. TeCo minimizes the entropy of the prediction based on the global content from video clips. Meanwhile, it also feeds local content to regularize the temporal coherence at the feature level. TeCo retains the generalization ability of various video classification models and achieves significant improvements in corruption robustness across Mini Kinetics-C and Mini SSV2-C. Furthermore, TeCo sets a new baseline in video classification corruption robustness via test-time optimization.', 'bibtex': '@Article{Yi2023TemporalCT,\n author = {Chenyu Yi and Siyuan Yang and Yufei Wang and Haoliang Li and Yap-Peng Tan and A. Kot},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Temporal Coherent Test-Time Optimization for Robust Video Classification},\n volume = {abs/2302.14309},\n year = {2023}\n}\n', 'references': ['12ad6e798487223f4c17aac69c9853bca8bc7830', 'e9c9ea9c463c3a4a0589b196a7ac708698723c9c', '9e383228f5ccc9cdfb96c3eba0b925fc755ab8e2', '0430dbcbfed0a737881d22340fb044028ed851a9', '140a158aa77e0e5281bf4fb3b8fa44696a2dd209', '01594f00b0deed32cba4fc4ea8c74b60be31db4a', '7a993f18bc3481919beb4589aa35195428ff294f', '9137efc758f80dd22bb56f82cca5c94f78a5db3e', '4c13e8071dafeed6a257e4e127b24ffc637bcf81', 'd6331336dddc0a0342b2e9a0c24aadad240a3f4b', '5e83e5e332c8189102364f45204afe5f282b9188', '0820694e95d1a9bfe364727a5f568f139cf5a980', 'c143ea9e30b1f2d93a9c060253845423f9e60e1f', '2ad2981a53393dc5987419a22cbe1ee3d7fa6e42', '47b5b4c7d00cfcd980f67e62ed4f27193c088296', '814a47de0d0743c4d41dd404c076541ea3c34645', '8d19137fa2d7eb68473b4f51453cb34af67b72bb', '11a1e9e8e2c4913046306d5eb31216f9d54df892', 'd8728a567685e14e148a805b51a084d0f93763b8', '908cca0abefc35acc38033603714fbb1bcadc49d', '6e1bb490ae54b42f13d14d69b2012edda4664949', '86974ebbf9f7d5797e8ea919a347c3dbbe9a1f17', '02b1607af35b48f0bd716367caf6a7428b969369', '9c77ffc223e47388155a6e3bf58ae294f51f9ccb', '9ef24a6e06da2b6e65f0a60838b5c0a692e6c5d6', 'b97d8b27224efb6655091393f27441b6ed8e6395', '89917e19175eb4f3bca02e0bace8f99d6910b054', 'f3b76f7a1042972009c37302ea00daa30238934b', 'fd3f4b31b77a296a9a239437e441deee17d62c2d', 'db787640c9b42416ff8d7015546e667e58267177', '75662c7ab05db37c52a2d750af2a8b712bbf3d53', '49b64383fe36268410c430352637ed23b16820c5', '8b47b9c3c35b2b2a78bff7822605b3040f87d699', '41071dbbbcbb27af3fec70de045f19c28535f5b7', '4bbfd46721c145852e443ae4aad35148b814bf91', '2aa2fe58f441fbecc183965bf79f4ef7fb2dd4a7', '54fc23348ed840cb5f1fe2b41c80bfdcfc03631f', '1b9c6022598085dd892f360122c0fa4c630b3f18', 'e4092ce5b2c5a1997be2702335f7d33ba7a353ef', '815aa52cfc02961d82415f080384594639a21984', '8899094797e82c5c185a0893896320ef77f60e64', '07c83f544d0604e6bab5d741b0bf9a3621d133da', 'fb41177076327c40dee612f30996739e20cf1bd7', 'b68811a9b5cafe4795a11c1048541750068b7ad0', 'b61a3f8b80bbd44f24544dc915f52fd30bbdf485', 'ea3d7de6c0880e14455b9acb28f1bc1234321456', 'e5cf901b79981c1104f68613e3186cb13384cfdc', '65a93f76720ac96f47dda1e48bbb930d1da6ea5c', 'a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb', 'c426ba865e9158a0f7962a86a50575aa943051b1', '53e14bb909ef71388c8ca189c9db84b52af2db44', '6d4c9c923e9f145d1c01a2de2afc38ec23c44253', '67dccc9a856b60bdc4d058d83657a089b8ad4486', '5127759530ce213f488af2859190697770f557f3', '180c78b132f6369a384d22a9529551d86c8788d3', '257e70eb918515135336039f5a0fb75a3fe36c31', 'dcafbfa5030fe96fb7e39764d72742ccb58dc6a2', '8085bf032d71856bdc68fdb02cbc730875ba6ce5', '0d0abe5f0b2d9611f35b9782224ce8017072b0d8', '944a221e5cdc27501ed865bb002fc53d47a91a5d']}
{'paperID': '3055d5e50bb5ae8bda9d3e603d61d57e623366d2', 'abstract': None, 'bibtex': '@Article{Kumar2023ProvableRA,\n author = {Aounon Kumar and Alexander Levine and T. Goldstein and S. Feizi},\n booktitle = {International Conference on Learning Representations},\n title = {Provable Robustness against Wasserstein Distribution Shifts via Input Randomization},\n year = {2023}\n}\n', 'references': []}
{'paperID': '2272b37bae59c30ca218abfeecdb0a61270c48f1', 'abstract': "The robustness of a deep classifier can be characterized by its margins: the decision boundary's distances to natural data points. However, it is unclear whether existing robust training methods effectively increase the margin for each vulnerable point during training. To understand this, we propose a continuous-time framework for quantifying the relative speed of the decision boundary with respect to each individual point. Through visualizing the moving speed of the decision boundary under Adversarial Training, one of the most effective robust training algorithms, a surprising moving-behavior is revealed: the decision boundary moves away from some vulnerable points but simultaneously moves closer to others, decreasing their margins. To alleviate these conflicting dynamics of the decision boundary, we propose Dynamics-aware Robust Training (DyART), which encourages the decision boundary to engage in movement that prioritizes increasing smaller margins. In contrast to prior works, DyART directly operates on the margins rather than their indirect approximations, allowing for more targeted and effective robustness improvement. Experiments on the CIFAR-10 and Tiny-ImageNet datasets verify that DyART alleviates the conflicting dynamics of the decision boundary and obtains improved robustness under various perturbation sizes compared to the state-of-the-art defenses. Our code is available at https://github.com/Yuancheng-Xu/Dynamics-Aware-Robust-Training.", 'bibtex': '@Article{Xu2023ExploringAE,\n author = {Yuancheng Xu and Yanchao Sun and Micah Goldblum and T. Goldstein and Furong Huang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness},\n volume = {abs/2302.03015},\n year = {2023}\n}\n', 'references': ['884be3f18a201af62e0cecd1324dad065b0b39ba', '2f4c451922e227cbbd4f090b74298445bbd900d0', '9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342', 'bea1187a1f8a68f1a93f0c2fa10d31f93a30f84e', '83070fd24ad5d37d5e82f794fc37cd2b84fc6858', '762752eb9a9a92b028026b17c46d50474ddf3f06', '57acaf4538d1a6e26c77cfae5640e359e763952e', 'eef656a1683e9ea18a40a3a858b085101a088d8d', '1bcbf1efb3f81f0e777b4b754cf5b9789841d12f', '99a599d8fe56529f47e78243ed61250190f96196', '289db3be7bf77e06e75541ba93269de3d604ac72', '764eff31d9596033859895d9513b838d2c57a6fb', '574e8fb91ee0e089f4cadb4145302f97f6793bdf', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', 'b27da51d2b33c67b1b366f6f3a1e61e84dbab230', '08afb316faf7b2d6c8582881458350698fd3e554', '65c63d4143b70ba718c423743bb1a4c43513e7fc', '6802553e4c2b646ff59aa4abbf8588708dddf85c', 'c9d239db4ab86522a6fdecb86116d1083a48823c', '91a05cb84f1c7dbb0354da2ff11ae92549152435', 'ff22e140a0423f1cf0595d213f36402668084014', '15e80e4a4896aa9cf1546d44fddb06e46866200c', '3f7bc67330b3eff749459568e7995f0017dfe645', 'c92be891c5f8f0f60b6de206364f9a744612d1e8', 'f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '93314b89c218c02cc1a32cad7071215693599907', '2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9', '43a4a354b67ab6d5531355a368094815d2d2593d', 'ecba2826cd7a51d4d8b9820591ff0fa6b41d66a6', 'e5b4a134836f376fc368fb8cdb194c8ca2a8828e', 'd08b35243edc5be07387a9ed218070b31e502901', '06f35a25c12d33a93578711eccf7cceab4e66d54', 'b8989afff14fb630ca58b6afa917fb42574228ee', '4b23012689e0f17912fb38d4984775e567cff8d6', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'b022f2a277a4bf5f42382e86e4380b96340b9e86', '1c4e9156ca07705531e45960b7a919dc473abb51', '77f0a39b8e02686fd85b01971f8feb7f60971f80', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '4543670c4b2d88a9b67525e0084044adef94ae76', '1fc7e419bd7a44cf43abe3cf7d811d3d96e2252d', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'c187cd7a0379baa843a5ae89a296c8e404acbd14', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'c896b6c0e8e6be8ba28142e096d705241ce94b9a', 'ca1a8b92898c4e57aebcbd6c6f57e1cb9a4d0804', 'e3f41f4b6b4e3ab740176f022bcad522ad4c38ec', 'c8c4ab59ac29973a00df4e5c8df3773a3c59995a', '5d90f06bb70a0a3dced62413346235c02b1aa086', 'f6a883e5ce485ab9300d56cb440e8634d9aa1105']}
{'paperID': '16596dd03fa40ba278f9533ea9986982dcc81fb6', 'abstract': "Pretrained large-scale vision-language models like CLIP have exhibited strong generalization over unseen tasks. Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks. In this work, we identify and explore the problem of \\emph{adapting large-scale models for zero-shot adversarial robustness}. We first identify two key factors during model adaption -- training losses and adaptation methods -- that affect the model's zero-shot adversarial robustness. We then propose a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data. We apply this training loss to two adaption methods, model finetuning and visual prompt tuning. We find that visual prompt tuning is more effective in the absence of texts, while finetuning wins in the existence of text guidance. Overall, our approach significantly improves the zero-shot adversarial robustness over CLIP, seeing an average improvement of over 31 points over ImageNet and 15 zero-shot datasets. We hope this work can shed light on understanding the zero-shot adversarial robustness of large-scale models.", 'bibtex': '@Article{Mao2022UnderstandingZA,\n author = {Chengzhi Mao and Scott Geng and Junfeng Yang and Xin Eric Wang and Carl Vondrick},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Understanding Zero-Shot Adversarial Robustness for Large-Scale Models},\n volume = {abs/2212.07016},\n year = {2022}\n}\n', 'references': ['82d19ceba300875f108a91539ca555dfad142a99', '12ad6e798487223f4c17aac69c9853bca8bc7830', '0a25c137edc7c9752aa6d99ae4084683c3fe6b56', '5dad3748e8d4d8c659005903062e5d8e855fa86c', '07c70ca55793984ffdf31582a05170ef3d62381a', 'c57293882b2561e1ba03017902df9fc2f289dea2', '191038455e19980163ac1ec3acd3c295fc2952b6', 'cb5e3f085caefd1f3d5e08637ab55d39e61234fc', 'aa8c61c9f6bb21c57e49611ccb995cfda1b53b10', 'b9ea0a971ff6cb031fa3842e3daf65263b890b28', 'adb272fbdea3631059cf88ab764bb6c2ce29f965', 'b879450f50a6113f44a5baf0bcd5b4331eeb7bbc', '88399dde6052de10a23fe43cc7d6d659493b0c7e', 'd257547d681f3a153f4bbf85f5955b5a0189e500', '9289826beb6206eeaf500105f7329d6d5a495d8a', 'd59fb7b76578e725d3179aa236ba8a26c5e7b844', '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', '141a5033d9994242b18bb3b217e79582f1ee9306', '49f4a967f66d740ee3efb704f70b8d5da197394f', '67f74fe9d46f88661573003f8f1f12967ae49fa3', 'e3f67b654b96d17fdb643f22c6763f3bc7872783', '17293cd36ee5e7ec37dcec1d5ab85f9b77ad65d5', 'c7316921fa83d4b4c433fd04ed42839d641acbe0', '3f6570fd55dc5855f93a56150e6d99c7944a1c1e', '53a2ecf14a7c698875a0f4987840589a6b6478f1', '18939eadc9c4460c8385e0591cde214a1ead067b', '2eda2921a8da4b325f9d05f556594a5884c398a7', '2d75cf1dc599d7274fa57a02af5c2da2747db36c', '664af8114f89c3f9f53d80185c3dd23d231599ef', '48079a155e69816deb706590a500c7019f3228ac', 'e1dea4c733ee7c98aaa42972452f545821b5d3b5', '5aa478f93cd6af02b2fd05ade91c07a873dabcb2', '40e44c7737cdf254b9701e690549639f7ac5326f', '6d12401822a24b2ff5542a7fa72158d891960c62', '91d8795790baf687e4b928c720314bb0fc900dea', '6c405d4b5dc41a86be05acd59c06ed19daf01d14', '00ffb4121cbd03d09ad4672a20eecd25703540ea', '8bcd98bd5a451c2bbde4a22a4d1affe3c6407af0', 'fad53f59a3a3803838c50f331de8865e4385d87e', '821fd5bed14d6d06c25fbf44123fd7be382f7b4e', 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', '2a96afaf3261a87f0daa51699b4b3cf169e092c4', '1b9c6022598085dd892f360122c0fa4c630b3f18', '651adaa058f821a890f2c5d1053d69eb481a8352', 'cb3f5defe2120076ebbcc89b9256bbfcb8b4d8a1', '8e37a3b227b68953f8067215828dc8b8714cb21b', '9c88c2357abcd58cc330179c1965fe0a8c067ebc', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', '819167ace2f0caae7745d2f25a803979be5fbfae', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', '4b18303edf701e41a288da36f8f1ba129da67eb7', 'caccc069e658ea397c9faf673e74c959c734ff53', '8e3f12804882b60ad5f59aad92755c5edb34860e', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '4aa4069693bee00d1b0759ca3df35e59284e9845', 'a83cec6a91701bd8500f8c43ad731d4353c71d55', '18c125ce0f64e85577f7d30132cf0e92ec664bf4', '522d65a3db7431015aeaa201a7fc4450a57e40c3', '84b50ebe85f7a1721800125e7882fce8c45b5c5a', 'be9a17321537d9289875fe475b71f4821457b435', '908091b4a8757c3b2f7d9cfa2c4f616ee12c5157', '0f6911bc1e6abee8bbf9dd3f8d54d40466429da7', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', '0566bf06a0368b518b8b474166f7b1dfef3f9283', '02b28f3b71138a06e40dbd614abf8568420ae183', '5a5effa909cdeafaddbbb7855037e02f8e25d632', 'ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2', 'f59370679f35e461d6257aea8ff28d01a22f0249', '6e46300d9721159a3c7cd884c62d01b3dd257c74', 'ffbff2edb9994ceac5d7b08d0049424974d20eae', '5d90f06bb70a0a3dced62413346235c02b1aa086']}
{'paperID': 'ade34f95e9350477aa7f103e9607a8f770bffe6f', 'abstract': 'We prove that image classifiers are fundamentally sensitive to small perturbations in their inputs. Specifically, we show that given some image space of n -by- n images, all but a tiny fraction of images in any image class induced over that space can be moved outside that class by adding some perturbation whose p -norm is O ( n 1 / max( p, 1) ) , as long as that image class takes up at most half of the image space. We then show that O ( n 1 / max( p, 1) ) is asymptotically optimal. Finally, we show that an increase in the bit depth of the image space leads to a loss in robustness. We supplement our results with a discussion of their implications for vision systems', 'bibtex': '@Article{Dai2023FundamentalLO,\n author = {Zheng Dai and D. Gifford},\n booktitle = {International Conference on Learning Representations},\n title = {Fundamental limits on the robustness of image classifiers},\n year = {2023}\n}\n', 'references': ['8f419ebe41828b31dca15365a067d33b97f5574e', 'c5de8b8d1ca58bca4f6edf4238bfb5c4332ed598', 'f5c8464032a936451b222be1984cabf42d6adfa8', '58c143069444c7dff4be53531a47efefc40be497', '08afb316faf7b2d6c8582881458350698fd3e554', '4e0bb8c1c683b43357c5d5216f6b74ff2cb32434', 'b4dc59552f815283f371e6aa024a4ee7dd2d101d', 'b4c1cd3f391c34bab08d6094c91793cb59f2da81', '29227576be8dd5c058b3e30d618a7efcda71ed86', 'fe1d78ac0c02cfc6a2319c4997f0e03ff55a1569', '88311ee3fbb9d8d307386c0fb53aaa0283c5eb74', '1b9c6022598085dd892f360122c0fa4c630b3f18', '7005fe514458b538b7516b41af5f5e1971154070', 'e77171024bf7dc2ff33db89710f1184543c694e5', 'be4a4f7f65d397a4e07dc83b95da6b414e0634e2', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', 'f02441fccec9ed54dd7e21a17736933c84853011', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '92b773923b69abea465cd77d42711e5b9947766e', 'd822e347820921c4d70e619b61bffc6944de9458', '2da962bf85bfdb132209d81f0cdc4e4933ecfbd8', '30ffd4a8e479d04b1dea5749eac4a466dccde64b', '377797f2f838cef001eadb34b25eb576d4f3ac61']}
{'paperID': '143c6e80c97d51719b7659da00446ece95515bb7', 'abstract': '$\\alpha$-posteriors and their variational approximations distort standard posterior inference by downweighting the likelihood and introducing variational approximation errors. We show that such distortions, if tuned appropriately, reduce the Kullback-Leibler (KL) divergence from the true, but perhaps infeasible, posterior distribution when there is potential parametric model misspecification. To make this point, we derive a Bernstein-von Mises theorem showing convergence in total variation distance of $\\alpha$-posteriors and their variational approximations to limiting Gaussian distributions. We use these distributions to evaluate the KL divergence between true and reported posteriors. We show this divergence is minimized by choosing $\\alpha$ strictly smaller than one, assuming there is a vanishingly small probability of model misspecification. The optimized value becomes smaller as the the misspecification becomes more severe. The optimized KL divergence increases logarithmically in the degree of misspecification and not linearly as with the usual posterior.', 'bibtex': '@Article{Medina2021OnTR,\n author = {Marco Avella Medina and J. M. Olea and Cynthia Rush and Amilcar Velez},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {On the Robustness to Misspecification of α-Posteriors and Their Variational Approximations},\n volume = {abs/2104.08324},\n year = {2021}\n}\n', 'references': ['b1d95800cbbe5451ac303d0ca45b30ecac8fd15a', '1f9c368316ba57215f8bdea6d4617ea298f73632', '25a3cbb5e4569ffaf03c6ac8b1f3b91e0847439c', '1cd641fcc7797666368b78dae3d9b998e4fa0b7e', '64bd7faa0044a240cfe3103554f2bd92f0049544', '23af2d124b52288d1172558e45199c87e7fa8892', '0a538c7e1f943a30f048eec0a3b0dfff25062b1a', '4aaf784dd3c65ee000e4d94d4cdc7f828f760339', '79b5cd6e30b1c0c917958aeadbd194372e6a1fdb', 'ef74bfee5f16891ca1cd43ba04a7106310b488b6', '287547fc81364e64d196abb8d891ade3f6599a5a', 'f9832eb4d8dc7cde63676a195f778167c8cafc31', '0c9248465ffdff9404cbf6d3dfdce14a5bc891a8', '534403a65fa011cc29a09e29864a88cd6216a0dd', '3f00b4b1cc85196374472de34eeda34367dd606f', '9f5ccbe7529a63c61daaabc1174ec499bfe6967d', '437f030fd6a64d6fe56a21b76926a3172d950108', 'dd22616483014b5033c5c2ed748799717838a917', 'a90226c41b79f8b06007609f39f82757073641e2', '592f65f1ab8d4949d53bc1f94ab8cd1f53caf7b0', '434954d40776c87d8b354677ac393cb121f5c80b', '7ff104b9e80bb70cc8e5a3d058eb2cf3d892632a', '6f24d7a6e1c88828e18d16c6db20f5329f6a6827', '19de6b41be976f7035fa8927932a0a89bf359a42', '401d9f30130271cc76b8c2f62b122f523459fa72', 'eea300d771ebdb5966ddf67c10df00e867e198f7', '5528877fa98a12a3bbc0d0b2159a78de08ef1ab8', '812f12569ac77bc53052df89a9d613bbfd7ecbf6', '10b938f7981ec5f243b4ba3bb231f073a7bcfad2', '9ffe0e272a5a57022644927344e91a98851c39a6', '2ce6ef4a18a28039e1148ee27b949527211de8ea', 'd0bc88745d70bdc2411a7618362fa7be13ae19ac', '94d642ba3b0f46109dfe531509a910475fb33366', '961a2295798d53dd4df5edf9e61d997f73141a23', 'ac4b7a6dad66a9a09e69019936d1c182f7f2a6e0', '1af5c57357bbe22364ce106c23ea7b016c316f96', '4991ea3eb6e75191ef0b522119323ea0e64361d7', 'e88b04fc6cbe295e0b7a28063e391e7717a5c5a2', 'fca98082fa9ff8e9dbae9922491ae54976a0ccef', '8f80ae7531ae8c8e06f53ca78d5ad8a2dfbc8697', '721f54f6fa32f5f02c5124a2b73ce5f4280b4eaf', '3cc7cb5622c17867035c6d61bffad9a723fea0fe', '926c3c4b211fcb666674ad4104263857377a7ffc', 'c9c4160f8696e265f3ea0a25fba872f4fafd2a6e', 'c0f964b2a6cab2483b824b3a4868a135cfbbd01b', '395bc88e0c22c646acd1d95d69ccca9c03e4113d', 'c87d57da3b1f2b467ef4995d30df832ee2281107', '64b4ddcf066597a200423c55652f11ce89780063', 'ba56155267c29d1b540e089df044db8f22c55a9a']}
{'paperID': '9078452e8b927597bc0aef46c64b475ea06e2a72', 'abstract': None, 'bibtex': '@Article{Liu2023RobustGD,\n author = {Weijie Liu and Jiahao Xie and Chao Zhang and Makoto Yamada and Nenggan Zheng and Hui Qian},\n booktitle = {International Conference on Learning Representations},\n title = {Robust Graph Dictionary Learning},\n year = {2023}\n}\n', 'references': []}
{'paperID': '889ea8c9210f49ff6e1690f5c06e626bbb1d17b0', 'abstract': None, 'bibtex': '@Article{Tian2023LearningMO,\n author = {Yijun Tian and Chuxu Zhang and Zhichun Guo and Xiangliang Zhang and N. Chawla},\n booktitle = {International Conference on Learning Representations},\n title = {Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness, and Efficiency},\n year = {2023}\n}\n', 'references': []}
